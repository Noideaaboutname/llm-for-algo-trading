{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1673,
     "status": "ok",
     "timestamp": 1751567366562,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "4lfzrGaLVxCG",
    "outputId": "142c2680-c3a5-416c-b5e0-89732d7722a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "✅ Clean notebook written to: /content/drive/MyDrive/Sentimental Analysis for Algo trading/embedding_using_different_LLMs.ipynb\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os, nbformat as nbf\n",
    "\n",
    "src = \"/content/drive/MyDrive/Sentimental Analysis for Algo trading/extracting_methods.ipynb\"\n",
    "\n",
    "dst = \"/content/drive/MyDrive/Sentimental Analysis for Algo trading/embedding_using_different_LLMs.ipynb\"\n",
    "\n",
    "nb = nbf.read(src, as_version=nbf.NO_CONVERT)\n",
    "nb.metadata.pop(\"widgets\", None)\n",
    "for cell in nb.cells:\n",
    "    cell.metadata.pop(\"widgets\", None)\n",
    "nbf.write(nb, dst)\n",
    "\n",
    "print(\"✅ Clean notebook written to:\", dst)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dFZMSN91tR92"
   },
   "outputs": [],
   "source": [
    "!pip install -q --upgrade transformers sentence-transformers accelerate\n",
    "!pip install mteb\n",
    "!pip install -U --no-cache-dir --force-reinstall \"datasets>=2.19.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10360,
     "status": "ok",
     "timestamp": 1751559868606,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "yPe9NVRbRmPb",
    "outputId": "fbc6085f-5115-4ab0-c7e6-0075d3d4ef28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mteb in /usr/local/lib/python3.11/dist-packages (1.38.33)\n",
      "Requirement already satisfied: datasets>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from mteb) (3.6.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from mteb) (2.0.2)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from mteb) (2.32.3)\n",
      "Requirement already satisfied: scikit_learn>=1.0.2 in /usr/local/lib/python3.11/dist-packages (from mteb) (1.6.1)\n",
      "Requirement already satisfied: scipy>=0.0.0 in /usr/local/lib/python3.11/dist-packages (from mteb) (1.15.3)\n",
      "Requirement already satisfied: sentence_transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from mteb) (4.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from mteb) (4.14.0)\n",
      "Requirement already satisfied: torch>1.0.0 in /usr/local/lib/python3.11/dist-packages (from mteb) (2.6.0+cu124)\n",
      "Requirement already satisfied: tqdm>1.0.0 in /usr/local/lib/python3.11/dist-packages (from mteb) (4.67.1)\n",
      "Requirement already satisfied: rich>=0.0.0 in /usr/local/lib/python3.11/dist-packages (from mteb) (13.9.4)\n",
      "Requirement already satisfied: pytrec-eval-terrier>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from mteb) (0.5.7)\n",
      "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from mteb) (2.11.7)\n",
      "Requirement already satisfied: eval_type_backport>=0.0.0 in /usr/local/lib/python3.11/dist-packages (from mteb) (0.2.2)\n",
      "Requirement already satisfied: polars>=0.20.22 in /usr/local/lib/python3.11/dist-packages (from mteb) (1.21.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19.0->mteb) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19.0->mteb) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19.0->mteb) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19.0->mteb) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19.0->mteb) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19.0->mteb) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->mteb) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19.0->mteb) (0.33.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19.0->mteb) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19.0->mteb) (6.0.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->mteb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->mteb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->mteb) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->mteb) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->mteb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->mteb) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->mteb) (2025.6.15)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=0.0.0->mteb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=0.0.0->mteb) (2.19.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit_learn>=1.0.2->mteb) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit_learn>=1.0.2->mteb) (3.6.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers>=3.0.0->mteb) (4.53.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence_transformers>=3.0.0->mteb) (11.2.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>1.0.0->mteb) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>1.0.0->mteb) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>1.0.0->mteb) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>1.0.0->mteb) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>1.0.0->mteb) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>1.0.0->mteb) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>1.0.0->mteb) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>1.0.0->mteb) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>1.0.0->mteb) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>1.0.0->mteb) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>1.0.0->mteb) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>1.0.0->mteb) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>1.0.0->mteb) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>1.0.0->mteb) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>1.0.0->mteb) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>1.0.0->mteb) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>1.0.0->mteb) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>1.0.0->mteb) (1.3.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->mteb) (3.11.15)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets>=2.19.0->mteb) (1.1.5)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=0.0.0->mteb) (0.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers>=3.0.0->mteb) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers>=3.0.0->mteb) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers>=3.0.0->mteb) (0.5.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>1.0.0->mteb) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.19.0->mteb) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.19.0->mteb) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.19.0->mteb) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->mteb) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->mteb) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->mteb) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->mteb) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->mteb) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->mteb) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->mteb) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.19.0->mteb) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install mteb\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "from sentence_transformers.models import Transformer, Pooling\n",
    "from mteb import get_tasks, MTEB\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import scipy.stats as st\n",
    "import numpy as np, time, torch\n",
    "from datasets  import load_dataset\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report,f1_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV , StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree        import DecisionTreeRegressor\n",
    "from sklearn.metrics     import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.tree        import DecisionTreeRegressor\n",
    "from sklearn.metrics     import mean_squared_error, mean_absolute_error, r2_score\n",
    "import yfinance as yf\n",
    "import openai\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3475,
     "status": "ok",
     "timestamp": 1751559892881,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "_2gUawud889_",
    "outputId": "80405b44-82ac-48ea-d042-db14ac0f93ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "-rw------- 1 root root 2.4M Jun 29 14:49 /content/drive/MyDrive/colab_saves/company_dfs.pkl\n",
      "<class 'dict'>\n",
      "dict_keys(['JPM', 'BAC', 'WFC', 'RY'])\n"
     ]
    }
   ],
   "source": [
    "# Download the data from my google drive so that I could avoid rerun the function below\n",
    "from google.colab import drive\n",
    "import pickle, os\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "path = '/content/drive/MyDrive/colab_saves/company_dfs.pkl'\n",
    "\n",
    "!ls -lh \"{path}\"\n",
    "with open(path, 'rb') as f:\n",
    "    company_dfs = pickle.load(f)\n",
    "print(type(company_dfs))\n",
    "print(company_dfs.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "740G82DBBXZZ"
   },
   "outputs": [],
   "source": [
    "import os, time, requests, pandas as pd\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "API_KEY = os.getenv(\"RAPIDAPI_KEY\") or \"ad973560c8msh96909b7fb3cc3fdp1c1065jsn5a49e6b93474\"\n",
    "HEADERS = {\n",
    "    \"X-RapidAPI-Key\":  API_KEY,\n",
    "    \"X-RapidAPI-Host\": \"seeking-alpha.p.rapidapi.com\",\n",
    "}\n",
    "\n",
    "session = requests.Session()\n",
    "retry_cfg = Retry(\n",
    "    total=5,\n",
    "    backoff_factor=1,\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    allowed_methods=[\"GET\"],\n",
    ")\n",
    "session.mount(\"https://\", HTTPAdapter(max_retries=retry_cfg))\n",
    "\n",
    "\n",
    "def _respect_rate_limit(resp):\n",
    "    \"\"\"Sleep if remaining-requests header is low.\"\"\"\n",
    "    left = resp.headers.get(\"X-RateLimit-Requests-Remaining\")\n",
    "    reset = resp.headers.get(\"X-RateLimit-Requests-Reset\")\n",
    "    if left is not None and reset is not None:\n",
    "        left, reset = int(left), int(reset)\n",
    "        if left < 2:\n",
    "            sleep_s = max(reset - int(time.time()), 1)\n",
    "            print(f\"[rate-limit] only {left} calls left → sleeping {sleep_s}s\")\n",
    "            time.sleep(sleep_s)\n",
    "\n",
    "# extracting the news with particular symbols\n",
    "\n",
    "def fetch_symbol_news(symbol: str,\n",
    "                      max_items: int = 200,\n",
    "                      page_size: int = 40,\n",
    "                      since: int = 0,\n",
    "                      until: int = 0) -> pd.DataFrame:\n",
    "    url   = \"https://seeking-alpha.p.rapidapi.com/news/v2/list-by-symbol\"\n",
    "    items = []\n",
    "    page  = 1\n",
    "\n",
    "    while len(items) < max_items:\n",
    "        p = {\"id\": symbol, \"size\": page_size, \"number\": page}\n",
    "        if since: p[\"since\"] = since\n",
    "        if until: p[\"until\"] = until\n",
    "\n",
    "        r = session.get(url, headers=HEADERS, params=p, timeout=20)\n",
    "        r.raise_for_status()\n",
    "        batch = r.json().get(\"data\", [])\n",
    "        if not batch:\n",
    "            break\n",
    "\n",
    "        items.extend(batch)\n",
    "        if len(batch) < page_size:\n",
    "            break\n",
    "\n",
    "        page += 1\n",
    "        time.sleep(0.25 + 0.25 * os.urandom(1)[0] / 255)\n",
    "\n",
    "    if not items:\n",
    "        return pd.DataFrame(columns=[\"title\", \"publishOn\", \"symbol\"])\n",
    "\n",
    "    df = pd.json_normalize(items[:max_items])\n",
    "\n",
    "    for c in [\"attributes.title\", \"attributes.headline\", \"attributes.teaser\"]:\n",
    "        if c in df.columns:\n",
    "            df = df.rename(columns={c: \"title\"})\n",
    "            break\n",
    "    if \"title\" not in df.columns:\n",
    "        df[\"title\"] = \"\"\n",
    "\n",
    "    if \"publishOn\" in df.columns:\n",
    "        df[\"publishOn\"] = pd.to_datetime(df[\"publishOn\"], utc=True)\n",
    "    elif \"publishedAt\" in df.columns:\n",
    "        df[\"publishOn\"] = pd.to_datetime(df[\"publishedAt\"], utc=True)\n",
    "\n",
    "    df[\"symbol\"] = symbol\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LjyNU2SsaiKn"
   },
   "outputs": [],
   "source": [
    "# collecting the news relating to JPM, BAC, WFC and RY\n",
    "df_jpm = fetch_symbol_news(\"JPM\", max_items=200, since=0, until=0)\n",
    "df_bac = fetch_symbol_news(\"BAC\", max_items=200, since=0, until=0)\n",
    "df_wfc = fetch_symbol_news(\"WFC\", max_items=200, since=0, until=0)\n",
    "df_ry  = fetch_symbol_news(\"RY\",  max_items=200, since=0, until=0)\n",
    "company_dfs = {\n",
    "    \"JPM\": df_jpm,\n",
    "    \"BAC\": df_bac,\n",
    "    \"WFC\": df_wfc,\n",
    "    \"RY\":  df_ry,}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WkhkQHe6tRxU"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TpodW_o2woXc"
   },
   "outputs": [],
   "source": [
    "# try 6 LLM transformers to do embedding\n",
    "transformer_1 = Transformer(\"sentence-transformers/paraphrase-MiniLM-L6-v2\",\n",
    "                          max_seq_length=128)\n",
    "\n",
    "transformer_2 = Transformer(\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    max_seq_length=128)\n",
    "\n",
    "transformer_3 = Transformer(\"sentence-transformers/gtr-t5-large\",\n",
    "                          max_seq_length=128)\n",
    "\n",
    "transformer_4 = Transformer(\"intfloat/e5-large-v2\",\n",
    "    max_seq_length=128)\n",
    "\n",
    "transformer_5 = Transformer(\"sentence-transformers/all-MiniLM-L12-v2\")\n",
    "\n",
    "transformer_6 = Transformer(\"sentence-transformers/stsb-roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uRibgj1xM_EN"
   },
   "outputs": [],
   "source": [
    "# testing on the LLMs embedding results\n",
    "backbones = [\n",
    "    \"sentence-transformers/paraphrase-MiniLM-L6-v2\",\n",
    "    \"sentence-transformers/all-mpnet-base-v2\",\n",
    "    \"sentence-transformers/gtr-t5-large\",\n",
    "    \"intfloat/e5-large-v2\",\n",
    "    \"sentence-transformers/all-MiniLM-L12-v2\",\n",
    "    \"sentence-transformers/stsb-roberta-large\"]\n",
    "\n",
    "def quick_sts(model_name: str) -> float:\n",
    "    model = SentenceTransformer(model_name, device=\"cuda\")\n",
    "    sts   = load_dataset(\"stsb_multi_mt\", name=\"en\", split=\"dev[:1500]\")\n",
    "    emb1  = model.encode(sts[\"sentence1\"], batch_size=64, convert_to_numpy=True)\n",
    "    emb2  = model.encode(sts[\"sentence2\"], batch_size=64, convert_to_numpy=True)\n",
    "    cos   = util.cos_sim(emb1, emb2).diagonal()\n",
    "    return st.spearmanr(cos, sts[\"similarity_score\"]).correlation\n",
    "\n",
    "def quick_banking(model_name: str,\n",
    "                  n_train: int = 1000,\n",
    "                  n_test: int  = 2000) -> float:\n",
    "    ds   = load_dataset(\"PolyAI/banking77\", split=\"train\").shuffle(seed=42)\n",
    "    text = ds[\"text\"][:n_train + n_test]\n",
    "    y    = ds[\"label\"][:n_train + n_test]\n",
    "\n",
    "    X_tr_raw, X_te_raw, y_tr, y_te = train_test_split(\n",
    "        text, y, train_size=n_train, stratify=y, random_state=42)\n",
    "\n",
    "    model = SentenceTransformer(model_name, device=\"cuda\")\n",
    "    X_tr  = model.encode(X_tr_raw, convert_to_numpy=True)\n",
    "    X_te  = model.encode(X_te_raw, convert_to_numpy=True)\n",
    "\n",
    "    clf   = LogisticRegression(max_iter=1000).fit(X_tr, y_tr)\n",
    "    return accuracy_score(y_te, clf.predict(X_te))\n",
    "\n",
    "def quick_speed(model_name: str,\n",
    "                n_sent: int = 2048,\n",
    "                sent_len: int = 16) -> tuple[float, int]:\n",
    "    dummy  = [\"hello world\"] * n_sent\n",
    "    model  = SentenceTransformer(model_name, device=\"cuda\")\n",
    "    t0     = time.time()\n",
    "    _      = model.encode(dummy, batch_size=128, convert_to_numpy=True)\n",
    "    sec    = time.time() - t0\n",
    "    dim    = model.get_sentence_embedding_dimension()\n",
    "    return n_sent / sec, dim\n",
    "\n",
    "for mdl in backbones:\n",
    "    short = mdl.split('/')[-1]\n",
    "    sts_rho        = quick_sts(mdl)\n",
    "    banking_acc    = quick_banking(mdl)\n",
    "    toks_per_sec, d = quick_speed(mdl)\n",
    "\n",
    "    print(f\"{short:30s} | STS ρ  {sts_rho: .4f}\"\n",
    "          f\" | Banking acc {banking_acc: .3f}\"\n",
    "          f\" | {int(toks_per_sec):5d} sents/s\"\n",
    "          f\" | {d:4d} dims\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NKG_cJtlX1BQ"
   },
   "source": [
    "In the previous example, I tested the embedding performance of 6 sentence‐embedding backbones by measuring (1) spearman's rho comparing to human scoring (2) classification accuracy on a simple logistic classifier (3) number of embeddings that could be proceeded per second. Considering the task onward, I choose (2) as main criterion for model selection because I aim to classify the stock price movement in the next step. Overall, the embedding of LLM paraphrase-MiniLM-L6-v2 and stsb-roberta-large has highest classification accuracy, which I choose as the optimal models in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ydf04xCZXxAV"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GTb9ymkddD8c"
   },
   "outputs": [],
   "source": [
    "# use stsb-roberta-large to do embedding\n",
    "backbone     = \"sentence-transformers/stsb-roberta-large\"\n",
    "transformer  = Transformer(backbone, max_seq_length=128)\n",
    "dim          = transformer.get_word_embedding_dimension()\n",
    "\n",
    "mean_pool    = Pooling(dim, pooling_mode_mean_tokens=True)\n",
    "max_pool     = Pooling(dim, pooling_mode_mean_tokens=False, pooling_mode_max_tokens=True)\n",
    "maxmean_pool = Pooling(dim, pooling_mode_mean_tokens=True,  pooling_mode_max_tokens=True)\n",
    "pseudo_pool  = Pooling(dim, pooling_mode_mean_tokens=False, pooling_mode_max_tokens=False,\n",
    "                       pooling_mode_cls_token=True)\n",
    "\n",
    "model_mean    = SentenceTransformer(modules=[transformer, mean_pool],    device=\"cuda\")\n",
    "model_max     = SentenceTransformer(modules=[transformer, max_pool],     device=\"cuda\")\n",
    "model_maxmean = SentenceTransformer(modules=[transformer, maxmean_pool], device=\"cuda\")\n",
    "model_pseudo  = SentenceTransformer(modules=[transformer, pseudo_pool],  device=\"cuda\")\n",
    "\n",
    "variants = {\n",
    "    \"mean\":    model_mean,\n",
    "    \"max\":     model_max,\n",
    "    \"maxmean\": model_maxmean,\n",
    "    \"pseudo\":  model_pseudo,\n",
    "}\n",
    "\n",
    "buffers = {name: [] for name in variants}\n",
    "\n",
    "for sym, df in company_dfs.items():\n",
    "    if \"attributes.publishOn\" in df.columns:\n",
    "        df = df.rename(columns={\"attributes.publishOn\": \"publishOn\"})\n",
    "\n",
    "    texts = df[\"title\"].fillna(\"\").tolist()\n",
    "\n",
    "    for name, model in variants.items():\n",
    "        emb = model.encode(texts, batch_size=32, convert_to_numpy=True)\n",
    "        print(f\"{name:7s} → {emb.shape[0]} samples × {emb.shape[1]}-dim embeddings\")\n",
    "        df[f\"emb_{name}\"] = list(emb)\n",
    "\n",
    "\n",
    "        buffers[name].append(df.assign(symbol=sym))\n",
    "\n",
    "X_store = {}\n",
    "\n",
    "for name, parts in buffers.items():\n",
    "    big_df = pd.concat(parts, ignore_index=True)\n",
    "    X = np.vstack(big_df[f\"emb_{name}\"].values)\n",
    "\n",
    "    X_store[name] = X\n",
    "    globals()[f\"X_{name}\"] = X\n",
    "\n",
    "    print(f\"{name:7s} → X_{name}.shape = {X.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hfBTkx1-LppK"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report,f1_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV , StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree        import DecisionTreeRegressor\n",
    "from sklearn.metrics     import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.tree        import DecisionTreeRegressor\n",
    "from sklearn.metrics     import mean_squared_error, mean_absolute_error, r2_score\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ava47OwQ49HA"
   },
   "outputs": [],
   "source": [
    "# collect the return data on the publish day of news and convert to 1,0\n",
    "\n",
    "def label_news_df(df: pd.DataFrame, symbol: str) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if \"attributes.publishOn\" in df.columns:\n",
    "        df = df.rename(columns={\"attributes.publishOn\": \"publishOn\"})\n",
    "\n",
    "\n",
    "    df[\"publishOn\"] = (\n",
    "        pd.to_datetime(df[\"publishOn\"], utc=True, errors=\"coerce\")\n",
    "          .dt.tz_localize(None)\n",
    "    )\n",
    "    df = df.dropna(subset=[\"publishOn\"])\n",
    "\n",
    "    # 1) build price window\n",
    "    earliest = df[\"publishOn\"].dt.date.min()\n",
    "    latest   = df[\"publishOn\"].dt.date.max()\n",
    "    start_dt = (pd.Timestamp(earliest) - pd.Timedelta(days=7)).strftime(\"%Y-%m-%d\")\n",
    "    end_dt   = (pd.Timestamp(latest)   + pd.Timedelta(days=7)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    px = yf.download(symbol, start=start_dt, end=end_dt, interval=\"1d\")\n",
    "\n",
    "    # extract the Close price\n",
    "    closes = px[\"Close\"]\n",
    "    if isinstance(closes, pd.DataFrame):\n",
    "        closes = closes.squeeze()\n",
    "\n",
    "    # normalise to midnight dates\n",
    "    closes.index = closes.index.tz_localize(None).normalize()\n",
    "    trading_dates = closes.index\n",
    "\n",
    "    ret_vals, labels = [], []\n",
    "    for ts in df[\"publishOn\"]:\n",
    "        pub = pd.to_datetime(ts).tz_localize(None).normalize()\n",
    "\n",
    "        prev_days = trading_dates[trading_dates < pub]\n",
    "        next_days = trading_dates[trading_dates > pub]\n",
    "        if prev_days.empty or next_days.empty:\n",
    "            ret_vals.append(np.nan)\n",
    "            labels.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        d0, d1 = prev_days.max(), next_days.min()\n",
    "        p0, p1 = closes.at[d0], closes.at[d1]\n",
    "\n",
    "        if pd.isna(p0) or pd.isna(p1) or p0 == 0:\n",
    "            ret_vals.append(np.nan)\n",
    "            labels.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        r = p1 / p0 - 1\n",
    "        ret_vals.append(r)\n",
    "        labels.append(int(r > 0))\n",
    "\n",
    "    df[\"return_1d\"] = ret_vals\n",
    "    df[\"label\"]     = labels\n",
    "    df = df.dropna(subset=[\"label\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2AepaoI2gR2"
   },
   "outputs": [],
   "source": [
    "# collect the embeddings\n",
    "labeled_dfs = {\n",
    "    sym: label_news_df(df, sym)\n",
    "    for sym, df in company_dfs.items()\n",
    "}\n",
    "\n",
    "buffers = {name: [] for name in variants}\n",
    "\n",
    "for sym, df in labeled_dfs.items():\n",
    "    texts = df[\"title\"].fillna(\"\").tolist()\n",
    "    for name, model in variants.items():\n",
    "        df[f\"emb_{name}\"] = list(\n",
    "            model.encode(texts, batch_size=32, convert_to_numpy=True)\n",
    "        )\n",
    "        buffers[name].append(df.assign(symbol=sym))\n",
    "\n",
    "X_store = {}\n",
    "y_vec   = None\n",
    "\n",
    "for name, parts in buffers.items():\n",
    "    big_df = pd.concat(parts, ignore_index=True)\n",
    "    X = np.vstack(big_df[f\"emb_{name}\"].values)\n",
    "    X_store[name] = X\n",
    "    globals()[f\"X_{name}\"] = X\n",
    "\n",
    "    if y_vec is None:\n",
    "        y_vec = big_df[\"label\"].astype(int).values\n",
    "\n",
    "    print(f\"{name:7s} → X_{name}.shape = {X.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "akH3zfee8GJq"
   },
   "outputs": [],
   "source": [
    "# fit PCA on the mean embeddings\n",
    "pca = PCA(n_components=788)\n",
    "pca.fit(X_mean)\n",
    "explained = pca.explained_variance_ratio_\n",
    "cumulative = np.cumsum(explained)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(np.arange(1, len(cumulative)+1), cumulative, marker='o')\n",
    "plt.xlabel(\"Number of principal components\")\n",
    "plt.ylabel(\"Cumulative explained variance\")\n",
    "plt.title(\"PCA: Cumulative Explained Variance of News Embeddings\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Svrq2GnW93G-"
   },
   "outputs": [],
   "source": [
    "# fit PCA on the max embeddings\n",
    "pca = PCA(n_components=788)\n",
    "pca.fit(X_max)\n",
    "explained = pca.explained_variance_ratio_\n",
    "cumulative = np.cumsum(explained)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(np.arange(1, len(cumulative)+1), cumulative, marker='o')\n",
    "plt.xlabel(\"Number of principal components\")\n",
    "plt.ylabel(\"Cumulative explained variance\")\n",
    "plt.title(\"PCA: Cumulative Explained Variance of News Embeddings\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FEo5bZ969814"
   },
   "outputs": [],
   "source": [
    "# fit PCA on the concatenation embeddings\n",
    "pca = PCA(n_components=788)\n",
    "pca.fit(X_maxmean)\n",
    "explained = pca.explained_variance_ratio_\n",
    "cumulative = np.cumsum(explained)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(np.arange(1, len(cumulative)+1), cumulative, marker='o')\n",
    "plt.xlabel(\"Number of principal components\")\n",
    "plt.ylabel(\"Cumulative explained variance\")\n",
    "plt.title(\"PCA: Cumulative Explained Variance of News Embeddings\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YVDehxVO-GtE"
   },
   "outputs": [],
   "source": [
    "# fit PCA on the pseudo embeddings\n",
    "pca = PCA(n_components=788)\n",
    "pca.fit(X_pseudo)\n",
    "explained = pca.explained_variance_ratio_\n",
    "cumulative = np.cumsum(explained)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(np.arange(1, len(cumulative)+1), cumulative, marker='o')\n",
    "plt.xlabel(\"Number of principal components\")\n",
    "plt.ylabel(\"Cumulative explained variance\")\n",
    "plt.title(\"PCA: Cumulative Explained Variance of News Embeddings\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BAATP8QZAEI-"
   },
   "outputs": [],
   "source": [
    "# fit logistic regression on 100 PC of mean embeddings\n",
    "pca = PCA(n_components=100, random_state=42)\n",
    "X_mean_pca = pca.fit_transform(X_mean)\n",
    "\n",
    "X_train_pca, X_test_pca, y_train, y_test = train_test_split(\n",
    "    X_mean_pca, y_vec,\n",
    "    test_size=0.2,\n",
    "    random_state=42)\n",
    "\n",
    "logit = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logit.fit(X_train_pca, y_train)\n",
    "\n",
    "y_pred = logit.predict(X_test_pca)\n",
    "\n",
    "acc      = accuracy_score(y_test, y_pred)\n",
    "micro_f1 = f1_score(y_test, y_pred, average=\"micro\")\n",
    "macro_f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "w_f1     = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "mac_rec  = recall_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "print(f\"Accuracy:         {acc:.4f}\")\n",
    "print(f\"Micro F1:         {micro_f1:.4f}\")\n",
    "print(f\"Macro F1:         {macro_f1:.4f}\")\n",
    "print(f\"Weighted F1:      {w_f1:.4f}\")\n",
    "print(f\"Macro Recall:     {mac_rec:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tdqs7ONID0CP"
   },
   "outputs": [],
   "source": [
    "# fit logistic regression on 100 PC of max embeddings\n",
    "pca_max = PCA(n_components=100, random_state=42)\n",
    "X_max_pca = pca_max.fit_transform(X_max)\n",
    "\n",
    "X_train_max, X_test_max, y_train, y_test = train_test_split(\n",
    "    X_max_pca, y_vec,\n",
    "    test_size=0.2,\n",
    "    random_state=42)\n",
    "\n",
    "logit_max = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logit_max.fit(X_train_max, y_train)\n",
    "\n",
    "y_pred_max = logit_max.predict(X_test_max)\n",
    "\n",
    "acc_max      = accuracy_score(y_test, y_pred_max)\n",
    "micro_f1_max = f1_score(y_test, y_pred_max, average=\"micro\")\n",
    "macro_f1_max = f1_score(y_test, y_pred_max, average=\"macro\")\n",
    "w_f1_max     = f1_score(y_test, y_pred_max, average=\"weighted\")\n",
    "mac_rec_max  = recall_score(y_test, y_pred_max, average=\"macro\")\n",
    "\n",
    "print(f\"Accuracy:         {acc_max:.4f}\")\n",
    "print(f\"Micro F1:         {micro_f1_max:.4f}\")\n",
    "print(f\"Macro F1:         {macro_f1_max:.4f}\")\n",
    "print(f\"Weighted F1:      {w_f1_max:.4f}\")\n",
    "print(f\"Macro Recall:     {mac_rec_max:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ccyM_EgpBZ7Q"
   },
   "outputs": [],
   "source": [
    "# fit logistic regression on 100 PC of concatenation embeddings\n",
    "pca_maxmean = PCA(n_components=100, random_state=42)\n",
    "X_maxmean_pca = pca_maxmean.fit_transform(X_maxmean)\n",
    "X_train_maxmean, X_test_maxmean, y_train, y_test = train_test_split(\n",
    "    X_maxmean_pca, y_vec,\n",
    "    test_size=0.2,\n",
    "    random_state=42)\n",
    "\n",
    "\n",
    "logit_maxmean = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logit_maxmean.fit(X_train_maxmean, y_train)\n",
    "\n",
    "y_pred_maxmean = logit_maxmean.predict(X_test_maxmean)\n",
    "\n",
    "acc_maxmean      = accuracy_score(y_test, y_pred_maxmean)\n",
    "micro_f1_maxmean = f1_score(y_test, y_pred_maxmean, average=\"micro\")\n",
    "macro_f1_maxmean = f1_score(y_test, y_pred_maxmean, average=\"macro\")\n",
    "w_f1_maxmean     = f1_score(y_test, y_pred_maxmean, average=\"weighted\")\n",
    "mac_rec_maxmean  = recall_score(y_test, y_pred_maxmean, average=\"macro\")\n",
    "\n",
    "print(f\"Accuracy:         {acc_maxmean:.4f}\")\n",
    "print(f\"Micro F1:         {micro_f1_maxmean:.4f}\")\n",
    "print(f\"Macro F1:         {macro_f1_maxmean:.4f}\")\n",
    "print(f\"Weighted F1:      {w_f1_maxmean:.4f}\")\n",
    "print(f\"Macro Recall:     {mac_rec_maxmean:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XS71YUJxGexo"
   },
   "outputs": [],
   "source": [
    "# fit logistic regression on 100 PC of pseudo embeddings\n",
    "pca_pseudo = PCA(n_components=100, random_state=42)\n",
    "X_pseudo_pca = pca_pseudo.fit_transform(X_pseudo)\n",
    "\n",
    "X_train_pseudo, X_test_pseudo, y_train, y_test = train_test_split(\n",
    "    X_pseudo_pca, y_vec,\n",
    "    test_size=0.2,\n",
    "    random_state=42)\n",
    "\n",
    "logit_pseudo = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logit_pseudo.fit(X_train_pseudo, y_train)\n",
    "\n",
    "y_pred_pseudo = logit_pseudo.predict(X_test_pseudo)\n",
    "\n",
    "acc_pseudo      = accuracy_score(y_test, y_pred_pseudo)\n",
    "micro_f1_pseudo = f1_score(y_test, y_pred_pseudo, average=\"micro\")\n",
    "macro_f1_pseudo = f1_score(y_test, y_pred_pseudo, average=\"macro\")\n",
    "w_f1_pseudo     = f1_score(y_test, y_pred_pseudo, average=\"weighted\")\n",
    "mac_rec_pseudo  = recall_score(y_test, y_pred_pseudo, average=\"macro\")\n",
    "print(f\"Accuracy:         {acc_pseudo:.4f}\")\n",
    "print(f\"Micro F1:         {micro_f1_pseudo:.4f}\")\n",
    "print(f\"Macro F1:         {macro_f1_pseudo:.4f}\")\n",
    "print(f\"Weighted F1:      {w_f1_pseudo:.4f}\")\n",
    "print(f\"Macro Recall:     {mac_rec_pseudo:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Be69MDcObJr"
   },
   "source": [
    "In the next part, I will use paraphrase-MiniLM-L6-v2 to construct the embeddings, and use such embeddings to classify the stock price movements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yKqJ8x3mOFn4"
   },
   "outputs": [],
   "source": [
    "# Try paraphrase-MiniLM-L6-v2 to do embedding\n",
    "backbone     = \"sentence-transformers/paraphrase-MiniLM-L6-v2\"\n",
    "transformer  = Transformer(backbone, max_seq_length=128)\n",
    "dim          = transformer.get_word_embedding_dimension()\n",
    "\n",
    "mean_pool    = Pooling(dim, pooling_mode_mean_tokens=True)\n",
    "max_pool     = Pooling(dim, pooling_mode_mean_tokens=False, pooling_mode_max_tokens=True)\n",
    "maxmean_pool = Pooling(dim, pooling_mode_mean_tokens=True,  pooling_mode_max_tokens=True)\n",
    "pseudo_pool  = Pooling(dim, pooling_mode_mean_tokens=False, pooling_mode_max_tokens=False,\n",
    "                       pooling_mode_cls_token=True)\n",
    "\n",
    "model_mean    = SentenceTransformer(modules=[transformer, mean_pool],    device=\"cuda\")\n",
    "model_max     = SentenceTransformer(modules=[transformer, max_pool],     device=\"cuda\")\n",
    "model_maxmean = SentenceTransformer(modules=[transformer, maxmean_pool], device=\"cuda\")\n",
    "model_pseudo  = SentenceTransformer(modules=[transformer, pseudo_pool],  device=\"cuda\")\n",
    "\n",
    "variants = {\n",
    "    \"mean\":    model_mean,\n",
    "    \"max\":     model_max,\n",
    "    \"maxmean\": model_maxmean,\n",
    "    \"pseudo\":  model_pseudo,\n",
    "}\n",
    "\n",
    "buffers = {name: [] for name in variants}\n",
    "\n",
    "for sym, df in company_dfs.items():\n",
    "    if \"attributes.publishOn\" in df.columns:\n",
    "        df = df.rename(columns={\"attributes.publishOn\": \"publishOn\"})\n",
    "\n",
    "    texts = df[\"title\"].fillna(\"\").tolist()\n",
    "\n",
    "    for name, model in variants.items():\n",
    "        emb = model.encode(texts, batch_size=32, convert_to_numpy=True)\n",
    "        print(f\"{name:7s} → {emb.shape[0]} samples × {emb.shape[1]}-dim embeddings\")\n",
    "        df[f\"emb_{name}\"] = list(emb)\n",
    "\n",
    "\n",
    "        buffers[name].append(df.assign(symbol=sym))\n",
    "\n",
    "X_store = {}\n",
    "\n",
    "for name, parts in buffers.items():\n",
    "    big_df = pd.concat(parts, ignore_index=True)\n",
    "    X = np.vstack(big_df[f\"emb_{name}\"].values)\n",
    "    X_store[name + \"_1\"] = X\n",
    "    globals()[f\"X_{name}_1\"] = X\n",
    "\n",
    "    print(f\"{name:7s} → X_{name}_1.shape = {X.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1YSL37ADTN01"
   },
   "outputs": [],
   "source": [
    "# solve out mean, max, concatenation and pseudo results\n",
    "\n",
    "variants = {\n",
    "    \"mean\":    model_mean,\n",
    "    \"max\":     model_max,\n",
    "    \"maxmean\": model_maxmean,\n",
    "    \"pseudo\":  model_pseudo,\n",
    "}\n",
    "\n",
    "buffers1 = {name: [] for name in variants}\n",
    "\n",
    "for sym, df in labeled_dfs.items():\n",
    "    texts = df[\"title\"].fillna(\"\").tolist()\n",
    "    for name, model in variants.items():\n",
    "        emb = model.encode(texts, batch_size=32, convert_to_numpy=True)\n",
    "        df1 = df.copy()\n",
    "        df1[f\"emb_{name}_1\"] = list(emb)\n",
    "        buffers1[name].append(df1.assign(symbol=sym))\n",
    "\n",
    "\n",
    "X_store_1 = {}\n",
    "y_vec_1   = None\n",
    "\n",
    "for name, parts in buffers1.items():\n",
    "    big_df = pd.concat(parts, ignore_index=True)\n",
    "    X1 = np.vstack(big_df[f\"emb_{name}_1\"].values)\n",
    "\n",
    "    X_store_1[name + \"_1\"] = X1\n",
    "    globals()[f\"X_{name}_1\"] = X1\n",
    "\n",
    "    if y_vec_1 is None:\n",
    "        y_vec_1 = big_df[\"label\"].astype(int).values\n",
    "\n",
    "    print(f\"{name:7s}_1 → X_{name}_1.shape = {X1.shape}\")\n",
    "\n",
    "for name in variants:\n",
    "    assert X_store_1[name + \"_1\"].shape[0] == len(y_vec_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j8vcYHswUoXC"
   },
   "outputs": [],
   "source": [
    "# do PCA on mean pooling embedding and fit logistic regression\n",
    "pca_mean_1 = PCA(n_components=100, random_state=42)\n",
    "X_mean_1_pca = pca_mean_1.fit_transform(X_mean_1)\n",
    "\n",
    "X_train_mean_1, X_test_mean_1, y_train_1, y_test_1 = train_test_split(\n",
    "    X_mean_1_pca, y_vec_1,\n",
    "    test_size=0.2,\n",
    "    random_state=42)\n",
    "\n",
    "logit_mean_1 = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logit_mean_1.fit(X_train_mean_1, y_train_1)\n",
    "y_pred_mean_1 = logit_mean_1.predict(X_test_mean_1)\n",
    "\n",
    "acc_mean_1      = accuracy_score(y_test_1, y_pred_mean_1)\n",
    "micro_f1_mean_1  = f1_score(y_test_1, y_pred_mean_1, average=\"micro\")\n",
    "macro_f1_mean_1  = f1_score(y_test_1, y_pred_mean_1, average=\"macro\")\n",
    "w_f1_mean_1      = f1_score(y_test_1, y_pred_mean_1, average=\"weighted\")\n",
    "mac_rec_mean_1   = recall_score(y_test_1, y_pred_mean_1, average=\"macro\")\n",
    "\n",
    "print(f\"Accuracy:         {acc_mean_1 :.4f}\")\n",
    "print(f\"Micro F1:         {micro_f1_mean_1 :.4f}\")\n",
    "print(f\"Macro F1:         {macro_f1_mean_1 :.4f}\")\n",
    "print(f\"Weighted F1:      {w_f1_mean_1 :.4f}\")\n",
    "print(f\"Macro Recall:     {mac_rec_mean_1 :.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bRuEO-fAYa8h"
   },
   "outputs": [],
   "source": [
    "# do PCA on max pooling embedding and fit logistic regression\n",
    "pca_max_1 = PCA(n_components=100, random_state=42)\n",
    "X_max_1_pca = pca_max_1.fit_transform(X_max_1)\n",
    "\n",
    "X_train_max_1, X_test_max_1, y_train_1, y_test_1 = train_test_split(\n",
    "    X_max_1_pca, y_vec_1,\n",
    "    test_size=0.2,\n",
    "    random_state=42)\n",
    "\n",
    "\n",
    "logit_max_1 = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logit_max_1.fit(X_train_max_1, y_train_1)\n",
    "y_pred_max_1 = logit_max_1.predict(X_test_max_1)\n",
    "\n",
    "acc_max_1      = accuracy_score(y_test_1, y_pred_max_1)\n",
    "micro_f1_max_1 = f1_score(y_test_1, y_pred_max_1, average=\"micro\")\n",
    "macro_f1_max_1 = f1_score(y_test_1, y_pred_max_1, average=\"macro\")\n",
    "w_f1_max_1     = f1_score(y_test_1, y_pred_max_1, average=\"weighted\")\n",
    "mac_rec_max_1  = recall_score(y_test_1, y_pred_max_1, average=\"macro\")\n",
    "print(f\"Accuracy:         {acc_max_1:.4f}\")\n",
    "print(f\"Micro F1:         {micro_f1_max_1:.4f}\")\n",
    "print(f\"Macro F1:         {macro_f1_max_1:.4f}\")\n",
    "print(f\"Weighted F1:      {w_f1_max_1:.4f}\")\n",
    "print(f\"Macro Recall:     {mac_rec_max_1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WX5RxjTrYrm8"
   },
   "outputs": [],
   "source": [
    "# do PCA on concatenation embedding and fit logistic regression\n",
    "pca_maxmean_1 = PCA(n_components=100, random_state=42)\n",
    "X_maxmean_1_pca = pca_maxmean_1.fit_transform(X_maxmean_1)\n",
    "\n",
    "X_train_maxmean_1, X_test_maxmean_1 = train_test_split(\n",
    "    X_maxmean_1_pca,test_size=0.2,random_state=42)\n",
    "\n",
    "logit_maxmean_1 = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logit_maxmean_1.fit(X_train_maxmean_1, y_train_1)\n",
    "\n",
    "y_pred_maxmean_1 = logit_maxmean_1.predict(X_test_maxmean_1)\n",
    "\n",
    "acc_maxmean_1      = accuracy_score(y_test_1, y_pred_maxmean_1)\n",
    "micro_f1_maxmean_1 = f1_score(y_test_1, y_pred_maxmean_1, average=\"micro\")\n",
    "macro_f1_maxmean_1 = f1_score(y_test_1, y_pred_maxmean_1, average=\"macro\")\n",
    "w_f1_maxmean_1     = f1_score(y_test_1, y_pred_maxmean_1, average=\"weighted\")\n",
    "mac_rec_maxmean_1  = recall_score(y_test_1, y_pred_maxmean_1, average=\"macro\")\n",
    "\n",
    "print(f\"Accuracy:         {acc_maxmean_1:.4f}\")\n",
    "print(f\"Micro F1:         {micro_f1_maxmean_1:.4f}\")\n",
    "print(f\"Macro F1:         {macro_f1_maxmean_1:.4f}\")\n",
    "print(f\"Weighted F1:      {w_f1_maxmean_1:.4f}\")\n",
    "print(f\"Macro Recall:     {mac_rec_maxmean_1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bHakvc38ZB9r"
   },
   "outputs": [],
   "source": [
    "# do PCA on pseudo pooling embedding and fit logistic regression\n",
    "pca_pseudo_1 = PCA(n_components=100, random_state=42)\n",
    "X_pseudo_1_pca = pca_pseudo_1.fit_transform(X_pseudo_1)\n",
    "\n",
    "X_train_pseudo_1, X_test_pseudo_1= train_test_split(\n",
    "    X_pseudo_1_pca,\n",
    "    test_size=0.2,\n",
    "    random_state=42)\n",
    "\n",
    "logit_pseudo_1 = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logit_pseudo_1.fit(X_train_pseudo_1, y_train_1)\n",
    "\n",
    "y_pred_pseudo_1 = logit_pseudo_1.predict(X_test_pseudo_1)\n",
    "\n",
    "acc_pseudo_1      = accuracy_score(y_test_1, y_pred_pseudo_1)\n",
    "micro_f1_pseudo_1 = f1_score(y_test_1, y_pred_pseudo_1, average=\"micro\")\n",
    "macro_f1_pseudo_1 = f1_score(y_test_1, y_pred_pseudo_1, average=\"macro\")\n",
    "w_f1_pseudo_1     = f1_score(y_test_1, y_pred_pseudo_1, average=\"weighted\")\n",
    "mac_rec_pseudo_1  = recall_score(y_test_1, y_pred_pseudo_1, average=\"macro\")\n",
    "print(f\"Accuracy:         {acc_pseudo_1:.4f}\")\n",
    "print(f\"Micro F1:         {micro_f1_pseudo_1:.4f}\")\n",
    "print(f\"Macro F1:         {macro_f1_pseudo_1:.4f}\")\n",
    "print(f\"Weighted F1:      {w_f1_pseudo_1:.4f}\")\n",
    "print(f\"Macro Recall:     {mac_rec_pseudo_1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IvZHuiIzsALU"
   },
   "outputs": [],
   "source": [
    "# Try open ai family LLM\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "def openai_embed(texts,\n",
    "                 model=\"text-embedding-ada-002\",\n",
    "                 batch_size=32):\n",
    "    all_emb = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        resp = openai.embeddings.create(model=model, input=batch)\n",
    "        emb = [d.embedding for d in resp.data]\n",
    "        all_emb.extend(emb)\n",
    "    return np.array(all_emb, dtype=np.float32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U1dZHcka3sDW"
   },
   "outputs": [],
   "source": [
    "# Solve out embedding\n",
    "emb_buffers = []\n",
    "for sym, df in company_dfs.items():\n",
    "    df = df.copy()\n",
    "    texts = df[\"title\"].fillna(\"\").tolist()\n",
    "\n",
    "    emb = openai_embed(texts)\n",
    "    df[\"emb_ada_2\"] = list(emb)\n",
    "    emb_buffers.append(df.assign(symbol=sym))\n",
    "\n",
    "big_df   = pd.concat(emb_buffers, ignore_index=True)\n",
    "X_ada_2  = np.vstack(big_df[\"emb_ada_2\"].values)\n",
    "globals()[\"X_ada_2\"] = X_ada_2\n",
    "print(\"ADA         → X_ada_2.shape =\", X_ada_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lNELFun6Aid7"
   },
   "outputs": [],
   "source": [
    "labeled_dfs = {\n",
    "    sym: label_news_df(df, sym)\n",
    "    for sym, df in company_dfs.items()\n",
    "}\n",
    "\n",
    "emb_buffers = []\n",
    "for sym, df in company_dfs.items():\n",
    "    df_lab = label_news_df(df, sym)\n",
    "    if df_lab.empty:\n",
    "        continue\n",
    "\n",
    "    texts = df_lab[\"title\"].fillna(\"\").tolist()\n",
    "    emb   = openai_embed(texts)\n",
    "\n",
    "    df_lab[\"emb_ada_2\"] = list(emb)\n",
    "    emb_buffers.append(df_lab.assign(symbol=sym))\n",
    "\n",
    "big_df   = pd.concat(emb_buffers, ignore_index=True)\n",
    "X_ada_2  = np.vstack(big_df[\"emb_ada_2\"].values)\n",
    "y_vec_2  = big_df[\"label\"].astype(int).values\n",
    "\n",
    "print(\"X_ada_2.shape =\", X_ada_2.shape)\n",
    "print(\"y_vec_2.shape =\", y_vec_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0gW-shsW6nPs"
   },
   "outputs": [],
   "source": [
    "# do PCA based on embedding and fit logistic regression\n",
    "pca_ada    = PCA(n_components=100, random_state=42)\n",
    "X_ada_pca  = pca_ada.fit_transform(X_ada_2)\n",
    "\n",
    "X_train_ada, X_test_ada, y_train_ada, y_test_ada = train_test_split(\n",
    "    X_ada_pca,y_vec_2,test_size=0.2,random_state=42)\n",
    "\n",
    "logit_ada = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logit_ada.fit(X_train_ada, y_train_ada)\n",
    "\n",
    "\n",
    "y_pred_ada = logit_ada.predict(X_test_ada)\n",
    "\n",
    "acc_ada         = accuracy_score(y_test_ada, y_pred_ada)\n",
    "micro_f1_ada    = f1_score(y_test_ada, y_pred_ada, average=\"micro\")\n",
    "macro_f1_ada    = f1_score(y_test_ada, y_pred_ada, average=\"macro\")\n",
    "weighted_f1_ada = f1_score(y_test_ada, y_pred_ada, average=\"weighted\")\n",
    "macro_rec_ada   = recall_score(y_test_ada, y_pred_ada, average=\"macro\")\n",
    "\n",
    "print(f\"Accuracy:      {acc_ada:.4f}\")\n",
    "print(f\"Micro F1:      {micro_f1_ada:.4f}\")\n",
    "print(f\"Macro F1:      {macro_f1_ada:.4f}\")\n",
    "print(f\"Weighted F1:   {weighted_f1_ada:.4f}\")\n",
    "print(f\"Macro Recall:  {macro_rec_ada:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "7df834e2b8f347c7b7ebe67beb16b700",
      "3b0e5ba1c0fb4d319fc86de6008787cb",
      "cc2becacc4db40d0a31677783b40cc0c",
      "e71d8076143847c88ae5264827695249",
      "61f76e7fac5d49df9182adab45e4c32a",
      "5657353a6d264ee0b538740712cf4b61",
      "80238423f7634f08bdee6f01ed146aa3",
      "37d64e7fe8064f2e9ef833c4c3cbac90",
      "e061f6e35371443ea30ec080dff6b9d5",
      "2483f0ca19dd470c9fc371d6a4c66fdd",
      "6a980ad893704110a5a47f4d90dd078a",
      "eb0554294a1940c991a1c72cee97a309",
      "f9331e41cd5047ff97293da99bb7f269",
      "e877ff5b830e48bd8e442092f27ace57",
      "e3739fa919f54c73867c40b5d74831fc",
      "0c0db223a7314055a4cd455581073aef",
      "e27aaea389e84750a0330731eb74af3b",
      "af3c45d120cd4f6297bbb28a78ddb6c6",
      "568e6271805a48e6a7e70a7bccd4ea96",
      "9487252ba40240caa4a25dde7f75e04c"
     ]
    },
    "executionInfo": {
     "elapsed": 65,
     "status": "ok",
     "timestamp": 1751558822645,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "Yzd92y28Q29L",
    "outputId": "5ea48ba3-6f68-4676-8445-7d9cf9485c25"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7df834e2b8f347c7b7ebe67beb16b700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(new_session=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401,
     "referenced_widgets": [
      "e49ff761eedd4542a06bd64c3e87ff0d",
      "0aa9b73c6ae846eba623c7760064ac71",
      "93cd69ab37354bda87faa427f8483f90",
      "0d3e782029c94c778921f5a9a04bc275",
      "c91680af47d84dc3991f948ec4492d0e",
      "9aa6d7d6ffc240a88971117423532c24",
      "6e197ac3e71a4e0d90b30cced700a8ca",
      "df3489fccff8488dbe3aae0f4f632822",
      "54a7ab5cb2684a16be0df5072ec0ee6c",
      "285888c2031645049734649aa07cac9c",
      "380e48b353dd476a90ce50bd6981149d",
      "b52b30d8e3744f14ada12bb3249b80ac",
      "c606a072073d42ce9ce2129fafcbe414",
      "619bcbf858e34b9f857ab8114c318f5e",
      "3f8da254ba9c411a853516380b33353c",
      "ae0ea54a9cca4377ad71c90b44a2f429",
      "402cc13c49ed4c8d902c3b0b8d3a7a4b",
      "7820018f12f1423dad512c132c84866e",
      "085a3b4e690d4b6aac0255965bdb9bd7",
      "ef45ccb786f8479c916854f422ff0afd",
      "0692e98c5c1c4b0fa9e614c92e351cf3",
      "8e7b0c9c4d564036a96a399e238a1f3c",
      "c3681156d02a4039a052da7e28bfb9a4",
      "96b2887ede0c486188a8d938f0108c80",
      "af27bc7257ee4c2d9ca12ab6c011c5ce",
      "a3a60a35c8b645ad8db4aab78fd71247",
      "70227a66d99c4210a80da1a711039862",
      "0cc4da06a76e4c2082c4430c25eb34bb",
      "3808a4ca3da34679ab76e8b0698dc3d9",
      "95691009658e4a7780b18fc633196cee",
      "053de57ee5ad4623bad03add3f6c09ee",
      "42f7a44fdfd443db920e8d426f6b2043",
      "280fa831ae6b4c7f8e422bc9e13e238e",
      "b653a7b459294f2e99296270439b894c",
      "11c04f1b43c04a759d97fd0b14602b48",
      "e73dba02a2344cc09dde47b88c806277",
      "df91539f1dda47da948f82643b8bf078",
      "042fcceb333b4d8c92a38fbeefd6ea8b",
      "5c98e31906ed4d3cb75534af4e5de8c9",
      "0b9352afacab4faca6eb99bfcd74f42a",
      "903eab5ddf204d499fbee1c7371f524a",
      "5f68ca188d9e44d8a83f2e55a616f0ec",
      "774aa9ee84ba489f865784fcbe77f656",
      "d32929004b33431ebd1d9503795fed5a",
      "1aa6593c97ad4b4784187bf34c9db4b9",
      "8fee73575b2c4875a098311abd6acd5d",
      "5fb36663c52e4a4081c5cb84db4d6b19",
      "61349bd3b0f9466db58840cd7c1e3a45",
      "40f4497760934ffead7749e56879b210",
      "e2415c0f0b674dd187003dc38712a124",
      "254378359e42437abf02f4db5f8a1784",
      "dc99c4568d8549eeb55259097f1438d7",
      "6bb2daa12a41473382cedfd51b182904",
      "0fcf2f3eb8f749f690ea09658e557cb8",
      "da4ac6cb4e0d4735a697a5d906d5dbc3",
      "be902839196d4913993281c22d820771",
      "a4938a4c09174629a4502457a1d4121b",
      "436e54ce0f254bbaaf35d72da2c500a1",
      "aacc675f7b5e4eaba41c9bb70e8e4b41",
      "ed3c6b90945b4e639e7703024ab53538",
      "f55fe1b732f44353a28d11dea3c1fcab",
      "34a53c51de514c838c893642442a0bfa",
      "f490e278104c470aa212e66b24d6bcba",
      "aa8adccc39e34ab6a4316ccc7c67b5fb",
      "b49cbc867df94c02a3703c915265f8be",
      "e48f9288d70445f5a92705dde451cc05",
      "f563dfa290a04647a80544dfe1e236c2",
      "590bf5a396864fd785bf7da92dd728e4",
      "7e60a10c00b240aab68f6562d8813a8b",
      "f7179de1b83e40d38b381bfafcb79051",
      "103a195611ea4f4e876b0a801721ca0a",
      "0a58db9f8d3c476b82ecdd5fd9e5a307",
      "c2ad85962f664e24b2c32da28574c701",
      "fc10996f29db4b5991456cc45a5ca3c7",
      "6ffa312e8b6840e990fd7e0d32c1b888",
      "9a380058130b4a3ba5959e896ff1ad26",
      "5548060113034f308b0127d155a3982a",
      "116561e567cd427e965a5b3fad4fb516",
      "71493db096f449118563b42d884a3d6c",
      "abd8112edb1047758ab731104ed32b8a",
      "e059b0d824634421b9abc61c9c1a0e84",
      "d7933861b52e422aa640921bb9167dcf",
      "9a8d5a0b64184ff6bb4e7832ad9fba56",
      "dea13731eaf24f638fa17eea8c2d57e2",
      "461132ea37b94936b934493779036593",
      "683f9cd7a9b24f2d95b5bbc3825ead60",
      "4eee92d5ba4f46dea1c664e60ef247da",
      "97a3964c7f6a4f21a0f825197174a1b1",
      "1846b5e724944078ba75d5f080b8a4f8",
      "1996ddc9f62d4ee39a440eec71cf60cb",
      "3d0156f3450647499d90599be1c10eaa",
      "6ad7bfdd2ee1407a96650906be93f75a",
      "c9acae7117c34e548f1d10b0a6f2b2b9",
      "26ec319395d34abc97f7654a13e74e3b",
      "50d4ea3da41b4b66bccf8c7ee12137b3",
      "88e9bcc9e23a432fac769aa372e6f49f",
      "c5c439c5ea914d3ba22c9c037cce46dc",
      "752a9548e29345d0bfb8ed7069589e61",
      "36c4155ea2bc41dbbb84c9f35b560b77",
      "10a0c3cbf6b1475c86af1ac9a96598c9",
      "c8038471eeba4bbbb40deecceb1fa940",
      "15ae9aaf5abd407aa155b3028d857103",
      "ca4362037f18465595d3ebcea1dd7ec4",
      "18a0af47f24342729dde9f42a2122c40",
      "223dac1331534f96a547f897194d2395",
      "477fe6736f9f43dc91846f47e2f82409",
      "2ec207ffa7c84da09ef61b2b23d7294b",
      "ee079c7d7bcc4177b88d55472f92ebcb",
      "3780053c3ea941dfade0c805014f1669",
      "bc1318d56ef940a792d601282519686c",
      "a0d6df1e16754001935449dd626decf1",
      "ef1e0888cac5493aa8a5916ef47634e8",
      "da91033cb5bb41bca778f5a613b4b11d",
      "3d27d68953064f269c1e27137e224271",
      "d4c43885741a4d2889038986dede910a",
      "027f69fa85c346bc84b0f00edf96e484",
      "dacae7d34bb7428aacedbc6680bd83ca",
      "4bf6795b6cc44999a4e34a026710d80e",
      "1cf3ecba1a8b4f5f88c584ed0f01cb22",
      "cff3913ce6ac4d18bd68c5f98449eb3f",
      "977a12dcd8a64c56807b1f35c2c192d9",
      "dc306ffa3515498b9344b19b53ea0f83",
      "8c75304d621d4e3d9843e3e623af7130",
      "fa34435324c645a19a20c38d98add093",
      "8889602fb38d4474a41b46292e394ecc",
      "d4b9c27d037342029f39a1892cc80f66",
      "f9bb4b7025ec4f6f809cc5936aef732b",
      "b4013f249df44cf7a061dd0af9c1ffc7",
      "192d258dd6f4441dbe9c2349326d8950",
      "025b7339c04f49e39472b0f64a1af426",
      "2b1dc5e0b99644fe88f2042bf8929792",
      "05e0ae7ec16d4e508341320bf474394e"
     ]
    },
    "executionInfo": {
     "elapsed": 131487,
     "status": "ok",
     "timestamp": 1751558962600,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "Z4zigSrnUSme",
    "outputId": "a2b6b727-415c-4eb3-d0ef-4db4d1da6714"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e49ff761eedd4542a06bd64c3e87ff0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b52b30d8e3744f14ada12bb3249b80ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3681156d02a4039a052da7e28bfb9a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b653a7b459294f2e99296270439b894c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa6593c97ad4b4784187bf34c9db4b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be902839196d4913993281c22d820771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f563dfa290a04647a80544dfe1e236c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "116561e567cd427e965a5b3fad4fb516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1846b5e724944078ba75d5f080b8a4f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10a0c3cbf6b1475c86af1ac9a96598c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0d6df1e16754001935449dd626decf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc306ffa3515498b9344b19b53ea0f83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Try Llama-3.1-8B-Instruct to do embedding\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1J00pDQifZEz"
   },
   "outputs": [],
   "source": [
    "# solve out mean, max, concatenation and pseudo results\n",
    "model.config.output_hidden_states = True\n",
    "tokenizer.pad_token    = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "\n",
    "def encode_texts(texts, pool=\"mean\", batch_size=16, max_length=128):\n",
    "    all_embs = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            out = model(\n",
    "                **inputs,\n",
    "                use_cache=False,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "\n",
    "        hs   = out.hidden_states[-1]\n",
    "        mask = inputs.attention_mask.bool().unsqueeze(-1)\n",
    "        emb  = pools[pool](hs, mask)\n",
    "\n",
    "        all_embs.append(emb.cpu().numpy())\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return np.vstack(all_embs)\n",
    "\n",
    "\n",
    "def pool_mean(hs, mask):\n",
    "    s = (hs * mask).sum(1)\n",
    "    c = mask.sum(1).clamp(min=1)\n",
    "    return s / c\n",
    "\n",
    "def pool_max(hs, mask):\n",
    "    neg_inf = torch.finfo(hs.dtype).min\n",
    "    return hs.masked_fill(~mask, neg_inf).max(1).values\n",
    "\n",
    "def pool_meanmax(hs, mask):\n",
    "    return torch.cat([pool_mean(hs, mask), pool_max(hs, mask)], dim=1)\n",
    "\n",
    "def pool_cls(hs, mask):\n",
    "    return hs[:, 0, :]\n",
    "\n",
    "pools = {\n",
    "    \"mean\":    pool_mean,\n",
    "    \"max\":     pool_max,\n",
    "    \"meanmax\": pool_meanmax,\n",
    "    \"pseudo\":  pool_cls,\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2560358,
     "status": "ok",
     "timestamp": 1751562709044,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "Q9F_VWNYdeIv",
    "outputId": "816d6fba-c030-4974-854a-74813e30a718"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-9-2069372175.py:21: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  px = yf.download(symbol, start=start_dt, end=end_dt, interval=\"1d\")\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing symbol 'JPM': 200 articles\n",
      "  → pooling mean  ..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " done\n",
      "  → pooling max   ... done\n",
      "  → pooling meanmax... done\n",
      "  → pooling pseudo... done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-9-2069372175.py:21: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  px = yf.download(symbol, start=start_dt, end=end_dt, interval=\"1d\")\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing symbol 'BAC': 200 articles\n",
      "  → pooling mean  ..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " done\n",
      "  → pooling max   ... done\n",
      "  → pooling meanmax... done\n",
      "  → pooling pseudo... done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-9-2069372175.py:21: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  px = yf.download(symbol, start=start_dt, end=end_dt, interval=\"1d\")\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing symbol 'WFC': 200 articles\n",
      "  → pooling mean  ..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " done\n",
      "  → pooling max   ... done\n",
      "  → pooling meanmax... done\n",
      "  → pooling pseudo... done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-9-2069372175.py:21: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  px = yf.download(symbol, start=start_dt, end=end_dt, interval=\"1d\")\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing symbol 'RY': 200 articles\n",
      "  → pooling mean  ..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " done\n",
      "  → pooling max   ... done\n",
      "  → pooling meanmax... done\n",
      "  → pooling pseudo... done\n",
      "\n",
      "Concatenating embeddings for pool='mean' (4 symbols)... done → shape (800, 4096)\n",
      "\n",
      "Concatenating embeddings for pool='max' (4 symbols)... done → shape (800, 4096)\n",
      "\n",
      "Concatenating embeddings for pool='meanmax' (4 symbols)... done → shape (800, 8192)\n",
      "\n",
      "Concatenating embeddings for pool='pseudo' (4 symbols)... done → shape (800, 4096)\n",
      "\n",
      "Final y_vec_3d.shape = (800,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "buffers = {name: [] for name in pools}\n",
    "y_vec_3d = None\n",
    "\n",
    "for sym, raw_df in company_dfs.items():\n",
    "    df_lab = label_news_df(raw_df, sym)\n",
    "    if df_lab.empty:\n",
    "        print(f\"[{sym}] no data after labeling, skipping\")\n",
    "        continue\n",
    "\n",
    "    texts = df_lab[\"title\"].fillna(\"\").tolist()\n",
    "    print(f\"\\nProcessing symbol {sym!r}: {len(texts)} articles\")\n",
    "\n",
    "    for name in pools:\n",
    "        print(f\"  → pooling {name:6s}...\", end=\"\", flush=True)\n",
    "        embs = encode_texts(texts, pool=name)\n",
    "        df_tmp = df_lab.assign(**{f\"emb_{name}_3\": list(embs), \"symbol\": sym})\n",
    "        buffers[name].append(df_tmp)\n",
    "        print(\" done\")\n",
    "\n",
    "\n",
    "#   Concatenate into X_*_3 and build y_vec_3d\n",
    "for name, parts in buffers.items():\n",
    "    print(f\"\\nConcatenating embeddings for pool={name!r} ({len(parts)} symbols)...\", end=\"\", flush=True)\n",
    "    big = pd.concat(parts, ignore_index=True)\n",
    "    X   = np.vstack(big[f\"emb_{name}_3\"].values)\n",
    "    globals()[f\"X_{name}_3\"] = X\n",
    "    print(f\" done → shape {X.shape}\")\n",
    "\n",
    "    if y_vec_3d is None:\n",
    "        y_vec_3d = big[\"label\"].astype(int).values\n",
    "    else:\n",
    "        assert len(y_vec_3d) == X.shape[0]\n",
    "\n",
    "print(f\"\\nFinal y_vec_3d.shape = {y_vec_3d.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1032,
     "status": "ok",
     "timestamp": 1751562810612,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "sa622x4nd4F1",
    "outputId": "eb2bd9ed-37e2-4b1c-e724-c44112d63bed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "✅ Saved all embeddings to /content/drive/My Drive/Sentimental Analysis for Algo trading\n"
     ]
    }
   ],
   "source": [
    "# 1) Mount Google Drive (only needs to run once per session)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 2) Set up the target path\n",
    "import os\n",
    "base = '/content/drive/My Drive/Sentimental Analysis for Algo trading'\n",
    "os.makedirs(base, exist_ok=True)   # creates the folder if it doesn’t already exist\n",
    "\n",
    "# 3) Save each embedding matrix (and labels) as .npy or .npz\n",
    "import numpy as np\n",
    "\n",
    "# Option A: separate .npy files\n",
    "np.save(os.path.join(base, 'X_mean_3.npy'),   X_mean_3)\n",
    "np.save(os.path.join(base, 'X_max_3.npy'),    X_max_3)\n",
    "np.save(os.path.join(base, 'X_meanmax_3.npy'),X_meanmax_3)\n",
    "np.save(os.path.join(base, 'X_pseudo_3.npy'), X_pseudo_3)\n",
    "np.save(os.path.join(base, 'y_vec_3d.npy'),   y_vec_3d)\n",
    "\n",
    "# Option B: one .npz container\n",
    "np.savez(os.path.join(base, 'embeddings_3day.npz'),\n",
    "         X_mean_3=X_mean_3,\n",
    "         X_max_3=X_max_3,\n",
    "         X_meanmax_3=X_meanmax_3,\n",
    "         X_pseudo_3=X_pseudo_3,\n",
    "         y_vec_3d=y_vec_3d)\n",
    "\n",
    "print(\"✅ Saved all embeddings to\", base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1615,
     "status": "ok",
     "timestamp": 1751562825808,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "yywy8X8iHUiV",
    "outputId": "ba8f7d84-6305-4e26-ea92-5cc27fb0a296"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:      0.5687\n",
      "Micro F1:      0.5687\n",
      "Macro F1:      0.5156\n",
      "Weighted F1:   0.5477\n",
      "Macro Recall:  0.5234\n"
     ]
    }
   ],
   "source": [
    "# do PCA based on mean pooling embedding and fit logistic regression\n",
    "pca_mean    = PCA(n_components=100, random_state=42)\n",
    "X_mean_pca  = pca_mean.fit_transform(X_mean_3)\n",
    "\n",
    "X_tr_mean_3, X_te_mean_3, y_tr_mean_3, y_te_mean_3 = train_test_split(\n",
    "    X_mean_pca, y_vec_3d,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_vec_3d\n",
    ")\n",
    "\n",
    "logit = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logit.fit(X_tr_mean_3, y_tr_mean_3)\n",
    "\n",
    "y_pred_mean_3 = logit.predict(X_te_mean_3)\n",
    "\n",
    "acc_mean_3        = accuracy_score(y_te_mean_3, y_pred_mean_3)\n",
    "micro_f1_mean_3      = f1_score(y_te_mean_3, y_pred_mean_3, average=\"micro\")\n",
    "macro_f1_mean_3      = f1_score(y_te_mean_3, y_pred_mean_3, average=\"macro\")\n",
    "weighted_f1_mean_3   = f1_score(y_te_mean_3, y_pred_mean_3, average=\"weighted\")\n",
    "macro_recall_mean_3  = recall_score(y_te_mean_3, y_pred_mean_3, average=\"macro\")\n",
    "\n",
    "print(f\"Accuracy:      {acc_mean_3:.4f}\")\n",
    "print(f\"Micro F1:      {micro_f1_mean_3:.4f}\")\n",
    "print(f\"Macro F1:      {macro_f1_mean_3:.4f}\")\n",
    "print(f\"Weighted F1:   {weighted_f1_mean_3:.4f}\")\n",
    "print(f\"Macro Recall:  {macro_recall_mean_3:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1849,
     "status": "ok",
     "timestamp": 1751562846699,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "h6AiJVu0JJVR",
    "outputId": "38216033-d70e-4796-b469-a0b3e0a20c16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MAX Pooling ===\n",
      "Accuracy:      0.5625\n",
      "Micro F1:      0.5625\n",
      "Macro F1:      0.5267\n",
      "Weighted F1:   0.5527\n",
      "Macro Recall:  0.5286\n"
     ]
    }
   ],
   "source": [
    "# do PCA based on max pooling embedding and fit logistic regression\n",
    "pca_max     = PCA(n_components=100, random_state=42)\n",
    "X_max_pca   = pca_max.fit_transform(X_max_3)\n",
    "\n",
    "\n",
    "X_tr_max_3, X_te_max_3, y_tr_max_3, y_te_max_3 = train_test_split(\n",
    "    X_max_pca, y_vec_3d,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_vec_3d)\n",
    "\n",
    "logit_max = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logit_max.fit(X_tr_max_3, y_tr_max_3)\n",
    "\n",
    "y_pred_max_3 = logit_max.predict(X_te_max_3)\n",
    "\n",
    "acc_max_3        = accuracy_score(y_te_max_3, y_pred_max_3)\n",
    "micro_f1_max_3   = f1_score(y_te_max_3, y_pred_max_3, average=\"micro\")\n",
    "macro_f1_max_3   = f1_score(y_te_max_3, y_pred_max_3, average=\"macro\")\n",
    "weighted_f1_max_3= f1_score(y_te_max_3, y_pred_max_3, average=\"weighted\")\n",
    "macro_recall_max_3 = recall_score(y_te_max_3, y_pred_max_3, average=\"macro\")\n",
    "\n",
    "print(\"\\n=== MAX Pooling ===\")\n",
    "print(f\"Accuracy:      {acc_max_3:.4f}\")\n",
    "print(f\"Micro F1:      {micro_f1_max_3:.4f}\")\n",
    "print(f\"Macro F1:      {macro_f1_max_3:.4f}\")\n",
    "print(f\"Weighted F1:   {weighted_f1_max_3:.4f}\")\n",
    "print(f\"Macro Recall:  {macro_recall_max_3:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3583,
     "status": "ok",
     "timestamp": 1751562963587,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "HD_2gXqdJNkm",
    "outputId": "97f0ab1e-5843-4897-b12b-982c01b0e4fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:      0.5563\n",
      "Micro F1:      0.5563\n",
      "Macro F1:      0.5143\n",
      "Weighted F1:   0.5429\n",
      "Macro Recall:  0.5182\n"
     ]
    }
   ],
   "source": [
    "# do PCA based on concatenation pooling embedding and fit logistic regression\n",
    "pca_maxmean     = PCA(n_components=100, random_state=42)\n",
    "X_meanmax_pca   = pca_maxmean.fit_transform(X_meanmax_3)\n",
    "\n",
    "X_tr_maxmean_3, X_te_maxmean_3, y_tr_maxmean_3, y_te_maxmean_3 = train_test_split(\n",
    "    X_meanmax_pca, y_vec_3d,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_vec_3d)\n",
    "\n",
    "logit_maxmean = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logit_maxmean.fit(X_tr_maxmean_3, y_tr_maxmean_3)\n",
    "\n",
    "y_pred_maxmean_3 = logit_maxmean.predict(X_te_maxmean_3)\n",
    "\n",
    "acc_maxmean_3         = accuracy_score(y_te_maxmean_3, y_pred_maxmean_3)\n",
    "micro_f1_maxmean_3    = f1_score(y_te_maxmean_3, y_pred_maxmean_3, average=\"micro\")\n",
    "macro_f1_maxmean_3    = f1_score(y_te_maxmean_3, y_pred_maxmean_3, average=\"macro\")\n",
    "weighted_f1_maxmean_3 = f1_score(y_te_maxmean_3, y_pred_maxmean_3, average=\"weighted\")\n",
    "macro_recall_maxmean_3= recall_score(y_te_maxmean_3, y_pred_maxmean_3, average=\"macro\")\n",
    "\n",
    "print(f\"Accuracy:      {acc_maxmean_3:.4f}\")\n",
    "print(f\"Micro F1:      {micro_f1_maxmean_3:.4f}\")\n",
    "print(f\"Macro F1:      {macro_f1_maxmean_3:.4f}\")\n",
    "print(f\"Weighted F1:   {weighted_f1_maxmean_3:.4f}\")\n",
    "print(f\"Macro Recall:  {macro_recall_maxmean_3:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1974,
     "status": "ok",
     "timestamp": 1751562973395,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "ngGri3wIJRtz",
    "outputId": "5da6f9de-046e-4977-e63e-d67ce49c4d03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:      0.6000\n",
      "Micro F1:      0.6000\n",
      "Macro F1:      0.3750\n",
      "Weighted F1:   0.4500\n",
      "Macro Recall:  0.5000\n"
     ]
    }
   ],
   "source": [
    "# do PCA based on pseudo pooling embedding and fit logistic regression\n",
    "pca_pseudo    = PCA(n_components=100, random_state=42)\n",
    "X_pseudo_pca  = pca_pseudo.fit_transform(X_pseudo_3)\n",
    "\n",
    "X_tr_pseudo_3, X_te_pseudo_3, y_tr_pseudo_3, y_te_pseudo_3 = train_test_split(\n",
    "    X_pseudo_pca, y_vec_3d,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_vec_3d)\n",
    "\n",
    "\n",
    "logit_pseudo = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logit_pseudo.fit(X_tr_pseudo_3, y_tr_pseudo_3)\n",
    "\n",
    "\n",
    "y_pred_pseudo_3 = logit_pseudo.predict(X_te_pseudo_3)\n",
    "\n",
    "acc_pseudo_3         = accuracy_score(y_te_pseudo_3, y_pred_pseudo_3)\n",
    "micro_f1_pseudo_3    = f1_score(y_te_pseudo_3, y_pred_pseudo_3, average=\"micro\")\n",
    "macro_f1_pseudo_3    = f1_score(y_te_pseudo_3, y_pred_pseudo_3, average=\"macro\")\n",
    "weighted_f1_pseudo_3 = f1_score(y_te_pseudo_3, y_pred_pseudo_3, average=\"weighted\")\n",
    "macro_recall_pseudo_3= recall_score(y_te_pseudo_3, y_pred_pseudo_3, average=\"macro\")\n",
    "\n",
    "print(f\"Accuracy:      {acc_pseudo_3:.4f}\")\n",
    "print(f\"Micro F1:      {micro_f1_pseudo_3:.4f}\")\n",
    "print(f\"Macro F1:      {macro_f1_pseudo_3:.4f}\")\n",
    "print(f\"Weighted F1:   {weighted_f1_pseudo_3:.4f}\")\n",
    "print(f\"Macro Recall:  {macro_recall_pseudo_3:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QrHqnXf6HXTR"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
