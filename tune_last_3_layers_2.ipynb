{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GCEuni9zEp99"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import nbformat, os\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "SRC = '/content/drive/MyDrive/Colab Notebooks/Untitled11.ipynb'\n",
    "DST = '/content/drive/MyDrive/colab_saves/tune last 3 layers.ipynb'\n",
    "\n",
    "with open(SRC, 'r', encoding='utf-8') as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "\n",
    "nb.metadata.pop('widgets', None)\n",
    "for cell in nb.cells:\n",
    "    cell.metadata.pop('widgets', None)\n",
    "os.makedirs(os.path.dirname(DST), exist_ok=True)\n",
    "with open(DST, 'w', encoding='utf-8') as f:\n",
    "    nbformat.write(nb, f)\n",
    "\n",
    "print(\"✅ Clean copy saved to\", DST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IeIHoRUzr2ox"
   },
   "outputs": [],
   "source": [
    "!pip install datasets sentence-transformers\n",
    "!pip install --upgrade datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 19399,
     "status": "ok",
     "timestamp": 1754371035711,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -480
    },
    "id": "XQHq6892r61L"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, SentencesDataset, InputExample\n",
    "import torch\n",
    "import os, time, requests, pandas as pd\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers.losses import CosineSimilarityLoss\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "import os, time, requests, pandas as pd\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import numpy as np\n",
    "import pickle\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1754371035792,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -480
    },
    "id": "cTLYksujyOmi"
   },
   "outputs": [],
   "source": [
    "import os, time, requests, pandas as pd\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "API_KEY = os.getenv(\"RAPIDAPI_KEY\") or \"ad973560c8msh96909b7fb3cc3fdp1c1065jsn5a49e6b93474\"\n",
    "HEADERS = {\n",
    "    \"X-RapidAPI-Key\":  API_KEY,\n",
    "    \"X-RapidAPI-Host\": \"seeking-alpha.p.rapidapi.com\",\n",
    "}\n",
    "\n",
    "session = requests.Session()\n",
    "retry_cfg = Retry(\n",
    "    total=5,\n",
    "    backoff_factor=1,\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    allowed_methods=[\"GET\"],\n",
    ")\n",
    "session.mount(\"https://\", HTTPAdapter(max_retries=retry_cfg))\n",
    "\n",
    "\n",
    "def _respect_rate_limit(resp):\n",
    "    \"\"\"Sleep if remaining-requests header is low.\"\"\"\n",
    "    left = resp.headers.get(\"X-RateLimit-Requests-Remaining\")\n",
    "    reset = resp.headers.get(\"X-RateLimit-Requests-Reset\")\n",
    "    if left is not None and reset is not None:\n",
    "        left, reset = int(left), int(reset)\n",
    "        if left < 2:\n",
    "            sleep_s = max(reset - int(time.time()), 1)\n",
    "            print(f\"[rate-limit] only {left} calls left → sleeping {sleep_s}s\")\n",
    "            time.sleep(sleep_s)\n",
    "\n",
    "# extracting the news with particular symbols\n",
    "\n",
    "def fetch_symbol_news(symbol: str,\n",
    "                      max_items: int = 200,\n",
    "                      page_size: int = 40,\n",
    "                      since: int = 0,\n",
    "                      until: int = 0) -> pd.DataFrame:\n",
    "    url   = \"https://seeking-alpha.p.rapidapi.com/news/v2/list-by-symbol\"\n",
    "    items = []\n",
    "    page  = 1\n",
    "\n",
    "    while len(items) < max_items:\n",
    "        p = {\"id\": symbol, \"size\": page_size, \"number\": page}\n",
    "        if since: p[\"since\"] = since\n",
    "        if until: p[\"until\"] = until\n",
    "\n",
    "        r = session.get(url, headers=HEADERS, params=p, timeout=20)\n",
    "        r.raise_for_status()\n",
    "        batch = r.json().get(\"data\", [])\n",
    "        if not batch:\n",
    "            break\n",
    "\n",
    "        items.extend(batch)\n",
    "        if len(batch) < page_size:\n",
    "            break\n",
    "\n",
    "        page += 1\n",
    "        time.sleep(0.25 + 0.25 * os.urandom(1)[0] / 255)\n",
    "\n",
    "    if not items:\n",
    "        return pd.DataFrame(columns=[\"title\", \"publishOn\", \"symbol\"])\n",
    "\n",
    "    df = pd.json_normalize(items[:max_items])\n",
    "\n",
    "    for c in [\"attributes.title\", \"attributes.headline\", \"attributes.teaser\"]:\n",
    "        if c in df.columns:\n",
    "            df = df.rename(columns={c: \"title\"})\n",
    "            break\n",
    "    if \"title\" not in df.columns:\n",
    "        df[\"title\"] = \"\"\n",
    "\n",
    "    if \"publishOn\" in df.columns:\n",
    "        df[\"publishOn\"] = pd.to_datetime(df[\"publishOn\"], utc=True)\n",
    "    elif \"publishedAt\" in df.columns:\n",
    "        df[\"publishOn\"] = pd.to_datetime(df[\"publishedAt\"], utc=True)\n",
    "\n",
    "    df[\"symbol\"] = symbol\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 1515,
     "status": "ok",
     "timestamp": 1754371037308,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -480
    },
    "id": "eFzHlLckyRoO"
   },
   "outputs": [],
   "source": [
    "# collect the return data on the publish day of news and convert to 1,0\n",
    "import yfinance as yf\n",
    "def label_news_df(df: pd.DataFrame, symbol: str) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if \"attributes.publishOn\" in df.columns:\n",
    "        df = df.rename(columns={\"attributes.publishOn\": \"publishOn\"})\n",
    "\n",
    "\n",
    "    df[\"publishOn\"] = (\n",
    "        pd.to_datetime(df[\"publishOn\"], utc=True, errors=\"coerce\")\n",
    "          .dt.tz_localize(None)\n",
    "    )\n",
    "    df = df.dropna(subset=[\"publishOn\"])\n",
    "\n",
    "    # 1) build price window\n",
    "    earliest = df[\"publishOn\"].dt.date.min()\n",
    "    latest   = df[\"publishOn\"].dt.date.max()\n",
    "    start_dt = (pd.Timestamp(earliest) - pd.Timedelta(days=7)).strftime(\"%Y-%m-%d\")\n",
    "    end_dt   = (pd.Timestamp(latest)   + pd.Timedelta(days=7)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    px = yf.download(symbol, start=start_dt, end=end_dt, interval=\"1d\")\n",
    "\n",
    "    # extract the Close price\n",
    "    closes = px[\"Close\"]\n",
    "    if isinstance(closes, pd.DataFrame):\n",
    "        closes = closes.squeeze()\n",
    "\n",
    "    # normalise to midnight dates\n",
    "    closes.index = closes.index.tz_localize(None).normalize()\n",
    "    trading_dates = closes.index\n",
    "\n",
    "    ret_vals, labels = [], []\n",
    "    for ts in df[\"publishOn\"]:\n",
    "        pub = pd.to_datetime(ts).tz_localize(None).normalize()\n",
    "\n",
    "        prev_days = trading_dates[trading_dates < pub]\n",
    "        next_days = trading_dates[trading_dates > pub]\n",
    "        if prev_days.empty or next_days.empty:\n",
    "            ret_vals.append(np.nan)\n",
    "            labels.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        d0, d1 = prev_days.max(), next_days.min()\n",
    "        p0, p1 = closes.at[d0], closes.at[d1]\n",
    "\n",
    "        if pd.isna(p0) or pd.isna(p1) or p0 == 0:\n",
    "            ret_vals.append(np.nan)\n",
    "            labels.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        r = p1 / p0 - 1\n",
    "        ret_vals.append(r)\n",
    "        labels.append(int(r > 0))\n",
    "\n",
    "    df[\"return_1d\"] = ret_vals\n",
    "    df[\"label\"]     = labels\n",
    "    df = df.dropna(subset=[\"label\"]).reset_index(drop=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 61570,
     "status": "ok",
     "timestamp": 1754371098880,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -480
    },
    "id": "VljLHL2ZyrQx",
    "outputId": "1bdd93d8-0d48-4df4-aec9-ec0b37d270b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "path = '/content/drive/My Drive/colab_saves/company_dfs.pkl'\n",
    "with open(path, 'rb') as f:\n",
    "    company_dfs = pickle.load(f)\n",
    "\n",
    "company_df_2 = company_dfs['group2']\n",
    "company_df_3 = company_dfs['group3']\n",
    "company_df_4 = company_dfs['group4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3217,
     "status": "ok",
     "timestamp": 1754371102230,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -480
    },
    "id": "jlXIx53-y7ai",
    "outputId": "d1892ed6-10f5-4b7a-e6ef-f372acc8863e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3951426405.py:21: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  px = yf.download(symbol, start=start_dt, end=end_dt, interval=\"1d\")\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "/tmp/ipython-input-3951426405.py:21: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  px = yf.download(symbol, start=start_dt, end=end_dt, interval=\"1d\")\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "/tmp/ipython-input-3951426405.py:21: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  px = yf.download(symbol, start=start_dt, end=end_dt, interval=\"1d\")\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "/tmp/ipython-input-3951426405.py:21: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  px = yf.download(symbol, start=start_dt, end=end_dt, interval=\"1d\")\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "/tmp/ipython-input-3951426405.py:21: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  px = yf.download(symbol, start=start_dt, end=end_dt, interval=\"1d\")\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "/tmp/ipython-input-3951426405.py:21: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  px = yf.download(symbol, start=start_dt, end=end_dt, interval=\"1d\")\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "/tmp/ipython-input-3951426405.py:21: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  px = yf.download(symbol, start=start_dt, end=end_dt, interval=\"1d\")\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "/tmp/ipython-input-3951426405.py:21: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  px = yf.download(symbol, start=start_dt, end=end_dt, interval=\"1d\")\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "/tmp/ipython-input-3951426405.py:21: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  px = yf.download(symbol, start=start_dt, end=end_dt, interval=\"1d\")\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "/tmp/ipython-input-3951426405.py:21: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  px = yf.download(symbol, start=start_dt, end=end_dt, interval=\"1d\")\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "/tmp/ipython-input-3951426405.py:21: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  px = yf.download(symbol, start=start_dt, end=end_dt, interval=\"1d\")\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "/tmp/ipython-input-3951426405.py:21: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  px = yf.download(symbol, start=start_dt, end=end_dt, interval=\"1d\")\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "labeled_group2 = {\n",
    "    sym: label_news_df(df, sym)\n",
    "    for sym, df in company_df_2.items()\n",
    "}\n",
    "\n",
    "labeled_group3 = {\n",
    "    sym: label_news_df(df, sym)\n",
    "    for sym, df in company_df_3.items()\n",
    "}\n",
    "\n",
    "labeled_group4 = {\n",
    "    sym: label_news_df(df, sym)\n",
    "    for sym, df in company_df_4.items()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 500,
     "referenced_widgets": [
      "5406924f3d0145a0811ca4146f68b5ba",
      "19144a96c1834e1999de8e378a5144cf",
      "d6c594682fa94026bcd4b58e471e1bf5",
      "04b43e014e3840ed9c02838337e75088",
      "d21340064894446fb6b4e2d6e3197790",
      "3f7b999858294edbb8d1af846c2283bd",
      "9d2cc531b3be4eeeba0128d406b5b449",
      "6bbc920e0b1942cd9548392c9dccd53b",
      "532ff4e1ec7a4b67b1f25a0717f4debe",
      "6a28aa21152043a0b89554bad452dc40",
      "0b158054a029481bb6931de02274aed3",
      "779f3f237666488baad5a6c497fb3825",
      "28d89bf21dfe4c1e9b174f43adc03c12",
      "c7b4345fa0c24797a3c9c87e687e3c14",
      "e3f98c12ac414664937e727fa1fd259b",
      "5450d089705c46f2b936ea31b44a1a48",
      "69172c59578040328a58c9a4220bff91",
      "466e59ad4d714404bd1dba0465dd5c69",
      "2c5ef62c8081407f81f7b5490a0f43be",
      "16b009e7d2ae4c7a8d51468e26ae27f7",
      "02e6d76feed64ee79ae1aa01975413e2",
      "c8c329e477554f848aa6f9b157f31aaf",
      "fe2e2d71b6224307b0117f48153ed4d1",
      "f3cb419a86db40bea996f83c23d714eb",
      "148077aa4eec48268a9743da483a4ea0",
      "6a7b8894430f4cea8000b9bf3910325d",
      "3758f6fac5f340cfaf390237433c4775",
      "97c514e5327840a4aa611290f78ca4b4",
      "9eb8e45657624d33b92e2e79397c4caf",
      "065269e2d8054c22a68d616bb0bf8833",
      "9c3a13f711fa42aabd42d9a7288ff0cf",
      "e046ef2199aa43ee949cf33892de0194",
      "39ffac73fd574e5e97fbdeb4d8969539",
      "5a98496d883a4f08aa1e7d27d57acf45",
      "144b7cbb19954152a5b16c468a90257e",
      "fe0e31f4bea24e709fb10e8bd6758bf1",
      "da6be4308dbd40b4ac67c41380d92fe0",
      "a0cb19af8e4f479c808bba2f56f731b4",
      "824ca4e41eb44d0fabca538d51a01cec",
      "c1bc901e0f4a43d8bb61dfd8840cf861",
      "3e3705e935684668b25e9305247e410c",
      "d5f11b1174904fd28664622ffc8e5723",
      "f74216b50a9944928e1f3d0430b73ff9",
      "863202efc1fe4d82841c725f585ffc64",
      "17ee0ef78ba74300b063165503f56037",
      "7511648d93fd4099af074079ca5e4730",
      "6c8dcadce0804cff933b26dcaae76835",
      "5a186f32d29a4d58ab20eef6a32a8320",
      "14497301f367448e97748c1d73744975",
      "5b97b22668ca431b991cde426f4ffa8c",
      "a71eb02c49d74d91969b44d435421781",
      "8db5782b200c4b31a3ceabb4413fe3c4",
      "baa70e215e024a1195b026044fae2758",
      "592448a90c5c4912990866e7b3b58170",
      "f12021e8a6d0470ba0827e44b6e1db8d",
      "8e7205f91ff546ea9e442ee82e942328",
      "209c92a7f4154795ae54ac464269ad77",
      "7e6509a215ae43be868e56e0fed1a902",
      "c3d6cdfb969840719aa484528a91aafd",
      "9479dde2f0094a5997a42b99d8ea4ff7",
      "473797b1f38e443f990f87ae7f1aafdc",
      "d3b8d9f644f24b509920eaafab557bb6",
      "c056ffcf084942f6b26cb9a828ea19a2",
      "303ed9fbe4c14cda894634c3b7ec7297",
      "7eb13c3bac394fbca8c4930c38363930",
      "68f97a6cace941abbf2236e2b7814b2a",
      "c18e8d4f465848a3b1fcc094dd9d3bd9",
      "6e4d1cc42f964a868324549a6c7ff95a",
      "e7978bbca47f435abda4d9e891a5137a",
      "cec752c004d44c5a82d263dd1000b02c",
      "fe7d48dd6a5b4655a5f495279c732b3c",
      "87c37816d40346b0978d7ce574b3feef",
      "ec781658c5534f7fb81a933c4a47ec39",
      "782f6fad64744047acef29c95788b613",
      "642fb2f2d12d4be29f77744f5f9bde56",
      "57f0460e9b9641f58775d3a29d64c83d",
      "7876466f340047a58b1e1d1afe97b3e5",
      "ab80d4835ab54ac28b61481405b09046",
      "0fc437623cbf433e936939637ff825f0",
      "2f5e8aa0c58e45168d5ea4798e778849",
      "f118bbc4128646e3aa07efc9f47c7e09",
      "074f440043b64c7ea8fc555e409c0b9d",
      "15c2ca33897a4bd9b954c975f6122e6b",
      "c4fac1b91dd24261b0e2739523b19909",
      "e86d13a906a241b39f40f55ecc8d01a0",
      "c8dfa42075ee43ddabfce30f7e0361ec",
      "17bf0d84392142d983b405b2f4c55ca5",
      "df5814088bc341a49ac5f8750d4ad437",
      "a2f93240dc6940279a6e08104876183a",
      "9d11d27ba1bb42f18620d3646f860366",
      "dec9023e48644ba4a9adac83865ab344",
      "0d92908d70ef449fa5afc5bd0debfa9a",
      "984fa3226af341df8827dc61e61d11ed",
      "d15406f21fe9421eb8fc11887c6a5cf9",
      "1ca0f886677b41d2a27a2872c4bf32fe",
      "6a816e878d58453c88cce4ecc203ecca",
      "6b4f566f0033493aa9274d84fc82a8a2",
      "9529954feb3c462f9152dad44847364d",
      "c5fd26a10001493981eccdf367911506",
      "6f052d98b6d64a1ea688ad35f19db220",
      "6590b6b5de85492388669c22065dd468",
      "d38fd2dfddb74b5495a2c4b55b09fb0b",
      "6921cd214ace44b58752cd4691197ce3",
      "58978fb4edcb4e86ab88c65cf218bfd8",
      "df969e184c60478e9cb4799295e155c5",
      "71217ec12b0b4f8699eeb911c1dd9a7a",
      "5ca4bc184e974daa93c6da86216378c3",
      "6dccd7d8990941fcaa575d950594b391",
      "87f52a4bcfa348f9a1ee64c85c642a0e",
      "593eae316a2a41a09c8076e763e33ff9"
     ]
    },
    "executionInfo": {
     "elapsed": 20477,
     "status": "ok",
     "timestamp": 1754371127181,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -480
    },
    "id": "vAnL90F31nHk",
    "outputId": "c0dbca29-3bb1-4cde-9c26-ccc1eaeca5e2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5406924f3d0145a0811ca4146f68b5ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "779f3f237666488baad5a6c497fb3825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe2e2d71b6224307b0117f48153ed4d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a98496d883a4f08aa1e7d27d57acf45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ee0ef78ba74300b063165503f56037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.19G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e7205f91ff546ea9e442ee82e942328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c18e8d4f465848a3b1fcc094dd9d3bd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab80d4835ab54ac28b61481405b09046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2f93240dc6940279a6e08104876183a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f052d98b6d64a1ea688ad35f19db220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/313 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Found blocks under: layers\n",
      "Un-froze 34 params total.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
    "model = SentenceTransformer(\"Qwen/Qwen3-Embedding-0.6B\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "transformer = model._modules['0']\n",
    "hf_model    = transformer.auto_model\n",
    "\n",
    "emb = hf_model.get_input_embeddings()\n",
    "for p in emb.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "blocks = None\n",
    "for name, module in hf_model.named_modules():\n",
    "    if isinstance(module, torch.nn.ModuleList):\n",
    "        blocks = module\n",
    "        print(f\"→ Found blocks under: {name}\")\n",
    "        break\n",
    "\n",
    "if blocks is None:\n",
    "    raise RuntimeError(\"Could not find transformer blocks in hf_model\")\n",
    "\n",
    "# unfreeze last 3 blocks\n",
    "for block in blocks[-3:]:\n",
    "    for p in block.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "print(f\"Un-froze {sum(p.requires_grad for p in model.parameters())} params total.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "cda62a38677a4cdda50388a7caf7fd1f",
      "7e126449bddf4dd2923d73b5e463ff2f",
      "25dab9f5d6f24176a4af2f4239872b61",
      "15884ce2359041a1ae592cf8bd698369",
      "fcd5d6d222b643de8a41f545349fc1dc",
      "f6f0f7803813469ca3aaafff761cf110",
      "cab99cce0f374d6cb52a3d38f62274cf",
      "38849feebad9480a9def399d5e135344",
      "d047794f6fad43cbac7a7555b38574f6",
      "764831ad5a1a4729bd80344e93d8cc2a",
      "780de0c196f649f8b0c783429a41c069",
      "0931133b1f1349c8a30e4fad42413903",
      "1415849bf2f149d786c36a08e12f5e97",
      "beb085f15d7c4d4dbe266270b05a1b4d",
      "e5997352875a4727b6db291e0b594008",
      "50c47d4d5c29444683b2cd6f847bb4b7",
      "323ed4d27ac64f4e8ff80d3e3df4da5b",
      "f7bfbb2fa60447c3a5b235505a8592d8",
      "d813567c6934496182a177a25fb5445d",
      "f4824daa906e4553992e3fcb67a01f69",
      "2b20cdc74d5345e6808ac84faaa4fe73",
      "ff462975e4f44e88a890ca3bf24e622f",
      "42deeae6ca214b5c8b51c5c927676e80",
      "03c3d0231f61412eb25284c85cb1f2c8",
      "4b01af5ce5134b498b84b47ca8dfe8a5",
      "c13877bda8394b40995ae7d0a46d20e4",
      "0e5be91d3bc34b34be2c8d56652b3ce2",
      "950c9ec9acae454e8657a86e583809a3",
      "5dd85f1b01dc4537839a631f11089667",
      "4f821b9be9fa4d059481c24b54ccf535",
      "d8577d135a484773a0960f083da2b445",
      "d4bc2094769341a0b2225ae61ddcc742",
      "6916841bce3d46a4bca3f7b95e4d5fbe",
      "22108553f9d44c5dbf5e427107daeeb7",
      "ca18f47f546542f98ad62d777fd90805",
      "da56ce12f817485a98924ef8f60842ba",
      "4d21139bef8045ea9f3d7cbedecc39b8",
      "e48fda72b52846a6a8b25bc54a459187",
      "0395e8529a1d4c72b0a3e1b0e0081c8b",
      "a51cd65885144b76a9a23b78b9c7919e",
      "453eff0a35774d61b92e9689e6c156fd",
      "c98e8a9cc856458eb0486753593277fd",
      "58ae4e8739b04f5b876c4b57ad796391",
      "45b684146d784609a0595f8d006ab02e",
      "21e0f9095e46491cb357c195ea500bc3",
      "76c34e5c3a404a6f8f21c3b998db378c",
      "5dcec100a2f144e697c481ea1657c4a5",
      "54d117be8dd94bc2a70475986d294ead",
      "a5b6d64b0af84da6880311768ab4ec82",
      "451e0d54f99643bd8e30a98ffaf8ea47",
      "6d46eecf62d44ed29dad31fe7ca78978",
      "8224567f3298439ca53aee75503c2493",
      "e63b7f1754804867bf57bd1f140e244a",
      "30fb779abd6540cd88faeed0e2edf029",
      "57de45f71a584f67b39c001e7a9d4095",
      "42b556c818f7487891d859b5790e5803",
      "bb97e09680c54aa6969ea2bb82787fd7",
      "52ad7badf91741cc981122fd8e93e5b2",
      "2ac10d5330034b979ce67f74001e942a",
      "ca821f126c494d6ba4d964363a543338",
      "e309799d45584fada77be47ddc76fd31",
      "38fbc8f174f944f6a1a3c62bfc69df81",
      "f46516bc58c54e1d999d95104b523256",
      "fa728c79a8e144508fbd8798f2a9edc8",
      "9da6956aa51b4e4cb72c0075a88f359f",
      "5ad090a92218454a853f1236d5625169"
     ]
    },
    "executionInfo": {
     "elapsed": 49152,
     "status": "ok",
     "timestamp": 1754371180029,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -480
    },
    "id": "hgK3MyMzeN87",
    "outputId": "9b319264-702d-4302-8834-bc74e8398e3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Tuning on group2 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda62a38677a4cdda50388a7caf7fd1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0931133b1f1349c8a30e4fad42413903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch   1 — Loss: 0.695294\n",
      "  Epoch   2 — Loss: 0.690311\n",
      "  Epoch   3 — Loss: 0.686781\n",
      "  Epoch   4 — Loss: 0.683695\n",
      "  Epoch   5 — Loss: 0.680532\n",
      "  Epoch   6 — Loss: 0.677432\n",
      "  Epoch   7 — Loss: 0.673674\n",
      "  Epoch   8 — Loss: 0.670924\n",
      "  Epoch   9 — Loss: 0.667297\n",
      "  Epoch  10 — Loss: 0.663321\n",
      "  Epoch  11 — Loss: 0.659341\n",
      "  Epoch  12 — Loss: 0.656053\n",
      "  Epoch  13 — Loss: 0.652369\n",
      "  Epoch  14 — Loss: 0.649097\n",
      "  Epoch  15 — Loss: 0.646523\n",
      "  Epoch  16 — Loss: 0.643264\n",
      "  Epoch  17 — Loss: 0.640164\n",
      "  Epoch  18 — Loss: 0.637079\n",
      "  Epoch  19 — Loss: 0.634835\n",
      "  Epoch  20 — Loss: 0.632097\n",
      "  Epoch  21 — Loss: 0.628944\n",
      "  Epoch  22 — Loss: 0.627141\n",
      "  Epoch  23 — Loss: 0.624293\n",
      "  Epoch  24 — Loss: 0.622324\n",
      "  Epoch  25 — Loss: 0.619247\n",
      "  Epoch  26 — Loss: 0.617194\n",
      "  Epoch  27 — Loss: 0.616070\n",
      "  Epoch  28 — Loss: 0.613386\n",
      "  Epoch  29 — Loss: 0.610550\n",
      "  Epoch  30 — Loss: 0.608573\n",
      "  Epoch  31 — Loss: 0.606734\n",
      "  Epoch  32 — Loss: 0.604313\n",
      "  Epoch  33 — Loss: 0.603007\n",
      "  Epoch  34 — Loss: 0.600166\n",
      "  Epoch  35 — Loss: 0.598964\n",
      "  Epoch  36 — Loss: 0.596334\n",
      "  Epoch  37 — Loss: 0.594565\n",
      "  Epoch  38 — Loss: 0.593146\n",
      "  Epoch  39 — Loss: 0.590893\n",
      "  Epoch  40 — Loss: 0.591029\n",
      "  ↳ small improvement (Δ=-1.36e-04) stale 1/3\n",
      "  Epoch  41 — Loss: 0.587696\n",
      "  Epoch  42 — Loss: 0.585578\n",
      "  Epoch  43 — Loss: 0.583293\n",
      "  Epoch  44 — Loss: 0.581616\n",
      "  Epoch  45 — Loss: 0.580484\n",
      "  Epoch  46 — Loss: 0.579191\n",
      "  Epoch  47 — Loss: 0.576897\n",
      "  Epoch  48 — Loss: 0.575386\n",
      "  Epoch  49 — Loss: 0.573590\n",
      "  Epoch  50 — Loss: 0.571432\n",
      "  Epoch  51 — Loss: 0.570600\n",
      "  Epoch  52 — Loss: 0.567916\n",
      "  Epoch  53 — Loss: 0.567236\n",
      "  Epoch  54 — Loss: 0.564623\n",
      "  Epoch  55 — Loss: 0.563159\n",
      "  Epoch  56 — Loss: 0.561693\n",
      "  Epoch  57 — Loss: 0.560274\n",
      "  Epoch  58 — Loss: 0.558442\n",
      "  Epoch  59 — Loss: 0.557523\n",
      "  Epoch  60 — Loss: 0.554756\n",
      "  Epoch  61 — Loss: 0.553289\n",
      "  Epoch  62 — Loss: 0.551618\n",
      "  Epoch  63 — Loss: 0.550595\n",
      "  Epoch  64 — Loss: 0.548198\n",
      "  Epoch  65 — Loss: 0.547579\n",
      "  Epoch  66 — Loss: 0.545768\n",
      "  Epoch  67 — Loss: 0.544815\n",
      "  Epoch  68 — Loss: 0.542337\n",
      "  Epoch  69 — Loss: 0.543348\n",
      "  ↳ small improvement (Δ=-1.01e-03) stale 1/3\n",
      "  Epoch  70 — Loss: 0.539318\n",
      "  Epoch  71 — Loss: 0.537977\n",
      "  Epoch  72 — Loss: 0.536742\n",
      "  Epoch  73 — Loss: 0.535046\n",
      "  Epoch  74 — Loss: 0.534001\n",
      "  Epoch  75 — Loss: 0.532362\n",
      "  Epoch  76 — Loss: 0.531538\n",
      "  Epoch  77 — Loss: 0.529143\n",
      "  Epoch  78 — Loss: 0.527368\n",
      "  Epoch  79 — Loss: 0.526146\n",
      "  Epoch  80 — Loss: 0.525430\n",
      "  Epoch  81 — Loss: 0.522614\n",
      "  Epoch  82 — Loss: 0.521932\n",
      "  Epoch  83 — Loss: 0.519587\n",
      "  Epoch  84 — Loss: 0.519403\n",
      "  Epoch  85 — Loss: 0.518240\n",
      "  Epoch  86 — Loss: 0.515989\n",
      "  Epoch  87 — Loss: 0.513886\n",
      "  Epoch  88 — Loss: 0.513905\n",
      "  ↳ small improvement (Δ=-1.89e-05) stale 1/3\n",
      "  Epoch  89 — Loss: 0.512328\n",
      "  Epoch  90 — Loss: 0.510668\n",
      "  Epoch  91 — Loss: 0.508691\n",
      "  Epoch  92 — Loss: 0.507688\n",
      "  Epoch  93 — Loss: 0.505999\n",
      "  Epoch  94 — Loss: 0.505243\n",
      "  Epoch  95 — Loss: 0.503078\n",
      "  Epoch  96 — Loss: 0.502321\n",
      "  Epoch  97 — Loss: 0.500112\n",
      "  Epoch  98 — Loss: 0.497891\n",
      "  Epoch  99 — Loss: 0.499540\n",
      "  ↳ small improvement (Δ=-1.65e-03) stale 1/3\n",
      "  Epoch 100 — Loss: 0.495524\n",
      "Metrics for group2:\n",
      "  Accuracy    :  60.31%\n",
      "  Micro-F1    :  60.31%\n",
      "  Macro-F1    :  59.28%\n",
      "  Weighted-F1 :  59.81%\n",
      "  Macro-Recall:  59.41%\n",
      "\n",
      "=== Tuning on group3 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42deeae6ca214b5c8b51c5c927676e80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22108553f9d44c5dbf5e427107daeeb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch   1 — Loss: 0.687900\n",
      "  Epoch   2 — Loss: 0.685328\n",
      "  Epoch   3 — Loss: 0.683175\n",
      "  Epoch   4 — Loss: 0.681184\n",
      "  Epoch   5 — Loss: 0.678728\n",
      "  Epoch   6 — Loss: 0.676334\n",
      "  Epoch   7 — Loss: 0.673273\n",
      "  Epoch   8 — Loss: 0.669825\n",
      "  Epoch   9 — Loss: 0.666936\n",
      "  Epoch  10 — Loss: 0.663457\n",
      "  Epoch  11 — Loss: 0.660308\n",
      "  Epoch  12 — Loss: 0.656733\n",
      "  Epoch  13 — Loss: 0.653184\n",
      "  Epoch  14 — Loss: 0.650052\n",
      "  Epoch  15 — Loss: 0.646600\n",
      "  Epoch  16 — Loss: 0.643048\n",
      "  Epoch  17 — Loss: 0.639781\n",
      "  Epoch  18 — Loss: 0.636634\n",
      "  Epoch  19 — Loss: 0.632952\n",
      "  Epoch  20 — Loss: 0.629421\n",
      "  Epoch  21 — Loss: 0.626524\n",
      "  Epoch  22 — Loss: 0.622782\n",
      "  Epoch  23 — Loss: 0.619146\n",
      "  Epoch  24 — Loss: 0.616555\n",
      "  Epoch  25 — Loss: 0.613369\n",
      "  Epoch  26 — Loss: 0.611466\n",
      "  Epoch  27 — Loss: 0.607057\n",
      "  Epoch  28 — Loss: 0.603855\n",
      "  Epoch  29 — Loss: 0.601269\n",
      "  Epoch  30 — Loss: 0.597699\n",
      "  Epoch  31 — Loss: 0.595580\n",
      "  Epoch  32 — Loss: 0.592054\n",
      "  Epoch  33 — Loss: 0.589703\n",
      "  Epoch  34 — Loss: 0.586998\n",
      "  Epoch  35 — Loss: 0.583503\n",
      "  Epoch  36 — Loss: 0.581109\n",
      "  Epoch  37 — Loss: 0.578496\n",
      "  Epoch  38 — Loss: 0.577263\n",
      "  Epoch  39 — Loss: 0.573474\n",
      "  Epoch  40 — Loss: 0.571058\n",
      "  Epoch  41 — Loss: 0.568431\n",
      "  Epoch  42 — Loss: 0.565746\n",
      "  Epoch  43 — Loss: 0.563069\n",
      "  Epoch  44 — Loss: 0.561180\n",
      "  Epoch  45 — Loss: 0.559025\n",
      "  Epoch  46 — Loss: 0.556707\n",
      "  Epoch  47 — Loss: 0.554153\n",
      "  Epoch  48 — Loss: 0.552280\n",
      "  Epoch  49 — Loss: 0.550111\n",
      "  Epoch  50 — Loss: 0.547975\n",
      "  Epoch  51 — Loss: 0.545625\n",
      "  Epoch  52 — Loss: 0.542894\n",
      "  Epoch  53 — Loss: 0.541125\n",
      "  Epoch  54 — Loss: 0.539189\n",
      "  Epoch  55 — Loss: 0.536572\n",
      "  Epoch  56 — Loss: 0.534675\n",
      "  Epoch  57 — Loss: 0.532789\n",
      "  Epoch  58 — Loss: 0.530488\n",
      "  Epoch  59 — Loss: 0.529248\n",
      "  Epoch  60 — Loss: 0.526987\n",
      "  Epoch  61 — Loss: 0.525003\n",
      "  Epoch  62 — Loss: 0.522183\n",
      "  Epoch  63 — Loss: 0.521818\n",
      "  Epoch  64 — Loss: 0.518232\n",
      "  Epoch  65 — Loss: 0.517396\n",
      "  Epoch  66 — Loss: 0.515963\n",
      "  Epoch  67 — Loss: 0.514025\n",
      "  Epoch  68 — Loss: 0.510778\n",
      "  Epoch  69 — Loss: 0.510988\n",
      "  ↳ small improvement (Δ=-2.11e-04) stale 1/3\n",
      "  Epoch  70 — Loss: 0.508650\n",
      "  Epoch  71 — Loss: 0.506475\n",
      "  Epoch  72 — Loss: 0.505005\n",
      "  Epoch  73 — Loss: 0.502725\n",
      "  Epoch  74 — Loss: 0.500289\n",
      "  Epoch  75 — Loss: 0.498917\n",
      "  Epoch  76 — Loss: 0.496533\n",
      "  Epoch  77 — Loss: 0.495304\n",
      "  Epoch  78 — Loss: 0.493725\n",
      "  Epoch  79 — Loss: 0.493112\n",
      "  Epoch  80 — Loss: 0.491181\n",
      "  Epoch  81 — Loss: 0.489805\n",
      "  Epoch  82 — Loss: 0.487382\n",
      "  Epoch  83 — Loss: 0.485705\n",
      "  Epoch  84 — Loss: 0.483799\n",
      "  Epoch  85 — Loss: 0.483081\n",
      "  Epoch  86 — Loss: 0.480784\n",
      "  Epoch  87 — Loss: 0.478397\n",
      "  Epoch  88 — Loss: 0.478044\n",
      "  Epoch  89 — Loss: 0.475626\n",
      "  Epoch  90 — Loss: 0.474585\n",
      "  Epoch  91 — Loss: 0.473424\n",
      "  Epoch  92 — Loss: 0.470851\n",
      "  Epoch  93 — Loss: 0.470399\n",
      "  Epoch  94 — Loss: 0.468370\n",
      "  Epoch  95 — Loss: 0.468086\n",
      "  Epoch  96 — Loss: 0.465157\n",
      "  Epoch  97 — Loss: 0.464200\n",
      "  Epoch  98 — Loss: 0.461759\n",
      "  Epoch  99 — Loss: 0.460174\n",
      "  Epoch 100 — Loss: 0.458832\n",
      "Metrics for group3:\n",
      "  Accuracy    :  58.44%\n",
      "  Micro-F1    :  58.44%\n",
      "  Macro-F1    :  57.35%\n",
      "  Weighted-F1 :  58.20%\n",
      "  Macro-Recall:  57.34%\n",
      "\n",
      "=== Tuning on group4 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21e0f9095e46491cb357c195ea500bc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42b556c818f7487891d859b5790e5803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch   1 — Loss: 0.691186\n",
      "  Epoch   2 — Loss: 0.686917\n",
      "  Epoch   3 — Loss: 0.684729\n",
      "  Epoch   4 — Loss: 0.683142\n",
      "  Epoch   5 — Loss: 0.681488\n",
      "  Epoch   6 — Loss: 0.679624\n",
      "  Epoch   7 — Loss: 0.677680\n",
      "  Epoch   8 — Loss: 0.675755\n",
      "  Epoch   9 — Loss: 0.673161\n",
      "  Epoch  10 — Loss: 0.670975\n",
      "  Epoch  11 — Loss: 0.668877\n",
      "  Epoch  12 — Loss: 0.666188\n",
      "  Epoch  13 — Loss: 0.663342\n",
      "  Epoch  14 — Loss: 0.661270\n",
      "  Epoch  15 — Loss: 0.658217\n",
      "  Epoch  16 — Loss: 0.655543\n",
      "  Epoch  17 — Loss: 0.652600\n",
      "  Epoch  18 — Loss: 0.650092\n",
      "  Epoch  19 — Loss: 0.647826\n",
      "  Epoch  20 — Loss: 0.644532\n",
      "  Epoch  21 — Loss: 0.642669\n",
      "  Epoch  22 — Loss: 0.639380\n",
      "  Epoch  23 — Loss: 0.636801\n",
      "  Epoch  24 — Loss: 0.633805\n",
      "  Epoch  25 — Loss: 0.631374\n",
      "  Epoch  26 — Loss: 0.629801\n",
      "  Epoch  27 — Loss: 0.626540\n",
      "  Epoch  28 — Loss: 0.623956\n",
      "  Epoch  29 — Loss: 0.621356\n",
      "  Epoch  30 — Loss: 0.619273\n",
      "  Epoch  31 — Loss: 0.616577\n",
      "  Epoch  32 — Loss: 0.614549\n",
      "  Epoch  33 — Loss: 0.612039\n",
      "  Epoch  34 — Loss: 0.609169\n",
      "  Epoch  35 — Loss: 0.607419\n",
      "  Epoch  36 — Loss: 0.605493\n",
      "  Epoch  37 — Loss: 0.603688\n",
      "  Epoch  38 — Loss: 0.600380\n",
      "  Epoch  39 — Loss: 0.598067\n",
      "  Epoch  40 — Loss: 0.596189\n",
      "  Epoch  41 — Loss: 0.595061\n",
      "  Epoch  42 — Loss: 0.592279\n",
      "  Epoch  43 — Loss: 0.589933\n",
      "  Epoch  44 — Loss: 0.587525\n",
      "  Epoch  45 — Loss: 0.587569\n",
      "  ↳ small improvement (Δ=-4.37e-05) stale 1/3\n",
      "  Epoch  46 — Loss: 0.584471\n",
      "  Epoch  47 — Loss: 0.581990\n",
      "  Epoch  48 — Loss: 0.579561\n",
      "  Epoch  49 — Loss: 0.579050\n",
      "  Epoch  50 — Loss: 0.575810\n",
      "  Epoch  51 — Loss: 0.574669\n",
      "  Epoch  52 — Loss: 0.572045\n",
      "  Epoch  53 — Loss: 0.569992\n",
      "  Epoch  54 — Loss: 0.569149\n",
      "  Epoch  55 — Loss: 0.566552\n",
      "  Epoch  56 — Loss: 0.565470\n",
      "  Epoch  57 — Loss: 0.563613\n",
      "  Epoch  58 — Loss: 0.561562\n",
      "  Epoch  59 — Loss: 0.559453\n",
      "  Epoch  60 — Loss: 0.557874\n",
      "  Epoch  61 — Loss: 0.555038\n",
      "  Epoch  62 — Loss: 0.554669\n",
      "  Epoch  63 — Loss: 0.551786\n",
      "  Epoch  64 — Loss: 0.549274\n",
      "  Epoch  65 — Loss: 0.548951\n",
      "  Epoch  66 — Loss: 0.546858\n",
      "  Epoch  67 — Loss: 0.544849\n",
      "  Epoch  68 — Loss: 0.543078\n",
      "  Epoch  69 — Loss: 0.542400\n",
      "  Epoch  70 — Loss: 0.539611\n",
      "  Epoch  71 — Loss: 0.537677\n",
      "  Epoch  72 — Loss: 0.536027\n",
      "  Epoch  73 — Loss: 0.534277\n",
      "  Epoch  74 — Loss: 0.532709\n",
      "  Epoch  75 — Loss: 0.530427\n",
      "  Epoch  76 — Loss: 0.529037\n",
      "  Epoch  77 — Loss: 0.527237\n",
      "  Epoch  78 — Loss: 0.525193\n",
      "  Epoch  79 — Loss: 0.524499\n",
      "  Epoch  80 — Loss: 0.522554\n",
      "  Epoch  81 — Loss: 0.520765\n",
      "  Epoch  82 — Loss: 0.519262\n",
      "  Epoch  83 — Loss: 0.517433\n",
      "  Epoch  84 — Loss: 0.515780\n",
      "  Epoch  85 — Loss: 0.514135\n",
      "  Epoch  86 — Loss: 0.512283\n",
      "  Epoch  87 — Loss: 0.511118\n",
      "  Epoch  88 — Loss: 0.508961\n",
      "  Epoch  89 — Loss: 0.507516\n",
      "  Epoch  90 — Loss: 0.505179\n",
      "  Epoch  91 — Loss: 0.503444\n",
      "  Epoch  92 — Loss: 0.502606\n",
      "  Epoch  93 — Loss: 0.500631\n",
      "  Epoch  94 — Loss: 0.499405\n",
      "  Epoch  95 — Loss: 0.497442\n",
      "  Epoch  96 — Loss: 0.496351\n",
      "  Epoch  97 — Loss: 0.495378\n",
      "  Epoch  98 — Loss: 0.493542\n",
      "  Epoch  99 — Loss: 0.491243\n",
      "  Epoch 100 — Loss: 0.490218\n",
      "Metrics for group4:\n",
      "  Accuracy    :  53.12%\n",
      "  Micro-F1    :  53.12%\n",
      "  Macro-F1    :  51.04%\n",
      "  Weighted-F1 :  52.18%\n",
      "  Macro-Recall:  51.46%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "results = {}\n",
    "\n",
    "for group_name, labeled_group in [\n",
    "    (\"group2\", labeled_group2),\n",
    "    (\"group3\", labeled_group3),\n",
    "    (\"group4\", labeled_group4),\n",
    "]:\n",
    "    print(f\"=== Tuning on {group_name} ===\")\n",
    "    df_all = pd.concat(labeled_group.values(), ignore_index=True)\n",
    "    texts = df_all[\"title\"].tolist()\n",
    "    labels = df_all[\"label\"].astype(int).tolist()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        texts, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "    )\n",
    "\n",
    "    emb_train = model.encode(\n",
    "        X_train, device=device, batch_size=16,\n",
    "        convert_to_tensor=True, show_progress_bar=True\n",
    "    )\n",
    "    del X_train\n",
    "    emb_test = model.encode(\n",
    "        X_test, device=device, batch_size=16,\n",
    "        convert_to_tensor=True, show_progress_bar=True\n",
    "    )\n",
    "    del X_test\n",
    "\n",
    "    d = emb_train.size(1)\n",
    "    head = nn.Sequential(\n",
    "        nn.Linear(d, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 2)\n",
    "    ).to(device)\n",
    "\n",
    "    trainable = list(head.parameters()) + [\n",
    "        p for p in model.parameters() if p.requires_grad\n",
    "    ]\n",
    "    optimizer = torch.optim.Adam(trainable, lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_ds = TensorDataset(emb_train, torch.tensor(y_train, device=device))\n",
    "    train_dl = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "\n",
    "    best_prev_loss = float(\"inf\")\n",
    "    tol = 1e-4\n",
    "    patience = 3\n",
    "    stale_epochs = 0\n",
    "    max_epochs = 100\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        head.train()\n",
    "        epoch_loss = 0.0\n",
    "        for emb_b, lab_b in train_dl:\n",
    "            optimizer.zero_grad()\n",
    "            logits = head(emb_b)\n",
    "            loss = criterion(logits, lab_b)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * emb_b.size(0)\n",
    "        avg_loss = epoch_loss / len(train_ds)\n",
    "        print(f\"  Epoch {epoch:>3} — Loss: {avg_loss:.6f}\")\n",
    "        improvement = best_prev_loss - avg_loss\n",
    "        if improvement < tol:\n",
    "            stale_epochs += 1\n",
    "            print(f\"  ↳ small improvement (Δ={improvement:.2e}) stale {stale_epochs}/{patience}\")\n",
    "        else:\n",
    "            stale_epochs = 0\n",
    "            best_prev_loss = avg_loss\n",
    "        if stale_epochs >= patience:\n",
    "            print(f\"  ↳ Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "    head.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = head(emb_test.to(device))\n",
    "        preds = logits.argmax(dim=1).cpu().tolist()\n",
    "\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    micro_f1 = f1_score(y_test, preds, average=\"micro\")\n",
    "    macro_f1 = f1_score(y_test, preds, average=\"macro\")\n",
    "    weighted_f1 = f1_score(y_test, preds, average=\"weighted\")\n",
    "    macro_recall = recall_score(y_test, preds, average=\"macro\")\n",
    "\n",
    "    print(f\"Metrics for {group_name}:\")\n",
    "    print(f\"  Accuracy    : {acc*100:6.2f}%\")\n",
    "    print(f\"  Micro-F1    : {micro_f1*100:6.2f}%\")\n",
    "    print(f\"  Macro-F1    : {macro_f1*100:6.2f}%\")\n",
    "    print(f\"  Weighted-F1 : {weighted_f1*100:6.2f}%\")\n",
    "    print(f\"  Macro-Recall: {macro_recall*100:6.2f}%\\n\")\n",
    "\n",
    "    results[group_name] = head\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542,
     "referenced_widgets": [
      "eb34690a001e4640abbdb52fe82f82a6",
      "8dee6d87e7e0491097db7bb5b3221adc",
      "d5567b6ec68a45ccbfa4e8ed8d1c31c9",
      "aaaf0f276a8947faa7faad2702a08e97",
      "73a79c308fc24742a34f8fe5c995308c",
      "f169001723364d81a6494729e9d35c8f",
      "8a617bf773284faa9a6b8eeab24dc897",
      "c579d5ec31034af794397f4ff802f47d",
      "29cf64ddea124132ab67324737fd7b20",
      "8778fa66c9f749f5a01efe0bd9d4d43d",
      "9bfe2c43af0e414c8b608d4ac6becd20",
      "c1ef345c73f34f5e9229ad9158b9a953",
      "0cb41c7b5f4047428fb4e674976f54cf",
      "bc8de7baaac34ba0bc01415ea8cd2c9d",
      "e835a2531bc347478d07c7ad3c8c6c61",
      "91db1cc1de714c11a4e2713b2b389c94",
      "9050b874ac774a558e791960ec9eee52",
      "273b116f43d342fd9c31bba8a1ed861f",
      "12b89f0684a24a238ce73134b23ec3a7",
      "03ffcc83c5e74447a679a519ca9b4556",
      "61d648c8e8644293b64a1bf4c9f46b80",
      "688b8dd6f19a4ddcbafc2e8933b5a88d",
      "5e800711db6c497eb77aeb189e1067e1",
      "9b8bd6970c204f088611e6f419dc0ffd",
      "564251fcdc444554bca2bb6fd170dfa3",
      "aa5e5331f7e84c5e86dc6182e7f2afdc",
      "3581b59c8a8e4b8ca44e588688c287fe",
      "dba5bd7c21af4e7c99fdda7842b41b55",
      "3c536c3bbc2a4d6ca23945468bf9e692",
      "67d4803e4f754c0ea5a2a44f099e9664",
      "2c74f96789e1490e863aa62e832af3a1",
      "5e2becbfc58b4ae6baefc3f6b5fc1682",
      "4a0bddf10f60484f848e17e3280367f7"
     ]
    },
    "executionInfo": {
     "elapsed": 6811,
     "status": "ok",
     "timestamp": 1754371188422,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -480
    },
    "id": "VgVybGWywS4j",
    "outputId": "9125acfb-c393-4101-cbfa-1ae4863248ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Recomputing embeddings for group2 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb34690a001e4640abbdb52fe82f82a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics after fine-tuning embeddings for group2:\n",
      "  Accuracy    :  60.31%\n",
      "  Micro-F1    :  60.31%\n",
      "  Macro-F1    :  59.28%\n",
      "  Weighted-F1 :  59.81%\n",
      "  Macro-Recall:  59.41%\n",
      "\n",
      "=== Recomputing embeddings for group3 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1ef345c73f34f5e9229ad9158b9a953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics after fine-tuning embeddings for group3:\n",
      "  Accuracy    :  58.44%\n",
      "  Micro-F1    :  58.44%\n",
      "  Macro-F1    :  57.35%\n",
      "  Weighted-F1 :  58.20%\n",
      "  Macro-Recall:  57.34%\n",
      "\n",
      "=== Recomputing embeddings for group4 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e800711db6c497eb77aeb189e1067e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics after fine-tuning embeddings for group4:\n",
      "  Accuracy    :  53.12%\n",
      "  Micro-F1    :  53.12%\n",
      "  Macro-F1    :  51.04%\n",
      "  Weighted-F1 :  52.18%\n",
      "  Macro-Recall:  51.46%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for group_name, labeled_group in [\n",
    "    (\"group2\", labeled_group2),\n",
    "    (\"group3\", labeled_group3),\n",
    "    (\"group4\", labeled_group4),\n",
    "]:\n",
    "    print(f\"=== Recomputing embeddings for {group_name} ===\")\n",
    "    df_all = pd.concat(labeled_group.values(), ignore_index=True)\n",
    "    texts = df_all[\"title\"].tolist()\n",
    "    labels = df_all[\"label\"].astype(int).tolist()\n",
    "    _, X_test, _, y_test = train_test_split(\n",
    "        texts, labels,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=labels\n",
    "    )\n",
    "\n",
    "    emb_test_updated = model.encode(\n",
    "        X_test,\n",
    "        device=device,\n",
    "        batch_size=16,\n",
    "        convert_to_tensor=True,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "\n",
    "    head = results[group_name]\n",
    "    head.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = head(emb_test_updated.to(device))\n",
    "        preds  = logits.argmax(dim=1).cpu().tolist()\n",
    "\n",
    "    acc          = accuracy_score(y_test, preds)\n",
    "    micro_f1     = f1_score(y_test, preds, average=\"micro\")\n",
    "    macro_f1     = f1_score(y_test, preds, average=\"macro\")\n",
    "    weighted_f1  = f1_score(y_test, preds, average=\"weighted\")\n",
    "    macro_recall = recall_score(y_test, preds, average=\"macro\")\n",
    "\n",
    "    print(f\"Metrics after fine-tuning embeddings for {group_name}:\")\n",
    "    print(f\"  Accuracy    : {acc*100:6.2f}%\")\n",
    "    print(f\"  Micro-F1    : {micro_f1*100:6.2f}%\")\n",
    "    print(f\"  Macro-F1    : {macro_f1*100:6.2f}%\")\n",
    "    print(f\"  Weighted-F1 : {weighted_f1*100:6.2f}%\")\n",
    "    print(f\"  Macro-Recall: {macro_recall*100:6.2f}%\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423,
     "referenced_widgets": [
      "f0ce103d8ecd409ab783c4242689d08a",
      "8736a7e10dee4117a45ba8ed873325f7",
      "873d7951b09940db8b7b898906530cf7",
      "dc3c81da3d72482b9719465a6034733f",
      "bbec2ae130eb4bebb19706ddf55da064",
      "11a1ef10488f4649b698c3f6334acc13",
      "8b1806846e464185a35f4fd4562519c8",
      "68b76efd088045a5b625e232d050595c",
      "bb763363aa5044a2bf02f00c4973f025",
      "13b8c0770c654cd39a3e463f39ba88f9",
      "da66c70caadb4a04ad6415012ca90e5c",
      "72a8c75040fa41e8ab9fd9e7bdb51e7f",
      "a100f26ae17a40eab640c89307c6e65c",
      "171f0fea89fa4b82a96ab384fc04ba32",
      "3849f568c47140caa98625f5fdaef55e",
      "d170b3627fd3445190dd072e05bbae48",
      "7a9683119a364ca09e46ddd0e7869d18",
      "08bc96c3bfbb4924857f36d99062bbfe",
      "518fc053efaa4f64bb0b7d51ab2a213f",
      "1d25604247f34bc2b0b7da680dc0f1a8",
      "361740f8d85f40a4a0e76c2d05e5ee1d",
      "89fc7934df644d5b8eec98108d43a6dc",
      "672605d511f04e3c8cc660a765846daa",
      "dbc12110c97f4e759c11c62f5e39ba42",
      "345e0566a039429586e157bee4c7dee6",
      "52668d7a3cf6462fbf8721a6bcc36128",
      "cb71c751e26a4e3ba400b2c330e01e61",
      "d6f9f3932fcd4a9ea31c283eb869f38c",
      "ed1d1a4e7b8244b9a4391b8f2857f50e",
      "c3d83fd80cb048e196d342f3e06af7d3",
      "50834559347943409a0df6d3d5ce8747",
      "f9c37c6bc6fb4de5a3417f5f2309de5b",
      "a8517cee46e049389d6f84ea8a19b66a",
      "c47c7e8ff31b4c88a87e2410b6cde96c",
      "066eda9027bf4566bfb97991b77b4fe2",
      "68814c4dfc424741ab1a73c5f1a47b17",
      "be849374bf4b4d17860a283803c412a8",
      "16de65bb57f14199b4bbb67d2e1e77f1",
      "c3d02485fc3d487b9328faca6fbad49d",
      "1c9dab6504c74dcaabf8bdca43ca409e",
      "4d895a758bf342fb9710e7092e848546",
      "1d4a0403afc84800bdad30b40539e667",
      "b8e03e61e96e41d3a4eea85cb11f39a2",
      "b5cf6c305d4c4faf946c39ee17d383dc",
      "3514bb526748414f921b429d48f6b224",
      "e6e3353a85f14a7d827130b6f2ec6c6b",
      "f62bdda1303f459d9079046198b34a3e",
      "167e25fb53c64139bd741b693104fcae",
      "e990b6eb33214a3593069bd7d613171b",
      "fdb83561b306479999ff09fd367a2df6",
      "6be710b4d50e4d728e2d9c7838e1db70",
      "5a17318b542e44bf8d17c8024764d73a",
      "1dd874555ff541179cbc092116f1dabd",
      "14f26aee2dad4bbda354fd51967c01e5",
      "d5cd300631a043d294ae32af5190560a",
      "d258ec94e4d84a17ba7fba6246e96d1a",
      "065a607ad2b34fc5a1c12ccb67faafb8",
      "3338a02cc3414929abb30a17648cde91",
      "5a40983e0fe94d99a878830df14e5a54",
      "70ca71963ec14e3bace397abeeee4959",
      "4959d02a43ee465f894cbe0aab3834e5",
      "db89e0b994af45b08f845dfe8d1ad314",
      "599e477fdb07427ba1b867b1dc29aad4",
      "522cbc1d433b41c78a920110cf5f12de",
      "9d770655ce1d4f6393b756dff973a02d",
      "1e59255beb6e42368b8795562a9c266c",
      "166ef8fbcc1f4bf297563201640ee46b",
      "029e899704a04a3295a8d1c3f6313c2d",
      "efb4984fdc554ec3960d2be87707c53c",
      "1f22b85efe2641b082525f594ed02424",
      "dadeff80270645289a9f9db55f6a8a21",
      "81830af32451417f861f679d61981e87",
      "55a9f1d5791b480b80c8bd743dbb25fb",
      "1d30307962a241848a7cd39c3584f5a7",
      "cc5daa83929645b2b22759091a636166",
      "3a195fe74b4c4527939b2093234b5512",
      "0250311fe8b44b2b9131d9a3d6d2e977",
      "c18571a318f4484cb6adb4741d83ac16",
      "3b6e4e3613524a848c5ce88f020283c2",
      "7ca973ab880b4399b7aed7ef78ecc290",
      "456a74bd420e47029f4ee226ae8c6506",
      "c58b89bbade942fd84060dac7e1ffda5",
      "c55c2ecd163a41348ff1bcf77529b809",
      "72097cb76b024a7582b87d29bdfc1bea",
      "2995c074d78d431d8e1b7a1d15af0638",
      "e18fdf1bb96943b180e1681946426f94",
      "e06025d7c3864c0a8b0bb27746f0fcbf",
      "d716711a5395437e8f57fed22499397f",
      "9f5c24c3cf48422abf34cb4c55400dfc",
      "0e3e4f4325564ae3a6e9f6e14157370b",
      "ca2c74f3311740b9b67a7aeb51ea3e16",
      "b543fe8369fc48618c8ac21bb87e6d2d",
      "187f75c382d0409aaf6c30f5f0748332",
      "59cd2cf7d6814013ba6092e3e1728d6d",
      "03039690675540b7b75d1e20f2ef785c",
      "fa30697189bf401184cacad648d4c252",
      "55608ac6653744eea8acfac18b8de88c",
      "72051ee41c534ade910e528b0cc7e9bd",
      "4ca7bc95e85d455194890e57e3733fe5",
      "4230684430c34914bc5df8eb39a55b5a",
      "081a341ecdf3496fbf7fc79da909eb6d",
      "bcca47f3d0df49469a8ab5741698d55b",
      "f5a9f40ad9f946ef94574db0f7c34838",
      "43e47cce7a6945b0a1e24508054a941f",
      "0fc327784de14e29acdef3a1acd8fe3e",
      "c3bf4c137e2e42e9ade686d9cdb797d5",
      "7ee5679cac8d4b0e98ffb4b0d10bfef0",
      "d2c3ab7ea541442bbd106453b530c8fc",
      "f9e52a3bab86442d93f120fae65ce3db",
      "a9a40e4226294456b97788889f188c88",
      "06a4fd7503d14e0e851963741febad59",
      "2131d5c9b96740089b79f6fe3c940d72",
      "30ab29c68fd54b95b5c600c76be0d6b6",
      "c023bf755c794347bd4ef6df0353f5ac",
      "8c05fb61e92d48d7890b58512d420668",
      "f545ec8944ff4f6e9a72738bee88c856",
      "789789fca9904ccd930a44208fb4a551",
      "db195700eb3e404585597bd9f6f7c2a2",
      "f5ee7c92746d4439bebb296725771aa4",
      "e5d64f2f393742e9afee587da8ea7e4c",
      "26688e3549044873b3b30a36cc083332"
     ]
    },
    "executionInfo": {
     "elapsed": 10090,
     "status": "ok",
     "timestamp": 1754371367265,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -480
    },
    "id": "Y5hnV6_2o-S7",
    "outputId": "f8725360-021d-439f-a8e3-557cd00f24d5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0ce103d8ecd409ab783c4242689d08a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a8c75040fa41e8ab9fd9e7bdb51e7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/128 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "672605d511f04e3c8cc660a765846daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47c7e8ff31b4c88a87e2410b6cde96c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_xlm-roberta_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3514bb526748414f921b429d48f6b224",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/690 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d258ec94e4d84a17ba7fba6246e96d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "166ef8fbcc1f4bf297563201640ee46b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c18571a318f4484cb6adb4741d83ac16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f5c24c3cf48422abf34cb4c55400dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4230684430c34914bc5df8eb39a55b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06a4fd7503d14e0e851963741febad59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/271 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Found transformer blocks under: encoder.layer\n",
      "Un-froze 49 parameters total.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_1 = SentenceTransformer(\"intfloat/multilingual-e5-large-instruct\")\n",
    "model_1.to(device)\n",
    "\n",
    "for p in model_1.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "transformer = model_1._modules['0']\n",
    "hf_model    = transformer.auto_model\n",
    "for p in hf_model.get_input_embeddings().parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "blocks = None\n",
    "for name, module in hf_model.named_modules():\n",
    "    if isinstance(module, torch.nn.ModuleList):\n",
    "        blocks = module\n",
    "        print(f\"→ Found transformer blocks under: {name}\")\n",
    "        break\n",
    "if blocks is None:\n",
    "    raise RuntimeError(\"Could not find transformer blocks\")\n",
    "\n",
    "for block in blocks[-3:]:\n",
    "    for p in block.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "print(f\"Un-froze {sum(p.requires_grad for p in model_1.parameters())} parameters total.\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "6effd41b13be43b386081090f98906f5",
      "7590222e2aa04ad7a26655ed587dea97",
      "cc6553a7e3fc451791001f8cb2310d98",
      "8a519a8133624c8eab359b06d0b7dd1e",
      "2f1397527eea43eca7be7ee1ccc767a7",
      "a29497771a3e4d758e9807c3a44b9a4e",
      "562eb9e79a9448408939c7b838117945",
      "f824c4936c814f32985a363ced662227",
      "d3025d85924645649e6345ee3f586637",
      "17932ffdfbfe4a83bbc7eb2c47522c4a",
      "12619671666c46dfa9e25e579139d4da",
      "da2542258a82453ead57b411a64b118a",
      "30d0f21734334c0b866d5cd8de568609",
      "1887ffa05fbf4e80872ddb3ee82863b6",
      "a735861a5c4f4977ad75342a17e54010",
      "3a1ba019d73f43929f93fd0382ecb84c",
      "3078181e84224ba8aa5875746896c5cb",
      "13d8ab17ed2547a0b3bc9cd20a1eb9dc",
      "a8b06f78398d402c97db4980abd20bf7",
      "700fdde7ccfc471cb46fadff3f0f0cd0",
      "f55cd18b84f64fde80699c0746370ace",
      "692cdf1008aa458eaccd738bc79c9e67",
      "793fcd26a1144839ab7d2ea04cf036cc",
      "c9a8e9b6efc44a279999ace5e3c061b2",
      "d83bd3f773294193a1c7e16e6986ac63",
      "a281aeabd7fe4769959ee838539e64f5",
      "9267e7f320af40a89287e83ad9f0a6f3",
      "3ddb2383503544f1bc726f2e48fc693b",
      "60ac13e086b7471c8c085cfec9bdafa1",
      "8ae0a1ff1ee64c4080d88262334df48d",
      "16c8a3dbb6464732967fca0caef9a630",
      "3cb752e2d16e4d31b6a650df8aab786b",
      "fafd68a0cba74826b59ab6ffdfc3ddf9",
      "1ab9704eec5b49efbd942b5588485b31",
      "57f6bab0096e4b13a690ad43ce37a611",
      "349b7904a90b4890972bdd5c3c79488a",
      "332a4a4824694f0b88a597b8c05afc43",
      "269456f2c6af4caca029cc853326baf7",
      "3648facfea9d4c2091f822a76ab485f8",
      "c781fc896656483baf0b96854fb6594a",
      "5d9fab1038ac40d4b0202cd33e1a27a2",
      "ae019895e58f44879f8d6b361e88356e",
      "c5492964f78648a491a93e7612b460a1",
      "17fc63b352ae44daa3e46c80ef711d29",
      "d588932303924d74b3a44fca81bd7650",
      "5a75fb9128f041d1870f446445129eae",
      "39a7500689bb40a8910231776e9bfdae",
      "6d453f2bcdd24d8b9fc76c767ff64568",
      "0d2b581a9bc34b5c86da400bf490b46a",
      "7beb17a06c174f30a3d1bc7469960640",
      "9f37fbc752684cfd9ffb3ddd0744656b",
      "e260579a761448d6af7d246fabd4d5d5",
      "c4aa20b96b204eeb9fa29d4723d086f7",
      "0defee025701410aaa60917492360ce4",
      "9e72429448c34acaab7ed24a7cc21e4e",
      "f8f114e81bdd40a6803ec366a0e7ce9c",
      "79d5c61fe1b249ed9a6c9c190ff88d30",
      "76553eff9e534a95bbe8248de7adc5fc",
      "4453eee1ba454743b654ecc91e291b57",
      "f9705381f7314f57ae7d8d27f7a32bbe",
      "4ba1b703ee344e6fbfeeb97275767181",
      "d9f30711b4df4680ab3b4bbe719ab325",
      "a1b6e01afa274befbcda5731ac07ba79",
      "3f02c28da99042178ecf5a965275709f",
      "145401ccb5da4131b8ac39470a5c4b65",
      "cccff35d122f43199b33f241c57131a5"
     ]
    },
    "executionInfo": {
     "elapsed": 41908,
     "status": "ok",
     "timestamp": 1754371409177,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -480
    },
    "id": "PHVf22WQIveC",
    "outputId": "addd4dd6-74f1-4d97-fcef-055414af4945"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Tuning on group2 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6effd41b13be43b386081090f98906f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da2542258a82453ead57b411a64b118a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch   1 — Loss: 0.693612\n",
      "  Epoch   2 — Loss: 0.689324\n",
      "  Epoch   3 — Loss: 0.688075\n",
      "  Epoch   4 — Loss: 0.687093\n",
      "  Epoch   5 — Loss: 0.686177\n",
      "  Epoch   6 — Loss: 0.685764\n",
      "  Epoch   7 — Loss: 0.683765\n",
      "  Epoch   8 — Loss: 0.682302\n",
      "  Epoch   9 — Loss: 0.680985\n",
      "  Epoch  10 — Loss: 0.680023\n",
      "  Epoch  11 — Loss: 0.677966\n",
      "  Epoch  12 — Loss: 0.676489\n",
      "  Epoch  13 — Loss: 0.675168\n",
      "  Epoch  14 — Loss: 0.673455\n",
      "  Epoch  15 — Loss: 0.671443\n",
      "  Epoch  16 — Loss: 0.670294\n",
      "  Epoch  17 — Loss: 0.668089\n",
      "  Epoch  18 — Loss: 0.666471\n",
      "  Epoch  19 — Loss: 0.664802\n",
      "  Epoch  20 — Loss: 0.663031\n",
      "  Epoch  21 — Loss: 0.661499\n",
      "  Epoch  22 — Loss: 0.660181\n",
      "  Epoch  23 — Loss: 0.658979\n",
      "  Epoch  24 — Loss: 0.657566\n",
      "  Epoch  25 — Loss: 0.655376\n",
      "  Epoch  26 — Loss: 0.654151\n",
      "  Epoch  27 — Loss: 0.652940\n",
      "  Epoch  28 — Loss: 0.651008\n",
      "  Epoch  29 — Loss: 0.649806\n",
      "  Epoch  30 — Loss: 0.649135\n",
      "  Epoch  31 — Loss: 0.647601\n",
      "  Epoch  32 — Loss: 0.647085\n",
      "  Epoch  33 — Loss: 0.645406\n",
      "  Epoch  34 — Loss: 0.645294\n",
      "  Epoch  35 — Loss: 0.643256\n",
      "  Epoch  36 — Loss: 0.642014\n",
      "  Epoch  37 — Loss: 0.641002\n",
      "  Epoch  38 — Loss: 0.639590\n",
      "  Epoch  39 — Loss: 0.639324\n",
      "  Epoch  40 — Loss: 0.638843\n",
      "  Epoch  41 — Loss: 0.638081\n",
      "  Epoch  42 — Loss: 0.635700\n",
      "  Epoch  43 — Loss: 0.635304\n",
      "  Epoch  44 — Loss: 0.635006\n",
      "  Epoch  45 — Loss: 0.633642\n",
      "  Epoch  46 — Loss: 0.632000\n",
      "  Epoch  47 — Loss: 0.631920\n",
      "  ↳ small improvement (Δ=8.02e-05) stale 1/3\n",
      "  Epoch  48 — Loss: 0.631782\n",
      "  Epoch  49 — Loss: 0.630346\n",
      "  Epoch  50 — Loss: 0.631065\n",
      "  ↳ small improvement (Δ=-7.19e-04) stale 1/3\n",
      "  Epoch  51 — Loss: 0.627895\n",
      "  Epoch  52 — Loss: 0.629269\n",
      "  ↳ small improvement (Δ=-1.37e-03) stale 1/3\n",
      "  Epoch  53 — Loss: 0.627648\n",
      "  Epoch  54 — Loss: 0.626508\n",
      "  Epoch  55 — Loss: 0.625987\n",
      "  Epoch  56 — Loss: 0.624425\n",
      "  Epoch  57 — Loss: 0.624378\n",
      "  ↳ small improvement (Δ=4.66e-05) stale 1/3\n",
      "  Epoch  58 — Loss: 0.622509\n",
      "  Epoch  59 — Loss: 0.623180\n",
      "  ↳ small improvement (Δ=-6.71e-04) stale 1/3\n",
      "  Epoch  60 — Loss: 0.621491\n",
      "  Epoch  61 — Loss: 0.620029\n",
      "  Epoch  62 — Loss: 0.619839\n",
      "  Epoch  63 — Loss: 0.618361\n",
      "  Epoch  64 — Loss: 0.618573\n",
      "  ↳ small improvement (Δ=-2.12e-04) stale 1/3\n",
      "  Epoch  65 — Loss: 0.617319\n",
      "  Epoch  66 — Loss: 0.616908\n",
      "  Epoch  67 — Loss: 0.615324\n",
      "  Epoch  68 — Loss: 0.615416\n",
      "  ↳ small improvement (Δ=-9.19e-05) stale 1/3\n",
      "  Epoch  69 — Loss: 0.614715\n",
      "  Epoch  70 — Loss: 0.613425\n",
      "  Epoch  71 — Loss: 0.612954\n",
      "  Epoch  72 — Loss: 0.613429\n",
      "  ↳ small improvement (Δ=-4.74e-04) stale 1/3\n",
      "  Epoch  73 — Loss: 0.611726\n",
      "  Epoch  74 — Loss: 0.610144\n",
      "  Epoch  75 — Loss: 0.609731\n",
      "  Epoch  76 — Loss: 0.608772\n",
      "  Epoch  77 — Loss: 0.608827\n",
      "  ↳ small improvement (Δ=-5.46e-05) stale 1/3\n",
      "  Epoch  78 — Loss: 0.606779\n",
      "  Epoch  79 — Loss: 0.606782\n",
      "  ↳ small improvement (Δ=-3.07e-06) stale 1/3\n",
      "  Epoch  80 — Loss: 0.606356\n",
      "  Epoch  81 — Loss: 0.606176\n",
      "  Epoch  82 — Loss: 0.604258\n",
      "  Epoch  83 — Loss: 0.603416\n",
      "  Epoch  84 — Loss: 0.602561\n",
      "  Epoch  85 — Loss: 0.602240\n",
      "  Epoch  86 — Loss: 0.603140\n",
      "  ↳ small improvement (Δ=-9.00e-04) stale 1/3\n",
      "  Epoch  87 — Loss: 0.601222\n",
      "  Epoch  88 — Loss: 0.600310\n",
      "  Epoch  89 — Loss: 0.599749\n",
      "  Epoch  90 — Loss: 0.597552\n",
      "  Epoch  91 — Loss: 0.597731\n",
      "  ↳ small improvement (Δ=-1.79e-04) stale 1/3\n",
      "  Epoch  92 — Loss: 0.596176\n",
      "  Epoch  93 — Loss: 0.595348\n",
      "  Epoch  94 — Loss: 0.596303\n",
      "  ↳ small improvement (Δ=-9.54e-04) stale 1/3\n",
      "  Epoch  95 — Loss: 0.593931\n",
      "  Epoch  96 — Loss: 0.593861\n",
      "  ↳ small improvement (Δ=7.01e-05) stale 1/3\n",
      "  Epoch  97 — Loss: 0.592989\n",
      "  Epoch  98 — Loss: 0.592889\n",
      "  ↳ small improvement (Δ=9.97e-05) stale 1/3\n",
      "  Epoch  99 — Loss: 0.591385\n",
      "  Epoch 100 — Loss: 0.590298\n",
      "Metrics for group2:\n",
      "  Accuracy    :  59.06%\n",
      "  Micro-F1    :  59.06%\n",
      "  Macro-F1    :  58.72%\n",
      "  Weighted-F1 :  59.03%\n",
      "  Macro-Recall:  58.71%\n",
      "\n",
      "=== Tuning on group3 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "793fcd26a1144839ab7d2ea04cf036cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ab9704eec5b49efbd942b5588485b31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch   1 — Loss: 0.687592\n",
      "  Epoch   2 — Loss: 0.685561\n",
      "  Epoch   3 — Loss: 0.684552\n",
      "  Epoch   4 — Loss: 0.683831\n",
      "  Epoch   5 — Loss: 0.683113\n",
      "  Epoch   6 — Loss: 0.682567\n",
      "  Epoch   7 — Loss: 0.681761\n",
      "  Epoch   8 — Loss: 0.681027\n",
      "  Epoch   9 — Loss: 0.680065\n",
      "  Epoch  10 — Loss: 0.679377\n",
      "  Epoch  11 — Loss: 0.678324\n",
      "  Epoch  12 — Loss: 0.677723\n",
      "  Epoch  13 — Loss: 0.676814\n",
      "  Epoch  14 — Loss: 0.675840\n",
      "  Epoch  15 — Loss: 0.674690\n",
      "  Epoch  16 — Loss: 0.673258\n",
      "  Epoch  17 — Loss: 0.672322\n",
      "  Epoch  18 — Loss: 0.671264\n",
      "  Epoch  19 — Loss: 0.670121\n",
      "  Epoch  20 — Loss: 0.668504\n",
      "  Epoch  21 — Loss: 0.667509\n",
      "  Epoch  22 — Loss: 0.665938\n",
      "  Epoch  23 — Loss: 0.665338\n",
      "  Epoch  24 — Loss: 0.663546\n",
      "  Epoch  25 — Loss: 0.663152\n",
      "  Epoch  26 — Loss: 0.661455\n",
      "  Epoch  27 — Loss: 0.659267\n",
      "  Epoch  28 — Loss: 0.658394\n",
      "  Epoch  29 — Loss: 0.656864\n",
      "  Epoch  30 — Loss: 0.656133\n",
      "  Epoch  31 — Loss: 0.654904\n",
      "  Epoch  32 — Loss: 0.652934\n",
      "  Epoch  33 — Loss: 0.651922\n",
      "  Epoch  34 — Loss: 0.650366\n",
      "  Epoch  35 — Loss: 0.648965\n",
      "  Epoch  36 — Loss: 0.647754\n",
      "  Epoch  37 — Loss: 0.646278\n",
      "  Epoch  38 — Loss: 0.645081\n",
      "  Epoch  39 — Loss: 0.643865\n",
      "  Epoch  40 — Loss: 0.643328\n",
      "  Epoch  41 — Loss: 0.641012\n",
      "  Epoch  42 — Loss: 0.640434\n",
      "  Epoch  43 — Loss: 0.639144\n",
      "  Epoch  44 — Loss: 0.638023\n",
      "  Epoch  45 — Loss: 0.636794\n",
      "  Epoch  46 — Loss: 0.635944\n",
      "  Epoch  47 — Loss: 0.634312\n",
      "  Epoch  48 — Loss: 0.631984\n",
      "  Epoch  49 — Loss: 0.631295\n",
      "  Epoch  50 — Loss: 0.629146\n",
      "  Epoch  51 — Loss: 0.628944\n",
      "  Epoch  52 — Loss: 0.627493\n",
      "  Epoch  53 — Loss: 0.627984\n",
      "  ↳ small improvement (Δ=-4.91e-04) stale 1/3\n",
      "  Epoch  54 — Loss: 0.625444\n",
      "  Epoch  55 — Loss: 0.623933\n",
      "  Epoch  56 — Loss: 0.622774\n",
      "  Epoch  57 — Loss: 0.619755\n",
      "  Epoch  58 — Loss: 0.620723\n",
      "  ↳ small improvement (Δ=-9.69e-04) stale 1/3\n",
      "  Epoch  59 — Loss: 0.618237\n",
      "  Epoch  60 — Loss: 0.617712\n",
      "  Epoch  61 — Loss: 0.616448\n",
      "  Epoch  62 — Loss: 0.614734\n",
      "  Epoch  63 — Loss: 0.614033\n",
      "  Epoch  64 — Loss: 0.611769\n",
      "  Epoch  65 — Loss: 0.612369\n",
      "  ↳ small improvement (Δ=-6.00e-04) stale 1/3\n",
      "  Epoch  66 — Loss: 0.610121\n",
      "  Epoch  67 — Loss: 0.608633\n",
      "  Epoch  68 — Loss: 0.609896\n",
      "  ↳ small improvement (Δ=-1.26e-03) stale 1/3\n",
      "  Epoch  69 — Loss: 0.606766\n",
      "  Epoch  70 — Loss: 0.605505\n",
      "  Epoch  71 — Loss: 0.605132\n",
      "  Epoch  72 — Loss: 0.603202\n",
      "  Epoch  73 — Loss: 0.602308\n",
      "  Epoch  74 — Loss: 0.600733\n",
      "  Epoch  75 — Loss: 0.599524\n",
      "  Epoch  76 — Loss: 0.597766\n",
      "  Epoch  77 — Loss: 0.597173\n",
      "  Epoch  78 — Loss: 0.595941\n",
      "  Epoch  79 — Loss: 0.595142\n",
      "  Epoch  80 — Loss: 0.594297\n",
      "  Epoch  81 — Loss: 0.594641\n",
      "  ↳ small improvement (Δ=-3.44e-04) stale 1/3\n",
      "  Epoch  82 — Loss: 0.591732\n",
      "  Epoch  83 — Loss: 0.591170\n",
      "  Epoch  84 — Loss: 0.589809\n",
      "  Epoch  85 — Loss: 0.588874\n",
      "  Epoch  86 — Loss: 0.588754\n",
      "  Epoch  87 — Loss: 0.586695\n",
      "  Epoch  88 — Loss: 0.584888\n",
      "  Epoch  89 — Loss: 0.584848\n",
      "  ↳ small improvement (Δ=4.01e-05) stale 1/3\n",
      "  Epoch  90 — Loss: 0.584019\n",
      "  Epoch  91 — Loss: 0.582080\n",
      "  Epoch  92 — Loss: 0.580863\n",
      "  Epoch  93 — Loss: 0.579431\n",
      "  Epoch  94 — Loss: 0.578062\n",
      "  Epoch  95 — Loss: 0.577972\n",
      "  ↳ small improvement (Δ=8.99e-05) stale 1/3\n",
      "  Epoch  96 — Loss: 0.577740\n",
      "  Epoch  97 — Loss: 0.573909\n",
      "  Epoch  98 — Loss: 0.574881\n",
      "  ↳ small improvement (Δ=-9.72e-04) stale 1/3\n",
      "  Epoch  99 — Loss: 0.572736\n",
      "  Epoch 100 — Loss: 0.572861\n",
      "  ↳ small improvement (Δ=-1.25e-04) stale 1/3\n",
      "Metrics for group3:\n",
      "  Accuracy    :  57.50%\n",
      "  Micro-F1    :  57.50%\n",
      "  Macro-F1    :  56.60%\n",
      "  Weighted-F1 :  57.38%\n",
      "  Macro-Recall:  56.59%\n",
      "\n",
      "=== Tuning on group4 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d588932303924d74b3a44fca81bd7650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f114e81bdd40a6803ec366a0e7ce9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch   1 — Loss: 0.689470\n",
      "  Epoch   2 — Loss: 0.687361\n",
      "  Epoch   3 — Loss: 0.686586\n",
      "  Epoch   4 — Loss: 0.685887\n",
      "  Epoch   5 — Loss: 0.685318\n",
      "  Epoch   6 — Loss: 0.684577\n",
      "  Epoch   7 — Loss: 0.683731\n",
      "  Epoch   8 — Loss: 0.682822\n",
      "  Epoch   9 — Loss: 0.682252\n",
      "  Epoch  10 — Loss: 0.681144\n",
      "  Epoch  11 — Loss: 0.680427\n",
      "  Epoch  12 — Loss: 0.679706\n",
      "  Epoch  13 — Loss: 0.678462\n",
      "  Epoch  14 — Loss: 0.677267\n",
      "  Epoch  15 — Loss: 0.675957\n",
      "  Epoch  16 — Loss: 0.675048\n",
      "  Epoch  17 — Loss: 0.674182\n",
      "  Epoch  18 — Loss: 0.672852\n",
      "  Epoch  19 — Loss: 0.672648\n",
      "  Epoch  20 — Loss: 0.670701\n",
      "  Epoch  21 — Loss: 0.669128\n",
      "  Epoch  22 — Loss: 0.667729\n",
      "  Epoch  23 — Loss: 0.666627\n",
      "  Epoch  24 — Loss: 0.665430\n",
      "  Epoch  25 — Loss: 0.664121\n",
      "  Epoch  26 — Loss: 0.662417\n",
      "  Epoch  27 — Loss: 0.661937\n",
      "  Epoch  28 — Loss: 0.660749\n",
      "  Epoch  29 — Loss: 0.659288\n",
      "  Epoch  30 — Loss: 0.658571\n",
      "  Epoch  31 — Loss: 0.656871\n",
      "  Epoch  32 — Loss: 0.655430\n",
      "  Epoch  33 — Loss: 0.655025\n",
      "  Epoch  34 — Loss: 0.653184\n",
      "  Epoch  35 — Loss: 0.653173\n",
      "  ↳ small improvement (Δ=1.11e-05) stale 1/3\n",
      "  Epoch  36 — Loss: 0.651818\n",
      "  Epoch  37 — Loss: 0.651380\n",
      "  Epoch  38 — Loss: 0.649743\n",
      "  Epoch  39 — Loss: 0.648562\n",
      "  Epoch  40 — Loss: 0.646899\n",
      "  Epoch  41 — Loss: 0.646273\n",
      "  Epoch  42 — Loss: 0.645151\n",
      "  Epoch  43 — Loss: 0.646016\n",
      "  ↳ small improvement (Δ=-8.64e-04) stale 1/3\n",
      "  Epoch  44 — Loss: 0.644032\n",
      "  Epoch  45 — Loss: 0.642630\n",
      "  Epoch  46 — Loss: 0.641461\n",
      "  Epoch  47 — Loss: 0.640573\n",
      "  Epoch  48 — Loss: 0.640188\n",
      "  Epoch  49 — Loss: 0.639598\n",
      "  Epoch  50 — Loss: 0.637929\n",
      "  Epoch  51 — Loss: 0.637417\n",
      "  Epoch  52 — Loss: 0.636096\n",
      "  Epoch  53 — Loss: 0.635634\n",
      "  Epoch  54 — Loss: 0.634524\n",
      "  Epoch  55 — Loss: 0.633107\n",
      "  Epoch  56 — Loss: 0.632483\n",
      "  Epoch  57 — Loss: 0.632146\n",
      "  Epoch  58 — Loss: 0.630510\n",
      "  Epoch  59 — Loss: 0.629952\n",
      "  Epoch  60 — Loss: 0.629089\n",
      "  Epoch  61 — Loss: 0.628545\n",
      "  Epoch  62 — Loss: 0.629262\n",
      "  ↳ small improvement (Δ=-7.18e-04) stale 1/3\n",
      "  Epoch  63 — Loss: 0.626847\n",
      "  Epoch  64 — Loss: 0.626989\n",
      "  ↳ small improvement (Δ=-1.42e-04) stale 1/3\n",
      "  Epoch  65 — Loss: 0.624859\n",
      "  Epoch  66 — Loss: 0.624133\n",
      "  Epoch  67 — Loss: 0.624332\n",
      "  ↳ small improvement (Δ=-1.99e-04) stale 1/3\n",
      "  Epoch  68 — Loss: 0.623460\n",
      "  Epoch  69 — Loss: 0.622546\n",
      "  Epoch  70 — Loss: 0.621124\n",
      "  Epoch  71 — Loss: 0.621202\n",
      "  ↳ small improvement (Δ=-7.78e-05) stale 1/3\n",
      "  Epoch  72 — Loss: 0.619365\n",
      "  Epoch  73 — Loss: 0.618942\n",
      "  Epoch  74 — Loss: 0.618575\n",
      "  Epoch  75 — Loss: 0.617779\n",
      "  Epoch  76 — Loss: 0.616871\n",
      "  Epoch  77 — Loss: 0.616407\n",
      "  Epoch  78 — Loss: 0.615662\n",
      "  Epoch  79 — Loss: 0.615605\n",
      "  ↳ small improvement (Δ=5.74e-05) stale 1/3\n",
      "  Epoch  80 — Loss: 0.614044\n",
      "  Epoch  81 — Loss: 0.613573\n",
      "  Epoch  82 — Loss: 0.613115\n",
      "  Epoch  83 — Loss: 0.613164\n",
      "  ↳ small improvement (Δ=-4.89e-05) stale 1/3\n",
      "  Epoch  84 — Loss: 0.610903\n",
      "  Epoch  85 — Loss: 0.610242\n",
      "  Epoch  86 — Loss: 0.611013\n",
      "  ↳ small improvement (Δ=-7.71e-04) stale 1/3\n",
      "  Epoch  87 — Loss: 0.608705\n",
      "  Epoch  88 — Loss: 0.608120\n",
      "  Epoch  89 — Loss: 0.607093\n",
      "  Epoch  90 — Loss: 0.607333\n",
      "  ↳ small improvement (Δ=-2.40e-04) stale 1/3\n",
      "  Epoch  91 — Loss: 0.606559\n",
      "  Epoch  92 — Loss: 0.605575\n",
      "  Epoch  93 — Loss: 0.605000\n",
      "  Epoch  94 — Loss: 0.604304\n",
      "  Epoch  95 — Loss: 0.603860\n",
      "  Epoch  96 — Loss: 0.603450\n",
      "  Epoch  97 — Loss: 0.603092\n",
      "  Epoch  98 — Loss: 0.601834\n",
      "  Epoch  99 — Loss: 0.601225\n",
      "  Epoch 100 — Loss: 0.600159\n",
      "Metrics for group4:\n",
      "  Accuracy    :  55.94%\n",
      "  Micro-F1    :  55.94%\n",
      "  Macro-F1    :  53.52%\n",
      "  Weighted-F1 :  54.71%\n",
      "  Macro-Recall:  54.06%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = {}\n",
    "\n",
    "for group_name, labeled_group in [\n",
    "    (\"group2\", labeled_group2),\n",
    "    (\"group3\", labeled_group3),\n",
    "    (\"group4\", labeled_group4),\n",
    "]:\n",
    "    print(f\"=== Tuning on {group_name} ===\")\n",
    "    df_all = pd.concat(labeled_group.values(), ignore_index=True)\n",
    "    texts = df_all[\"title\"].tolist()\n",
    "    labels = df_all[\"label\"].astype(int).tolist()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        texts, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "    )\n",
    "\n",
    "    emb_train = model_1.encode(\n",
    "        X_train, device=device, batch_size=16,\n",
    "        convert_to_tensor=True, show_progress_bar=True\n",
    "    )\n",
    "    del X_train\n",
    "    emb_test = model_1.encode(\n",
    "        X_test, device=device, batch_size=16,\n",
    "        convert_to_tensor=True, show_progress_bar=True\n",
    "    )\n",
    "    del X_test\n",
    "\n",
    "    d = emb_train.size(1)\n",
    "    head = nn.Sequential(\n",
    "        nn.Linear(d, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 2)\n",
    "    ).to(device)\n",
    "\n",
    "    trainable = list(head.parameters()) + [\n",
    "        p for p in model_1.parameters() if p.requires_grad\n",
    "    ]\n",
    "    optimizer = torch.optim.Adam(trainable, lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_ds = TensorDataset(emb_train, torch.tensor(y_train, device=device))\n",
    "    train_dl = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "\n",
    "    best_prev_loss = float(\"inf\")\n",
    "    tol = 1e-4\n",
    "    patience = 3\n",
    "    stale_epochs = 0\n",
    "    max_epochs = 100\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        head.train()\n",
    "        epoch_loss = 0.0\n",
    "        for emb_b, lab_b in train_dl:\n",
    "            optimizer.zero_grad()\n",
    "            logits = head(emb_b)\n",
    "            loss = criterion(logits, lab_b)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * emb_b.size(0)\n",
    "        avg_loss = epoch_loss / len(train_ds)\n",
    "        print(f\"  Epoch {epoch:>3} — Loss: {avg_loss:.6f}\")\n",
    "        improvement = best_prev_loss - avg_loss\n",
    "        if improvement < tol:\n",
    "            stale_epochs += 1\n",
    "            print(f\"  ↳ small improvement (Δ={improvement:.2e}) stale {stale_epochs}/{patience}\")\n",
    "        else:\n",
    "            stale_epochs = 0\n",
    "            best_prev_loss = avg_loss\n",
    "        if stale_epochs >= patience:\n",
    "            print(f\"  ↳ Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "    head.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = head(emb_test.to(device))\n",
    "        preds = logits.argmax(dim=1).cpu().tolist()\n",
    "\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    micro_f1 = f1_score(y_test, preds, average=\"micro\")\n",
    "    macro_f1 = f1_score(y_test, preds, average=\"macro\")\n",
    "    weighted_f1 = f1_score(y_test, preds, average=\"weighted\")\n",
    "    macro_recall = recall_score(y_test, preds, average=\"macro\")\n",
    "\n",
    "    print(f\"Metrics for {group_name}:\")\n",
    "    print(f\"  Accuracy    : {acc*100:6.2f}%\")\n",
    "    print(f\"  Micro-F1    : {micro_f1*100:6.2f}%\")\n",
    "    print(f\"  Macro-F1    : {macro_f1*100:6.2f}%\")\n",
    "    print(f\"  Weighted-F1 : {weighted_f1*100:6.2f}%\")\n",
    "    print(f\"  Macro-Recall: {macro_recall*100:6.2f}%\\n\")\n",
    "\n",
    "    results[group_name] = head\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 615,
     "referenced_widgets": [
      "0a2c290668664f8ebd6ac650e057bf73",
      "93cc4e9be75547aaa00df0dc10e4fbb7",
      "ceac5ffcb24f4a59bec8c14e6eb32cd5",
      "5006a4ad834342b3909e99b2a4452a2e",
      "0a63b4e7b6c24e508d400254af5f3bc9",
      "1b283aee6651496dbbce406628cfefa0",
      "e8fb9205212c486b8e55534e09fb81ea",
      "e44cda5f46b44486bea55584f0807e9d",
      "96a70ab282084112b8b14cf5132d8177",
      "baba24a15e9a4625be6716dd3cdaf9eb",
      "cb91a6eff604402bb877dd16cd2415b4",
      "f799a12390bd4ac295f5611eca18a10d",
      "7012829facf24643811c75803bfe1506",
      "3a72de3834694b71b8ed4b0739070cbd",
      "08042bfbeda74595b164029522373a30",
      "d766d5537a50473eaed3ee5edb954e54",
      "9403c5fa56cf42aeb5b8de55b95a6a6d",
      "f4f63249be264eda806e5d12b4d387e6",
      "5cbd1c12b21b4b8cabc7aa7ff6ea7b42",
      "f634d7bcfdce409fb019145369c66066",
      "b7282c1f09fe4846a60fb20ed8b90ee3",
      "7d1893d2484447c58a4bfed40bccc07a",
      "17f99f4438b744c0ada561927e608c60",
      "1384d19ecc784ae78778057c4ebd6986",
      "e19c7043daf541f9b248aa96e5a5d9d7",
      "d66d4d6971b044bb80768f679da89a4d",
      "4e0212e9c2a644fbb22e15f47336a9d2",
      "199491e543394466b809d87979fe4110",
      "dc5f87473c9946a9acf5900236f4f7cb",
      "aa41aac53eee45ebbf42e10d6441b62a",
      "06a441829e9f44919fd0d4682ae40974",
      "10db87d263c64c3a860c5ad320ff6fd8",
      "9ce74b11f9a84ac5b973be6186780a7d"
     ]
    },
    "executionInfo": {
     "elapsed": 1608,
     "status": "ok",
     "timestamp": 1754371495438,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -480
    },
    "id": "pKTwipUxyC-q",
    "outputId": "693e401a-b498-4a03-dde8-9abc6ba9e008"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Re-evaluating group2 with updated weights ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a2c290668664f8ebd6ac650e057bf73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy    :  59.06%\n",
      "  Micro-F1    :  59.06%\n",
      "  Macro-F1    :  58.72%\n",
      "  Weighted-F1 :  59.03%\n",
      "  Macro-Recall:  58.71%\n",
      "\n",
      "=== Re-evaluating group3 with updated weights ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f799a12390bd4ac295f5611eca18a10d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy    :  57.50%\n",
      "  Micro-F1    :  57.50%\n",
      "  Macro-F1    :  56.60%\n",
      "  Weighted-F1 :  57.38%\n",
      "  Macro-Recall:  56.59%\n",
      "\n",
      "=== Re-evaluating group4 with updated weights ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17f99f4438b744c0ada561927e608c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy    :  55.94%\n",
      "  Micro-F1    :  55.94%\n",
      "  Macro-F1    :  53.52%\n",
      "  Weighted-F1 :  54.71%\n",
      "  Macro-Recall:  54.06%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Re-evaluate using the fine-tuned embedding layers + trained heads\n",
    "for group_name, labeled_group in [\n",
    "    (\"group2\", labeled_group2),\n",
    "    (\"group3\", labeled_group3),\n",
    "    (\"group4\", labeled_group4),\n",
    "]:\n",
    "    print(f\"=== Re-evaluating {group_name} with updated weights ===\")\n",
    "    df_all = pd.concat(labeled_group.values(), ignore_index=True)\n",
    "    texts  = df_all[\"title\"].tolist()\n",
    "    labels = df_all[\"label\"].astype(int).tolist()\n",
    "    _, X_test, _, y_test = train_test_split(\n",
    "        texts, labels,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=labels\n",
    "    )\n",
    "    emb_test_updated = model_1.encode(\n",
    "        X_test,\n",
    "        device=device,\n",
    "        batch_size=16,\n",
    "        convert_to_tensor=True,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    head = results[group_name]\n",
    "    head.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = head(emb_test_updated.to(device))\n",
    "        preds  = logits.argmax(dim=1).cpu().tolist()\n",
    "    acc          = accuracy_score(y_test, preds)\n",
    "    micro_f1     = f1_score(y_test, preds, average=\"micro\")\n",
    "    macro_f1     = f1_score(y_test, preds, average=\"macro\")\n",
    "    weighted_f1  = f1_score(y_test, preds, average=\"weighted\")\n",
    "    macro_recall = recall_score(y_test, preds, average=\"macro\")\n",
    "    print(f\"  Accuracy    : {acc*100:6.2f}%\")\n",
    "    print(f\"  Micro-F1    : {micro_f1*100:6.2f}%\")\n",
    "    print(f\"  Macro-F1    : {macro_f1*100:6.2f}%\")\n",
    "    print(f\"  Weighted-F1 : {weighted_f1*100:6.2f}%\")\n",
    "    print(f\"  Macro-Recall: {macro_recall*100:6.2f}%\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 576,
     "referenced_widgets": [
      "28ae7656e7ce4456855255c784f70636",
      "f590b71dfb5049b7a2b4921fbcea888e",
      "7bdd55071341463f88f4b78948dd9412",
      "7cebdd700e5b4e3f8abc9b8045b09f55",
      "29ed646cf0c94d84a62ad0940a62a91a",
      "4242c5d072684342bc7001c43b7e6f15",
      "2f5646801e314231b8dfdb3da7136d65",
      "ff0e944fdc7c4632bfd5f6d8f142a9e1",
      "69150db0fd48474680e64b9c0f1a3eeb",
      "98398298f91b4d40a0c0be1c43974efb",
      "8802ee489d844f1ea1df6a63430ddf96",
      "4d4146976c3243339a7262ef3e9904a0",
      "0417cb271f9f487f93454e78f7d70f9e",
      "c87d6025a19d475586f93f654e909b54",
      "e7e7bab4f0d846adab4fd989b87ebedb",
      "1f1e01b84e064a59839c2352d4bcfada",
      "1d2b40de10f84cfa854b95d0e222f863",
      "f163fb4f63104f6b87530b05762c1172",
      "82b5dd1fe9554c69978ae1d1bab0e7e3",
      "6ad18c80881c476f8ef9be23c439bc55",
      "b6838ba9660444df95067e20ff724275",
      "461aeb227dd840a4a193ce6be51fd596",
      "b2fe597396bb4f2fbca4685941d31786",
      "82b8b3580b794bdcaa7c4ba11622c097",
      "aa2143e6bfdc4169ab25a88e9c0abd12",
      "e2f2ca7c07894a82b57dcb52d0049672",
      "af05f51d72d54574a411e9c17b222510",
      "57fb2e746f8345e6a189868b432c4137",
      "305b88c28e964e31a31422b2cf03bddf",
      "1e31e243cca046268f058d4b3e87ad89",
      "bae4d96154fb4a0f983efb3a2d59ac0c",
      "9c57f0dc8b294f24a19a503673d7975f",
      "2936e14bd6764245b44a1ec8ffe59131",
      "4c51faeb1fa545089baa3d2c5006099d",
      "79a0347c7c1c41179a691c806f40d5f8",
      "3609c68ebf784306b746ed87a4a3e2b0",
      "62cddb7b1c6542d5b71358be82da8a72",
      "87e11ccbce334e70b7d8567eb67e0c53",
      "ed509d9664d54cab9444bdb60b33760c",
      "bdc8c843d09741a389017381bd29d640",
      "1532f59ee32c438090af4be3021d4776",
      "113d920a90fc46d59158d5285a13a4c2",
      "3b6a3f50250f44a1b108927898ef58ff",
      "c0c3291f1549469891f9b75dfadd5ff1",
      "00e231925f5b4029b5838e0d4434ebbc",
      "7be6d6f2fbe945b6a3a64991290734b0",
      "340470cf599646cba2d49971f07179fb",
      "60471d788dcd4ea0a4aa89d71c2f8c6c",
      "56f1d221d2c94d0c88e08f85c968122b",
      "c7848b93e75b4f9f95fe2aa032de9a11",
      "aa7c9b62b41f4f1390abe2c83ce4878d",
      "9633f9a9a01b41cab8d382e13d430fc2",
      "4a77bc94f917430cb056f71e764ef959",
      "6ad902b1fed84da686fcffd3b52ae98e",
      "2b37a7deeeb74c2683b3bdbb144de864",
      "ebea08a3ee9b43a7b7515335ff13dfd6",
      "7103fc6ad69e4a48a99662c1f2dabe0c",
      "c30317c1b5834a7299cbc568e1df3326",
      "a09b86c916814f879965e08f74404aa4",
      "e7b186119d804b8696a8b3d2d73c30f8",
      "c7d3bd8b1cdd4cdaabc70ef97e393cce",
      "5050437688d1403abfdaa197b955197d",
      "426c7e4b96f4480d90495f659d396153",
      "a402ad47a96649ccb3f1b14a91844284",
      "5d83003070a74fb6ac151ec72f96db39",
      "18476258aed94a6c8147f68e0137a97a",
      "c7600da5d63a4087a75e42971a3ddcd5",
      "981fc56c2e81467d880ba88a6f0cafeb",
      "6674b29bcdc24f73a7fa6f11f4710b16",
      "09797d483ace407c9be559f986b112a6",
      "48ee551c22774901a5ed5ad12ef816e3",
      "7faf87330cae4537aa1b8b6f7046e9fc",
      "f463ce522c5149a3959ea716167b4745",
      "29e39268be714227bb34e241efea5886",
      "377a399561f64448ac77a62c323b48b6",
      "9d1ababdf8d44f67af3604e850d84e5e",
      "e328419356d842528f848c892366371d",
      "5665d5a17eba44dead1a2c26790e9f10",
      "845f5d0cfead420c9ebf196df6dc9b32",
      "eb43d5336ba1449abd64e2ff71792d62",
      "374daa74551941c3a66ef5df111ee90f",
      "70de08ace1554032a5ff60e5c7d15f92",
      "d16271ba3aba4e7ab4e7ddec6d5e2f63",
      "9d32e3da69214ab09cd7c70144d31da7",
      "a09b4423e88c45ccbfe87a56227ba229",
      "96199740928f437fa9003fddfdc07976",
      "364fa472db764966961de4d9934b2965",
      "0517e5ca2f1847a5be500a25f8985f9e",
      "7a3fa7201e914e13ad5f0f9f2143666f",
      "4845771faccb4a55b0a9cdf508e34edb",
      "683057ed44b4479e9dd07460f7c5069a",
      "0550e4b55bae48ae9d7c9a49e52a6eb8",
      "34fdc472d91b456ca6ba967d1233adc5",
      "5721b75e0ef043e0aeb6f9c6f788cd0a",
      "b039a22e8bbf4593809c6758ca741dcd",
      "9585b4cae5b84d07a145728450b942d3",
      "ac0950fddeb7428cb9ac92c8f6ac6cf8",
      "b4698784429d48a4ab242847b1c144de",
      "a23a7d6828f044799cbf7b4519950a24",
      "753e6e56fb284d5c80c60f269be176a8",
      "98c7ed3d6da941908058466d315091f7",
      "ca8cc774325447b8b23246f1e15ca7e5",
      "de831c63b8184a7baa1f7c3b873826ee",
      "ed71a8f362004aa99873bc200f2c314e",
      "50563f2606d54c16b6c6cf44d65e5884",
      "b651c0e56b7b4799a044a639aa817ba8",
      "1d50c82e8de7424aa3cf885db688ec11",
      "94a7cf33d6764a61abcfd9436981a91d",
      "dbd0a2869c9f4ea7a112a40ae6acac63",
      "07c688ffa2364b81beb1511197f46f23",
      "a387aa1bb5e643ac95c64ec69c787e3a",
      "a722e56953fa4301800bfee7cf9f152d",
      "699b134d8f9a4737a54f77671f8b17ea",
      "4eafc786420d4b91940f52214f41e54e",
      "359376bae4fe42939fc0cc992794a600",
      "51a7605bff4e4f938e50a5af2a992469",
      "1491f1d6c6ac45a1b3e4b2c2af459446",
      "d3a80c302f3c425cb01452edd4b1e9ce",
      "ae7749d69fce47fcb16d15ddb3edefe8",
      "0e6757a3a1ea44538d8944827fa911f3",
      "d2d20909c86b43ea8a5f80574205cebe",
      "d32d50c6ce294a5a94195da2d83a94e7",
      "64f0df8814be419e83d3f2c163943105",
      "c23387fe7f6a45a394c23dc0790f6fd5",
      "34604a2dfc894baeb4fb997194be462f",
      "21971c24c055482b8ff3aac88f85691a",
      "c3072b698456497997e66dc0be58e9c7",
      "de767cb71d0d4816814ca9e56bc57b45",
      "4f6906ba65cd4d0e8f6bda513c9bcf58",
      "4fef7159678442e7b270a752aa5d8003",
      "6bc502e0be2f45b1a8a58d84f8308729",
      "9900763bdd0e45279263fa580ebe23f0",
      "3cf75855017f4c74b95de86451605815",
      "e1e60b084b664e5b9392b983294d6d71",
      "790fb557f70c47d9b76223fde394045b",
      "eb8ad7ed934f42d2a67903684bca203a",
      "1230dc1f300c4f3ea44c6377c3c8dff5",
      "930ead8f924749dda7b3b8bd42ae2f61",
      "ebec9ecc5e6046d38bd403f623e13ce8",
      "de70913d8be74a4e818b98a7de499211",
      "e9b7b5040d9f4ca7940f273a15c57d3d",
      "f978c8e9e87d473ab06fbd59a78ddec1",
      "625d7dede5984c7288cbfa977fea2846"
     ]
    },
    "executionInfo": {
     "elapsed": 104818,
     "status": "ok",
     "timestamp": 1754371647536,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -480
    },
    "id": "D-v47bTw4_Zf",
    "outputId": "496551ec-d21d-40ef-c932-2108285c21cc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28ae7656e7ce4456855255c784f70636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d4146976c3243339a7262ef3e9904a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/176 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2fe597396bb4f2fbca4685941d31786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c51faeb1fa545089baa3d2c5006099d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e231925f5b4029b5838e0d4434ebbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebea08a3ee9b43a7b7515335ff13dfd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/dangvantuan/bilingual_impl:\n",
      "- config.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7600da5d63a4087a75e42971a3ddcd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/dangvantuan/bilingual_impl:\n",
      "- modeling.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5665d5a17eba44dead1a2c26790e9f10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a3fa7201e914e13ad5f0f9f2143666f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "753e6e56fb284d5c80c60f269be176a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a387aa1bb5e643ac95c64ec69c787e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d32d50c6ce294a5a94195da2d83a94e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf75855017f4c74b95de86451605815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config-checkpoint.json:   0%|          | 0.00/297 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Found blocks under: encoder.layer\n",
      "Un-froze 49 params total.\n"
     ]
    }
   ],
   "source": [
    "model_2 = SentenceTransformer(\n",
    "    \"Lajavaness/bilingual-embedding-large\",\n",
    "    trust_remote_code=True)\n",
    "\n",
    "model_2.to(device)\n",
    "for p in model_2.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "transformer = model_2._modules['0']\n",
    "hf_model    = transformer.auto_model\n",
    "\n",
    "for p in hf_model.get_input_embeddings().parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "blocks = None\n",
    "for name, module in hf_model.named_modules():\n",
    "    if isinstance(module, torch.nn.ModuleList):\n",
    "        blocks = module\n",
    "        print(f\"→ Found blocks under: {name}\")\n",
    "        break\n",
    "if blocks is None:\n",
    "    raise RuntimeError(\"Could not find transformer blocks in hf_model\")\n",
    "\n",
    "for block in blocks[-3:]:\n",
    "    for p in block.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "print(f\"Un-froze {sum(p.requires_grad for p in model_1.parameters())} params total.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "40943c9c13804f6897496c11986a1aae",
      "11fffa69ec204ea1abce2cfda54be51d",
      "dee1e38916c74f1c89458400b48e151b",
      "d6866ef0296342469f342ba71ee42a69",
      "7dada27152224aa0b331b5e539d7465d",
      "82f964fe8b4741f4955f6858d6b0c1ee",
      "298b0ecc85064c18ad564377408f3849",
      "2b162478f8e1470d9a2b31f8cd2d517c",
      "5daae7901c92413189336db061dc117b",
      "907d05ed36384f1b95c990302681fc2a",
      "e4fc3bb5b4034891b422c5cbc56df305",
      "caa04922eea34711b9594936ee95f83d",
      "79fa19f32eb54b4ba3a3646dc0b3b6ca",
      "9f7f2100b7a94cae843b36e7562deea7",
      "b00f07920a9c4c61b805c68e62d3fbc0",
      "aeb9dc22710a4a47a0d5dcdf917ba422",
      "b4303a47777d45af92f10c0e9feba73c",
      "7dd7303788d840489baa0c5c8d99c08e",
      "658d9328032b482c8b33846d8e480f45",
      "041f6846a646458a9341feb481fcbc71",
      "ec364db9d70a4221964000a47173d276",
      "fb5b95cc41184930b960fc06cc62376d",
      "64e871c6d05740ffbaf824a121db353c",
      "bf313c639a8f4b53b813915b6ce8cb1f",
      "2367b8b62adc47bf96af11c61b69a922",
      "9e988b1f85384c718cc27afefcc34f07",
      "af8efb77b2e14712871dfc341d197f8c",
      "6309dec5cd6246d8a7ab2fa046b33cb4",
      "734c0c0ad24444a5a0f3717e1abc2774",
      "3d24a2375b6c410ab006b145bb42b403",
      "883be1619077487680c4743fc3b82723",
      "b34b5dc1aff94d8e8634612405f96b8b",
      "3b4554966cfd4404bbaa17903e3764ed",
      "c52f0655c16f4852b9dc4ae683db7b38",
      "3ca18de9cb524b50b17f9664a0a5ed2b",
      "6b4affdd8ea9462eb4f3c70ac3d92702",
      "2f62e7c1b74346869bce998f4a46ef17",
      "9b316034426147a0908c8afd2be71c52",
      "78e7dae00f0942bdba9d308793b2b0b1",
      "1d5004cb47bb46b19a21c778f55da96e",
      "de1cb1a2115641e1ba593bebc6a978d6",
      "6e53f2000bbe4082ba0cfb88f84a75c7",
      "15fdd1c94c5f40f893156f14312b5706",
      "6d4c247df41e44f58463c977bc40f161",
      "0f0853ae18504aaabe3b11976e643a55",
      "cff65b9678a0409fb323f80edd13be2a",
      "35fce118165c44619a789a1e66461387",
      "c6efe499194b446e9955020714324e72",
      "a66ab39707fa4ddbbb33a6d26253597a",
      "0c53ca5786c9400783033b33b0bb1678",
      "96440cf8abb6422bb729ec81c0323d95",
      "2bd6f86a8b75487e8a0412cde984b529",
      "3df3ca48e2294e46a5b9771675ac961d",
      "53985cb9e5014b6c89f2bd84e5005036",
      "6fa6ad7597b541e8bbc0b313b5613b4d",
      "d79769f4528c48e5bac839504aff864d",
      "3bc0e48e54e94280ada3ec80035e40d1",
      "b596f2dae365413389a9d85049853820",
      "035c594b777f4d11a890f391c84a1ef8",
      "3d7b7fde8a2f4e25b9913b54ba8c5939",
      "2786d7f4e9cc41238b83b9e558d050c7",
      "563a50318c954793ac7f6bbe4444df9f",
      "0a228426e89745e99b280e07c175fc07",
      "d63218dedd014133ab7f187e0179af9c",
      "66c8a0bb9d8e48d6a147fc17905e6021",
      "477cf1ca1a3c4940ad6e4f7a3b217645"
     ]
    },
    "executionInfo": {
     "elapsed": 42461,
     "status": "ok",
     "timestamp": 1754371828438,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -480
    },
    "id": "fEhVQUWE5LpC",
    "outputId": "be4e1fc2-1669-4053-fe24-457d09ea16df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Tuning on group2 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40943c9c13804f6897496c11986a1aae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa04922eea34711b9594936ee95f83d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch   1 — Loss: 0.689878\n",
      "  Epoch   2 — Loss: 0.687140\n",
      "  Epoch   3 — Loss: 0.683613\n",
      "  Epoch   4 — Loss: 0.679142\n",
      "  Epoch   5 — Loss: 0.673652\n",
      "  Epoch   6 — Loss: 0.668305\n",
      "  Epoch   7 — Loss: 0.663050\n",
      "  Epoch   8 — Loss: 0.657952\n",
      "  Epoch   9 — Loss: 0.653254\n",
      "  Epoch  10 — Loss: 0.649072\n",
      "  Epoch  11 — Loss: 0.645324\n",
      "  Epoch  12 — Loss: 0.642181\n",
      "  Epoch  13 — Loss: 0.638604\n",
      "  Epoch  14 — Loss: 0.635826\n",
      "  Epoch  15 — Loss: 0.632476\n",
      "  Epoch  16 — Loss: 0.630324\n",
      "  Epoch  17 — Loss: 0.627159\n",
      "  Epoch  18 — Loss: 0.624544\n",
      "  Epoch  19 — Loss: 0.622096\n",
      "  Epoch  20 — Loss: 0.619103\n",
      "  Epoch  21 — Loss: 0.616999\n",
      "  Epoch  22 — Loss: 0.614578\n",
      "  Epoch  23 — Loss: 0.611888\n",
      "  Epoch  24 — Loss: 0.609303\n",
      "  Epoch  25 — Loss: 0.607044\n",
      "  Epoch  26 — Loss: 0.604737\n",
      "  Epoch  27 — Loss: 0.602488\n",
      "  Epoch  28 — Loss: 0.600218\n",
      "  Epoch  29 — Loss: 0.598155\n",
      "  Epoch  30 — Loss: 0.595365\n",
      "  Epoch  31 — Loss: 0.593404\n",
      "  Epoch  32 — Loss: 0.591196\n",
      "  Epoch  33 — Loss: 0.588166\n",
      "  Epoch  34 — Loss: 0.586441\n",
      "  Epoch  35 — Loss: 0.584464\n",
      "  Epoch  36 — Loss: 0.582403\n",
      "  Epoch  37 — Loss: 0.579729\n",
      "  Epoch  38 — Loss: 0.578139\n",
      "  Epoch  39 — Loss: 0.575462\n",
      "  Epoch  40 — Loss: 0.573957\n",
      "  Epoch  41 — Loss: 0.571165\n",
      "  Epoch  42 — Loss: 0.568766\n",
      "  Epoch  43 — Loss: 0.566615\n",
      "  Epoch  44 — Loss: 0.564467\n",
      "  Epoch  45 — Loss: 0.562680\n",
      "  Epoch  46 — Loss: 0.561547\n",
      "  Epoch  47 — Loss: 0.557829\n",
      "  Epoch  48 — Loss: 0.556001\n",
      "  Epoch  49 — Loss: 0.553719\n",
      "  Epoch  50 — Loss: 0.551830\n",
      "  Epoch  51 — Loss: 0.549832\n",
      "  Epoch  52 — Loss: 0.547371\n",
      "  Epoch  53 — Loss: 0.545251\n",
      "  Epoch  54 — Loss: 0.543425\n",
      "  Epoch  55 — Loss: 0.540777\n",
      "  Epoch  56 — Loss: 0.538765\n",
      "  Epoch  57 — Loss: 0.536919\n",
      "  Epoch  58 — Loss: 0.534750\n",
      "  Epoch  59 — Loss: 0.532604\n",
      "  Epoch  60 — Loss: 0.530793\n",
      "  Epoch  61 — Loss: 0.528112\n",
      "  Epoch  62 — Loss: 0.525868\n",
      "  Epoch  63 — Loss: 0.523883\n",
      "  Epoch  64 — Loss: 0.522214\n",
      "  Epoch  65 — Loss: 0.519664\n",
      "  Epoch  66 — Loss: 0.517488\n",
      "  Epoch  67 — Loss: 0.516092\n",
      "  Epoch  68 — Loss: 0.513832\n",
      "  Epoch  69 — Loss: 0.511248\n",
      "  Epoch  70 — Loss: 0.509646\n",
      "  Epoch  71 — Loss: 0.507437\n",
      "  Epoch  72 — Loss: 0.505085\n",
      "  Epoch  73 — Loss: 0.503621\n",
      "  Epoch  74 — Loss: 0.502787\n",
      "  Epoch  75 — Loss: 0.498568\n",
      "  Epoch  76 — Loss: 0.497111\n",
      "  Epoch  77 — Loss: 0.494720\n",
      "  Epoch  78 — Loss: 0.492733\n",
      "  Epoch  79 — Loss: 0.491376\n",
      "  Epoch  80 — Loss: 0.488955\n",
      "  Epoch  81 — Loss: 0.486242\n",
      "  Epoch  82 — Loss: 0.484842\n",
      "  Epoch  83 — Loss: 0.483092\n",
      "  Epoch  84 — Loss: 0.480733\n",
      "  Epoch  85 — Loss: 0.479189\n",
      "  Epoch  86 — Loss: 0.476792\n",
      "  Epoch  87 — Loss: 0.474377\n",
      "  Epoch  88 — Loss: 0.472808\n",
      "  Epoch  89 — Loss: 0.470850\n",
      "  Epoch  90 — Loss: 0.468878\n",
      "  Epoch  91 — Loss: 0.467223\n",
      "  Epoch  92 — Loss: 0.464589\n",
      "  Epoch  93 — Loss: 0.462761\n",
      "  Epoch  94 — Loss: 0.461109\n",
      "  Epoch  95 — Loss: 0.458369\n",
      "  Epoch  96 — Loss: 0.457292\n",
      "  Epoch  97 — Loss: 0.454618\n",
      "  Epoch  98 — Loss: 0.453929\n",
      "  Epoch  99 — Loss: 0.451333\n",
      "  Epoch 100 — Loss: 0.449567\n",
      "Metrics for group2:\n",
      "  Accuracy    :  58.44%\n",
      "  Micro-F1    :  58.44%\n",
      "  Macro-F1    :  57.74%\n",
      "  Weighted-F1 :  58.18%\n",
      "  Macro-Recall:  57.78%\n",
      "\n",
      "=== Tuning on group3 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64e871c6d05740ffbaf824a121db353c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c52f0655c16f4852b9dc4ae683db7b38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch   1 — Loss: 0.695775\n",
      "  Epoch   2 — Loss: 0.689023\n",
      "  Epoch   3 — Loss: 0.683282\n",
      "  Epoch   4 — Loss: 0.679513\n",
      "  Epoch   5 — Loss: 0.676008\n",
      "  Epoch   6 — Loss: 0.672447\n",
      "  Epoch   7 — Loss: 0.669211\n",
      "  Epoch   8 — Loss: 0.665332\n",
      "  Epoch   9 — Loss: 0.661666\n",
      "  Epoch  10 — Loss: 0.657597\n",
      "  Epoch  11 — Loss: 0.653778\n",
      "  Epoch  12 — Loss: 0.649916\n",
      "  Epoch  13 — Loss: 0.646380\n",
      "  Epoch  14 — Loss: 0.642272\n",
      "  Epoch  15 — Loss: 0.638513\n",
      "  Epoch  16 — Loss: 0.635046\n",
      "  Epoch  17 — Loss: 0.631257\n",
      "  Epoch  18 — Loss: 0.627360\n",
      "  Epoch  19 — Loss: 0.623805\n",
      "  Epoch  20 — Loss: 0.620497\n",
      "  Epoch  21 — Loss: 0.616496\n",
      "  Epoch  22 — Loss: 0.613360\n",
      "  Epoch  23 — Loss: 0.610199\n",
      "  Epoch  24 — Loss: 0.606809\n",
      "  Epoch  25 — Loss: 0.603498\n",
      "  Epoch  26 — Loss: 0.600441\n",
      "  Epoch  27 — Loss: 0.597191\n",
      "  Epoch  28 — Loss: 0.593932\n",
      "  Epoch  29 — Loss: 0.591321\n",
      "  Epoch  30 — Loss: 0.588136\n",
      "  Epoch  31 — Loss: 0.584800\n",
      "  Epoch  32 — Loss: 0.581771\n",
      "  Epoch  33 — Loss: 0.579256\n",
      "  Epoch  34 — Loss: 0.575928\n",
      "  Epoch  35 — Loss: 0.572910\n",
      "  Epoch  36 — Loss: 0.570383\n",
      "  Epoch  37 — Loss: 0.568066\n",
      "  Epoch  38 — Loss: 0.564768\n",
      "  Epoch  39 — Loss: 0.562679\n",
      "  Epoch  40 — Loss: 0.559201\n",
      "  Epoch  41 — Loss: 0.556827\n",
      "  Epoch  42 — Loss: 0.554570\n",
      "  Epoch  43 — Loss: 0.551228\n",
      "  Epoch  44 — Loss: 0.548489\n",
      "  Epoch  45 — Loss: 0.546483\n",
      "  Epoch  46 — Loss: 0.543478\n",
      "  Epoch  47 — Loss: 0.541467\n",
      "  Epoch  48 — Loss: 0.538928\n",
      "  Epoch  49 — Loss: 0.537148\n",
      "  Epoch  50 — Loss: 0.533579\n",
      "  Epoch  51 — Loss: 0.531624\n",
      "  Epoch  52 — Loss: 0.528794\n",
      "  Epoch  53 — Loss: 0.526316\n",
      "  Epoch  54 — Loss: 0.524324\n",
      "  Epoch  55 — Loss: 0.521849\n",
      "  Epoch  56 — Loss: 0.519604\n",
      "  Epoch  57 — Loss: 0.516944\n",
      "  Epoch  58 — Loss: 0.514792\n",
      "  Epoch  59 — Loss: 0.512152\n",
      "  Epoch  60 — Loss: 0.510147\n",
      "  Epoch  61 — Loss: 0.507835\n",
      "  Epoch  62 — Loss: 0.505548\n",
      "  Epoch  63 — Loss: 0.503334\n",
      "  Epoch  64 — Loss: 0.501456\n",
      "  Epoch  65 — Loss: 0.498797\n",
      "  Epoch  66 — Loss: 0.496624\n",
      "  Epoch  67 — Loss: 0.494289\n",
      "  Epoch  68 — Loss: 0.492654\n",
      "  Epoch  69 — Loss: 0.490365\n",
      "  Epoch  70 — Loss: 0.488206\n",
      "  Epoch  71 — Loss: 0.486088\n",
      "  Epoch  72 — Loss: 0.483544\n",
      "  Epoch  73 — Loss: 0.481572\n",
      "  Epoch  74 — Loss: 0.479595\n",
      "  Epoch  75 — Loss: 0.477456\n",
      "  Epoch  76 — Loss: 0.475684\n",
      "  Epoch  77 — Loss: 0.473546\n",
      "  Epoch  78 — Loss: 0.471391\n",
      "  Epoch  79 — Loss: 0.469364\n",
      "  Epoch  80 — Loss: 0.467436\n",
      "  Epoch  81 — Loss: 0.465568\n",
      "  Epoch  82 — Loss: 0.462803\n",
      "  Epoch  83 — Loss: 0.460840\n",
      "  Epoch  84 — Loss: 0.459230\n",
      "  Epoch  85 — Loss: 0.457065\n",
      "  Epoch  86 — Loss: 0.455351\n",
      "  Epoch  87 — Loss: 0.453803\n",
      "  Epoch  88 — Loss: 0.451158\n",
      "  Epoch  89 — Loss: 0.449455\n",
      "  Epoch  90 — Loss: 0.446911\n",
      "  Epoch  91 — Loss: 0.445547\n",
      "  Epoch  92 — Loss: 0.444304\n",
      "  Epoch  93 — Loss: 0.441170\n",
      "  Epoch  94 — Loss: 0.439201\n",
      "  Epoch  95 — Loss: 0.438049\n",
      "  Epoch  96 — Loss: 0.435318\n",
      "  Epoch  97 — Loss: 0.434280\n",
      "  Epoch  98 — Loss: 0.431980\n",
      "  Epoch  99 — Loss: 0.430614\n",
      "  Epoch 100 — Loss: 0.427493\n",
      "Metrics for group3:\n",
      "  Accuracy    :  57.50%\n",
      "  Micro-F1    :  57.50%\n",
      "  Macro-F1    :  57.17%\n",
      "  Weighted-F1 :  57.64%\n",
      "  Macro-Recall:  57.30%\n",
      "\n",
      "=== Tuning on group4 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f0853ae18504aaabe3b11976e643a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d79769f4528c48e5bac839504aff864d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch   1 — Loss: 0.688638\n",
      "  Epoch   2 — Loss: 0.685498\n",
      "  Epoch   3 — Loss: 0.682630\n",
      "  Epoch   4 — Loss: 0.679024\n",
      "  Epoch   5 — Loss: 0.674858\n",
      "  Epoch   6 — Loss: 0.670322\n",
      "  Epoch   7 — Loss: 0.666086\n",
      "  Epoch   8 — Loss: 0.661535\n",
      "  Epoch   9 — Loss: 0.657152\n",
      "  Epoch  10 — Loss: 0.652488\n",
      "  Epoch  11 — Loss: 0.649171\n",
      "  Epoch  12 — Loss: 0.644251\n",
      "  Epoch  13 — Loss: 0.640496\n",
      "  Epoch  14 — Loss: 0.637168\n",
      "  Epoch  15 — Loss: 0.633441\n",
      "  Epoch  16 — Loss: 0.630276\n",
      "  Epoch  17 — Loss: 0.627018\n",
      "  Epoch  18 — Loss: 0.623191\n",
      "  Epoch  19 — Loss: 0.620272\n",
      "  Epoch  20 — Loss: 0.617175\n",
      "  Epoch  21 — Loss: 0.614618\n",
      "  Epoch  22 — Loss: 0.611340\n",
      "  Epoch  23 — Loss: 0.608489\n",
      "  Epoch  24 — Loss: 0.605613\n",
      "  Epoch  25 — Loss: 0.603171\n",
      "  Epoch  26 — Loss: 0.600409\n",
      "  Epoch  27 — Loss: 0.597495\n",
      "  Epoch  28 — Loss: 0.594707\n",
      "  Epoch  29 — Loss: 0.592335\n",
      "  Epoch  30 — Loss: 0.590396\n",
      "  Epoch  31 — Loss: 0.587503\n",
      "  Epoch  32 — Loss: 0.584803\n",
      "  Epoch  33 — Loss: 0.582461\n",
      "  Epoch  34 — Loss: 0.580247\n",
      "  Epoch  35 — Loss: 0.577740\n",
      "  Epoch  36 — Loss: 0.575046\n",
      "  Epoch  37 — Loss: 0.573489\n",
      "  Epoch  38 — Loss: 0.570777\n",
      "  Epoch  39 — Loss: 0.568296\n",
      "  Epoch  40 — Loss: 0.566306\n",
      "  Epoch  41 — Loss: 0.564218\n",
      "  Epoch  42 — Loss: 0.560982\n",
      "  Epoch  43 — Loss: 0.558754\n",
      "  Epoch  44 — Loss: 0.557363\n",
      "  Epoch  45 — Loss: 0.554878\n",
      "  Epoch  46 — Loss: 0.552323\n",
      "  Epoch  47 — Loss: 0.550443\n",
      "  Epoch  48 — Loss: 0.547638\n",
      "  Epoch  49 — Loss: 0.545760\n",
      "  Epoch  50 — Loss: 0.543184\n",
      "  Epoch  51 — Loss: 0.541448\n",
      "  Epoch  52 — Loss: 0.538984\n",
      "  Epoch  53 — Loss: 0.537384\n",
      "  Epoch  54 — Loss: 0.534873\n",
      "  Epoch  55 — Loss: 0.532875\n",
      "  Epoch  56 — Loss: 0.530601\n",
      "  Epoch  57 — Loss: 0.528642\n",
      "  Epoch  58 — Loss: 0.526982\n",
      "  Epoch  59 — Loss: 0.524673\n",
      "  Epoch  60 — Loss: 0.521426\n",
      "  Epoch  61 — Loss: 0.519633\n",
      "  Epoch  62 — Loss: 0.517734\n",
      "  Epoch  63 — Loss: 0.515543\n",
      "  Epoch  64 — Loss: 0.514092\n",
      "  Epoch  65 — Loss: 0.511604\n",
      "  Epoch  66 — Loss: 0.509377\n",
      "  Epoch  67 — Loss: 0.507019\n",
      "  Epoch  68 — Loss: 0.505727\n",
      "  Epoch  69 — Loss: 0.502986\n",
      "  Epoch  70 — Loss: 0.501315\n",
      "  Epoch  71 — Loss: 0.499959\n",
      "  Epoch  72 — Loss: 0.497604\n",
      "  Epoch  73 — Loss: 0.495166\n",
      "  Epoch  74 — Loss: 0.492422\n",
      "  Epoch  75 — Loss: 0.491633\n",
      "  Epoch  76 — Loss: 0.489246\n",
      "  Epoch  77 — Loss: 0.487027\n",
      "  Epoch  78 — Loss: 0.485356\n",
      "  Epoch  79 — Loss: 0.482590\n",
      "  Epoch  80 — Loss: 0.481006\n",
      "  Epoch  81 — Loss: 0.479252\n",
      "  Epoch  82 — Loss: 0.476933\n",
      "  Epoch  83 — Loss: 0.475047\n",
      "  Epoch  84 — Loss: 0.472583\n",
      "  Epoch  85 — Loss: 0.470986\n",
      "  Epoch  86 — Loss: 0.468847\n",
      "  Epoch  87 — Loss: 0.467035\n",
      "  Epoch  88 — Loss: 0.465379\n",
      "  Epoch  89 — Loss: 0.463047\n",
      "  Epoch  90 — Loss: 0.461010\n",
      "  Epoch  91 — Loss: 0.459038\n",
      "  Epoch  92 — Loss: 0.458529\n",
      "  Epoch  93 — Loss: 0.455293\n",
      "  Epoch  94 — Loss: 0.452946\n",
      "  Epoch  95 — Loss: 0.452830\n",
      "  Epoch  96 — Loss: 0.449882\n",
      "  Epoch  97 — Loss: 0.447655\n",
      "  Epoch  98 — Loss: 0.445263\n",
      "  Epoch  99 — Loss: 0.443473\n",
      "  Epoch 100 — Loss: 0.442313\n",
      "Metrics for group4:\n",
      "  Accuracy    :  52.81%\n",
      "  Micro-F1    :  52.81%\n",
      "  Macro-F1    :  52.02%\n",
      "  Weighted-F1 :  52.72%\n",
      "  Macro-Recall:  52.03%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for group_name, labeled_group in [\n",
    "    (\"group2\", labeled_group2),\n",
    "    (\"group3\", labeled_group3),\n",
    "    (\"group4\", labeled_group4),\n",
    "]:\n",
    "    print(f\"=== Tuning on {group_name} ===\")\n",
    "    df_all = pd.concat(labeled_group.values(), ignore_index=True)\n",
    "    texts = df_all[\"title\"].tolist()\n",
    "    labels = df_all[\"label\"].astype(int).tolist()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        texts, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "    emb_train = model_2.encode(\n",
    "        X_train, device=device, batch_size=16,\n",
    "        convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "    del X_train\n",
    "    emb_test = model_2.encode(\n",
    "        X_test, device=device, batch_size=16,\n",
    "        convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "    del X_test\n",
    "    d = emb_train.size(1)\n",
    "    head = nn.Sequential(\n",
    "        nn.Linear(d, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 2)\n",
    "    ).to(device)\n",
    "    trainable = list(head.parameters()) + [\n",
    "        p for p in model_2.parameters() if p.requires_grad]\n",
    "\n",
    "    optimizer = torch.optim.Adam(trainable, lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_ds = TensorDataset(emb_train, torch.tensor(y_train, device=device))\n",
    "    train_dl = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "    best_prev_loss = float(\"inf\")\n",
    "    tol = 1e-4\n",
    "    patience = 3\n",
    "    stale_epochs = 0\n",
    "    max_epochs = 100\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        head.train()\n",
    "        epoch_loss = 0.0\n",
    "        for emb_b, lab_b in train_dl:\n",
    "            optimizer.zero_grad()\n",
    "            logits = head(emb_b)\n",
    "            loss = criterion(logits, lab_b)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * emb_b.size(0)\n",
    "        avg_loss = epoch_loss / len(train_ds)\n",
    "        print(f\"  Epoch {epoch:>3} — Loss: {avg_loss:.6f}\")\n",
    "        improvement = best_prev_loss - avg_loss\n",
    "        if improvement < tol:\n",
    "            stale_epochs += 1\n",
    "            print(f\"  ↳ small improvement (Δ={improvement:.2e}) stale {stale_epochs}/{patience}\")\n",
    "        else:\n",
    "            stale_epochs = 0\n",
    "            best_prev_loss = avg_loss\n",
    "        if stale_epochs >= patience:\n",
    "            print(f\"  ↳ Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "    head.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = head(emb_test.to(device))\n",
    "        preds = logits.argmax(dim=1).cpu().tolist()\n",
    "\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    micro_f1 = f1_score(y_test, preds, average=\"micro\")\n",
    "    macro_f1 = f1_score(y_test, preds, average=\"macro\")\n",
    "    weighted_f1 = f1_score(y_test, preds, average=\"weighted\")\n",
    "    macro_recall = recall_score(y_test, preds, average=\"macro\")\n",
    "    print(f\"Metrics for {group_name}:\")\n",
    "    print(f\"  Accuracy    : {acc*100:6.2f}%\")\n",
    "    print(f\"  Micro-F1    : {micro_f1*100:6.2f}%\")\n",
    "    print(f\"  Macro-F1    : {macro_f1*100:6.2f}%\")\n",
    "    print(f\"  Weighted-F1 : {weighted_f1*100:6.2f}%\")\n",
    "    print(f\"  Macro-Recall: {macro_recall*100:6.2f}%\\n\")\n",
    "    results[group_name] = head\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542,
     "referenced_widgets": [
      "3c815b7c30fc43b8b5ff3ad9e4c59931",
      "9b778fe2a5f14d03ad9230116dc927a3",
      "3bd0d11a5ea641a7bbaf640d7dcd9609",
      "b63c37ed1243427fafbc726daaa36046",
      "1e3afb89112a44e1ad0c2fd8d785bcdb",
      "b6fc5b1a1adb46d1afaa08b72f17942d",
      "3c292df3a74f48b095e5de06b788db23",
      "aa4aa39cea574e558a1aa81e97052567",
      "78c09ad105e84e368de62f4a4be9480e",
      "a1fa0022650648cd87a2cb295df6d039",
      "7e5f1c1c4adc4a0aa94e03b94e779b12",
      "bc1bb799000d4f0d90672d9c449173b9",
      "143996d1b43143e1be5dee8c41d6a41d",
      "d64b65866fde4fcb9be73cde7d1b5cfb",
      "9f6524821db54ea0a86630ad32157820",
      "0fa063344bcd422a8b089fb15fe7f760",
      "205128ccb85f4d2c8a5a8ca0fa5375f7",
      "51f3196f76884625926ad93fa601ae9d",
      "a9f8fac6051c44629436437c45d225cf",
      "8fc7f6c020084daf8403a4d9261b85e6",
      "1a90df402f844f1e890cbbf76f4eaaf9",
      "6e75eb7db5a74564ae14728528014606",
      "8cc8befdddc54857adf2e0f10ea9c9fa",
      "97e8de111baa47f8a96fad92e7ae4d68",
      "bf00a96fcd744276a5dbd7116391b6f8",
      "aa4ee7235610414bbc1577fc06d96637",
      "bd2906ecedd245c99379eedd87dcd7e2",
      "1b89c945170445daa07be3e5ec56d73f",
      "8052696afdbb4096bf9ab55ad3d6e686",
      "cf15b0af89a84ddcbd6392aa909e7242",
      "8f7348c44df7400b9fa76c84b55922f5",
      "f383d2ae8ace44e3a17995fa3923df11",
      "66571c7f4d134be1bd39b1ad69e259be"
     ]
    },
    "executionInfo": {
     "elapsed": 1828,
     "status": "ok",
     "timestamp": 1754371842798,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -480
    },
    "id": "CRK-SOxrqRPy",
    "outputId": "182a6281-2329-4f96-f44e-e27850c3abe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Re-evaluating group2 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c815b7c30fc43b8b5ff3ad9e4c59931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for group2:\n",
      "  Accuracy    :  58.44%\n",
      "  Micro-F1    :  58.44%\n",
      "  Macro-F1    :  57.74%\n",
      "  Weighted-F1 :  58.18%\n",
      "  Macro-Recall:  57.78%\n",
      "\n",
      "=== Re-evaluating group3 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc1bb799000d4f0d90672d9c449173b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for group3:\n",
      "  Accuracy    :  57.50%\n",
      "  Micro-F1    :  57.50%\n",
      "  Macro-F1    :  57.17%\n",
      "  Weighted-F1 :  57.64%\n",
      "  Macro-Recall:  57.30%\n",
      "\n",
      "=== Re-evaluating group4 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cc8befdddc54857adf2e0f10ea9c9fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for group4:\n",
      "  Accuracy    :  52.81%\n",
      "  Micro-F1    :  52.81%\n",
      "  Macro-F1    :  52.02%\n",
      "  Weighted-F1 :  52.72%\n",
      "  Macro-Recall:  52.03%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Re-evaluate with updated embedding and head weights\n",
    "for group_name, labeled_group in [\n",
    "    (\"group2\", labeled_group2),\n",
    "    (\"group3\", labeled_group3),\n",
    "    (\"group4\", labeled_group4),\n",
    "]:\n",
    "    print(f\"=== Re-evaluating {group_name} ===\")\n",
    "    df_all = pd.concat(labeled_group.values(), ignore_index=True)\n",
    "    texts  = df_all[\"title\"].tolist()\n",
    "    labels = df_all[\"label\"].astype(int).tolist()\n",
    "    _, X_test, _, y_test = train_test_split(\n",
    "        texts, labels,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=labels\n",
    "    )\n",
    "    emb_test_updated = model_2.encode(\n",
    "        X_test,\n",
    "        device=device,\n",
    "        batch_size=16,\n",
    "        convert_to_tensor=True,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    head = results[group_name]\n",
    "    head.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = head(emb_test_updated.to(device))\n",
    "        preds  = logits.argmax(dim=1).cpu().tolist()\n",
    "    acc          = accuracy_score(y_test, preds)\n",
    "    micro_f1     = f1_score(y_test, preds, average=\"micro\")\n",
    "    macro_f1     = f1_score(y_test, preds, average=\"macro\")\n",
    "    weighted_f1  = f1_score(y_test, preds, average=\"weighted\")\n",
    "    macro_recall = recall_score(y_test, preds, average=\"macro\")\n",
    "    print(f\"Metrics for {group_name}:\")\n",
    "    print(f\"  Accuracy    : {acc*100:6.2f}%\")\n",
    "    print(f\"  Micro-F1    : {micro_f1*100:6.2f}%\")\n",
    "    print(f\"  Macro-F1    : {macro_f1*100:6.2f}%\")\n",
    "    print(f\"  Weighted-F1 : {weighted_f1*100:6.2f}%\")\n",
    "    print(f\"  Macro-Recall: {macro_recall*100:6.2f}%\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469,
     "referenced_widgets": [
      "129965005dd24e3c82179690e9aceeb1",
      "b22ad5c9b98f4c569126e13567eebe5c",
      "4efbf8ca1f224380b786c721b1814b07",
      "cb53c426612e417883e4116bd234dc01",
      "8e956ee782ab4d268ec483965b4c6e9c",
      "2949074ceabf4c9dbcdbb1b5684d2232",
      "0f159f157be5440f919ae25480d664b0",
      "cb17aa9a61424c64a924b94369a30c03",
      "6c07e6c4ea18498ab489d4e8a98b41e4",
      "b2cc8e706bd443ef9241e9d624d58c44",
      "1ca8b5fc48e94e4fa326d9bf0e42d477",
      "9541ffe0534d497cb73a625f9240fcac",
      "d7b42a0ecc1f4533bf80ea5a95012539",
      "c4063c3555074948aced985a188e4eb7",
      "dabad717d5a347a5bba3417e1e979979",
      "0200deffd42b40f7824b59276dc36f43",
      "d1aaed186b90467b9313457faefcf6fb",
      "59642c32db6f4ff89a79b4e2e0da179b",
      "44a2ecb3ac714e479d55439681d81ce5",
      "f2a2fa21702049f681537347a3adcc90",
      "9e81fb916de44291a6f657a5058d1cba",
      "f41a2a245bd5476c8855875907dc670f",
      "3e10566992ee4393a076695f7463f581",
      "a215779aa1554555bc9c46321859d226",
      "1970c254fe094a01be0f510d52727473",
      "a1148a85c0df4c70b068f17d78e98393",
      "9ec68411acec4108845ae9b9daa08a55",
      "e702701d6f6848f1a49dc13f289ced5c",
      "0195ae4ed4284a979a7f3e9503f416c8",
      "9879e27cbb4347c4a26e824f5602c7b5",
      "0a36854647ab44acbebfd93d7818217e",
      "557aa334380b437c98e4e33e58b20898",
      "a324919ddd7044f49087ddb4bc6f159e",
      "8f4e5a4332bc49219206a518a9861076",
      "2ad8aa1e54244ec7bbccfe1e76fe08f3",
      "061815a1631347e4b1e0372ecc6e8142",
      "d49f2eb46a6e4a98999d1aea404f877a",
      "eb1b7b0412fc4d2eb8363f148807add2",
      "85f35f1464c54cbbabde1904b0fcfd36",
      "72c23c0ae43a49119bab821dbcc043a3",
      "7e541d2d0fc14ac19e8b9b710a8e00ed",
      "fd38b29257c248c9aba9a6b3fe8cd797",
      "45dcaa12303d4384a780ec9c388cc0d5",
      "089bf59f5b51435ba21aab1fb88c0b1c",
      "b795e1654da94507a0860ab505d54921",
      "510de34b99604ade9c0d319b6d62baa6",
      "f56ea0685b0c436bba6b58a4f6df615b",
      "581f682f8b7f4348bb393694c384e424",
      "9d4c52e1be344f9b8a78c1d710a739be",
      "1eff0fe60af8424a835c14bf5fd2719d",
      "2d2152f007bf43d5b1e1f31272f7e80a",
      "2eea9cc5bf48419c9e15c97a94179992",
      "c7598e9ad4364a68baada7967cddd435",
      "ca9ab8ac0c42426f8ce6b41e98ce10a8",
      "6c8de6953cce4a4594141efb89262662",
      "640aedd2c51e4bfd9893cd392c407e49",
      "66803e5c15e5457cb6fda4700fe0fb82",
      "1ce14915eb07432196c78259423b00a0",
      "2b0513458c344996b91f094c32c2ad77",
      "c7ad6d44b5cb4cabac6429d3141c566f",
      "82398aa7895148e89ae267ba692708e6",
      "be9fb9f758c04b41afd62401356ad0fb",
      "450cd24279d748a78736ab0f4bc3c1a0",
      "9b4a965c076a484b9f295ed7a3cf7fd9",
      "53653e727f48459a8c8d4f2fb23bfffe",
      "b94ab83bda514f3893fbdc70930bbc56",
      "c6fcc894b3fe418da1412e3955cbaea4",
      "a88a7ab1dbd34bdf832c5278c26e44ca",
      "1aa70b82f1e34781b42f2440dd11678e",
      "0912b1bf7fd54b46ad4a58a37f797bf5",
      "49ec2d3f60554642804f1f0fa627297c",
      "4e512cb1e18e44d08132f9a84d5472ae",
      "9ef45398270f415189abd7cc7b4c28ae",
      "31d28087678a47c8b9dbb21b0a14e213",
      "cc949b8ccf89460b87f52f58225277f8",
      "7ab76aa4f6d94694b6ecd9970bb47ffe",
      "01b51ffe15384b349dacbbdbea0fd47a",
      "3f7fb7684f8c4108a520de4dd9e3f266",
      "c79eba9e4a0641beabfd0347e9fdebe7",
      "839bc54c57e94404abd94f4e55193ee6",
      "e034b2a2043144b0afecd5a3f6b5335f",
      "6a31cc830f1948819c394f5cf525ed00",
      "829090447dc047edbace192ffe2983b6",
      "c30c5175b2634c29937e4295c91fbef0",
      "27ab67860c6b4290ad1c88303c00f769",
      "25e2f5af5b9f4d2696133b0fafe2c8de",
      "855a654b8bc1484aa815037530ea49be",
      "fc0bd15dfcf840ca90152f8566389b64",
      "4e03f692359d4552b89782390f31b9a0",
      "857c255ce9e744a2b77e62e308dd0129",
      "04839a8f156d4fc8b52cfc8561bdfa47",
      "ea8358d873c04b7f84e848a739d33848",
      "34f41e61b71d46b8856863ab5e9c2149",
      "12b9467e209e4ab1bfd86196cd8a81bf",
      "1d28c75d3eb649398edf0ce287b8ea58",
      "20643638ee654819965b77280320d87b",
      "1d4860e9fa1f4c68b86cdc0d359a900c",
      "a222594391054ce8b9d4082987efff65",
      "3947295a7f8040dcae6d0070b28c2ebf",
      "7c25a588e08345839e61b8e60925dbc8",
      "2016f773b74a45b2869d74da6f2d5e37",
      "25db0ac8373940b5b6c52c2a4af2b3fc",
      "d62fb468b11b4a33803024b45cbb1d6e",
      "d0a89a024e544d98969446bc6eadf4e0",
      "8885c1bd46384f0faf48965a1a3ca47a",
      "0f4b22d618c1450c8eef9c17fd945298",
      "66c281d4130741d08eaee641e0eb1cdb",
      "eeaf802faad940c0a4034d3e8922ce46",
      "9f4becaf39cd4457adb8c8ab5069926d",
      "cb66b3df6ad74c21b89a661c43da3d3b",
      "ae39edebed4a468ca1f53a2f3a5f2e10",
      "ecedeb5a8f294246889b2141dd532d21",
      "78433c28f6344d4d9d3ecf398b2e4b84",
      "9b8e6394f84a45cebfe75f50bc3f80e9",
      "d08bc3b29f98492bbceacdc3302e04da",
      "9cd2bbfb14464aed9900e84e5e5b8b9f",
      "a63c0f8bb637440dafcb81e7241a1b55",
      "35418e5021a64cf7950db4a68bb41e28",
      "f53e61c0382c48dd8a488470b3f53793",
      "61fa01f2ac2345648645fadea6c97820",
      "548b755047eb46c0935bbd7f1a3488f9",
      "55c4007d8cfa411ab3fcdbca0817db20",
      "e105634969ca4e319d21eb3d61c2c36c",
      "6426829d5ff44098812f8765e5c5d6d3",
      "e3d94c0ebaa244eca33286e4fc82cdb5",
      "05cf5cfa492747209aad605e7c624147",
      "4625eb87b7594ff4b412adde5f5d9331",
      "2481ad8c733b485d9185fe36163c94a9",
      "08dd3d2e81164200bb3ad073caa328cf",
      "ec6bca19f4a2439583d5a7927eacc45d",
      "93f7d6dc11ef437c8cb4e02ec4fafdd6",
      "3535efc7d050413ab0bc4f6cfda0df91",
      "51c58e22117c4127af3ae1dbeeb33f0b",
      "74710db7e8724d379bc783c4d2115721",
      "29173612c9bd4b17b76b195f3adcc2cd",
      "fe031ad9d7cf425894f693da8a7359d4",
      "d26052c1f7034e50bce1b6d5a196dd16",
      "5a45644e7d92470f8fa8438e7f4d2705",
      "a5c0866d4f324145a2b439818927d449",
      "5b14781b712f486ebb295916e02e1c70",
      "557ec18f7ba9459da52f9eb62d9666db",
      "69c4e69550674bfca39df8fd86d48f41",
      "e8d570652a5645eab76d8f0b068868f6"
     ]
    },
    "executionInfo": {
     "elapsed": 11633,
     "status": "ok",
     "timestamp": 1754371840968,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -480
    },
    "id": "1gyJI7wpCGwP",
    "outputId": "0f826e9a-1df8-4932-991f-cb5b042ebeb4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "129965005dd24e3c82179690e9aceeb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9541ffe0534d497cb73a625f9240fcac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e10566992ee4393a076695f7463f581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f4e5a4332bc49219206a518a9861076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b795e1654da94507a0860ab505d54921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/674 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "640aedd2c51e4bfd9893cd392c407e49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6fcc894b3fe418da1412e3955cbaea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f7fb7684f8c4108a520de4dd9e3f266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e03f692359d4552b89782390f31b9a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c25a588e08345839e61b8e60925dbc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae39edebed4a468ca1f53a2f3a5f2e10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55c4007d8cfa411ab3fcdbca0817db20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51c58e22117c4127af3ae1dbeeb33f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Found blocks under: encoder.layer\n",
      "Un-froze 49 params total.\n"
     ]
    }
   ],
   "source": [
    "model_3 = SentenceTransformer(\n",
    "    \"sentence-transformers/stsb-roberta-large\",\n",
    "    trust_remote_code=True)\n",
    "\n",
    "for p in model_3.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "transformer = model_3._modules['0']\n",
    "hf_model    = transformer.auto_model\n",
    "\n",
    "for p in hf_model.get_input_embeddings().parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "blocks = None\n",
    "for name, module in hf_model.named_modules():\n",
    "    if isinstance(module, torch.nn.ModuleList):\n",
    "        blocks = module\n",
    "        print(f\"→ Found blocks under: {name}\")\n",
    "        break\n",
    "if blocks is None:\n",
    "    raise RuntimeError(\"Could not find transformer blocks in hf_model\")\n",
    "\n",
    "for block in blocks[-3:]:\n",
    "    for p in block.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "print(f\"Un-froze {sum(p.requires_grad for p in model_1.parameters())} params total.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "93a54cbc4e2d4fc8bfa76eae87e0b2d0",
      "08369bb85f914744892259648d4a941f",
      "31c0abe5d9f343ff8480e9b13c61d613",
      "4c8eb44059754dba82dfe94985046a23",
      "b5b28573ad844015bb2d4591bf8f699f",
      "249a27e9c5fd42329b806b16c842872a",
      "797509eef0c34664ad2de3be287cd0be",
      "9d8598c24bc348368fa19f6f0e501590",
      "3a5ddeed82374cadafbcf1cd7a59185c",
      "d8d383fb52644688b0b0f906862f884c",
      "61620bd4ad0344139944f7157dc6a303",
      "b0dc86304f9248b993f3976b443f1d58",
      "e899289023a34aac87b2215cf6f1c61b",
      "63559398bd1c43bdb175ae2a6166b75c",
      "cbdced08e0a04957b8fdb0f41c20b47f",
      "272af2500b884310a7c4cad54ed251fe",
      "0baeaf8c80fd4a358a10d616ba0c2999",
      "e86a6969d17e4765b98ff043156ef409",
      "c2cb50d0a99c4540837fa6bf3405d2e8",
      "b70d3da1789d4827b848846cbc518373",
      "81d900698b5c4ec8a1e1317aa92c1de1",
      "ca24bb468e7b40f1b572cad899ac570f",
      "8f291a7c9f38460ba739d3d89f775c5e",
      "cc05bea4bd074ef686a9445e0f83bf3e",
      "58cf9849df4b4b3c81a660c3c10a9006",
      "ccb7867abaf44e55a88804c4f1249f8c",
      "40cda04745b94f6ebb0b3757b0eb71e2",
      "fed51711227c4f8ba2bc981a97f55257",
      "b7b08b849b1144cc810ef523a288a50c",
      "9a20256568d5466abb558dcd84d2e919",
      "7af19486e97146a6ab2798159cf1847f",
      "78670d59735445c1b1e0a291217602f9",
      "8368d11a12cb43e6ba0ba89025819280",
      "2291e1dcc530406c8e2cbafa3db7811d",
      "344bb98cb2174f138c6190914f186fc3",
      "be34eba9f8f448a7acf603d2d5fabc3a",
      "8cfc686aec504e4098d4d265a15da1ae",
      "8b1cd9a2a3fd4f6c969a7457d0f9d720",
      "fca70fbb2ce44838a49c8c2e1ce55d4c",
      "1ac708c5777b4ad095a5c61c1402fe21",
      "a04102a5be97468cab6e1ef22286517d",
      "ff4ccbba145e48b4af24203d909163f9",
      "6c66145b3cda431ab9df9dd74ee7f872",
      "10507fe42b1644b69c35b1a27d44c026",
      "8cd61fd58c4149bbbebe46ad669d9a4e",
      "e3d1be8f22a849ed85112de2ace2c5c3",
      "eb6cd7fd4c3f49e0b6438aa57914fcb2",
      "e29a503880fc4aabbb0d73886037a3eb",
      "cf38b54d78074a059ccb6281a8b28053",
      "703c0a12d5224fe5b481ac8c8cc07bde",
      "6b10f236f5794c81bc9064d7e0dd6779",
      "19e61856ef264e64929ca3cf737377a9",
      "79e8fd616c0c4913b148b2b8ed3c6416",
      "9f8fe5d48df34adf904ac6a44348ab58",
      "e1429d631cc74ea18abae4e592e2eb37",
      "82b91f06b408433391de80d63c071fde",
      "c819f1e6b8d34181b51708632bf5635f",
      "ca06ee498ed84d72b63028786941ceac",
      "b528326895e543ac97f509746f63b99d",
      "bc7a4a2f34ea41f781bdb7e5474f9fc9",
      "cd93f5e226c84949a088b527477b2c42",
      "b11f7de52f8748a182da6125f8fba14c",
      "8b02de887c9e4de9920fdbb13fc077eb",
      "68dac9c96c6e49388121acaa4f5a9bc4",
      "69b96bc93fcd406fb1dcaab42c6014e6",
      "a9427da925cd4d0aac72e7f21be2875d"
     ]
    },
    "executionInfo": {
     "elapsed": 38047,
     "status": "ok",
     "timestamp": 1754371930199,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -480
    },
    "id": "3cs52PK0L822",
    "outputId": "88d4ac33-44af-4ae9-a409-9a8078227e1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Tuning on group2 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93a54cbc4e2d4fc8bfa76eae87e0b2d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0dc86304f9248b993f3976b443f1d58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch   1 — Loss: 0.684203\n",
      "  Epoch   2 — Loss: 0.651914\n",
      "  Epoch   3 — Loss: 0.634159\n",
      "  Epoch   4 — Loss: 0.616365\n",
      "  Epoch   5 — Loss: 0.602416\n",
      "  Epoch   6 — Loss: 0.585122\n",
      "  Epoch   7 — Loss: 0.568690\n",
      "  Epoch   8 — Loss: 0.552730\n",
      "  Epoch   9 — Loss: 0.539183\n",
      "  Epoch  10 — Loss: 0.522088\n",
      "  Epoch  11 — Loss: 0.505964\n",
      "  Epoch  12 — Loss: 0.490110\n",
      "  Epoch  13 — Loss: 0.473711\n",
      "  Epoch  14 — Loss: 0.459154\n",
      "  Epoch  15 — Loss: 0.442652\n",
      "  Epoch  16 — Loss: 0.426321\n",
      "  Epoch  17 — Loss: 0.414032\n",
      "  Epoch  18 — Loss: 0.397117\n",
      "  Epoch  19 — Loss: 0.384131\n",
      "  Epoch  20 — Loss: 0.372073\n",
      "  Epoch  21 — Loss: 0.359825\n",
      "  Epoch  22 — Loss: 0.348472\n",
      "  Epoch  23 — Loss: 0.333871\n",
      "  Epoch  24 — Loss: 0.325948\n",
      "  Epoch  25 — Loss: 0.312595\n",
      "  Epoch  26 — Loss: 0.303959\n",
      "  Epoch  27 — Loss: 0.292465\n",
      "  Epoch  28 — Loss: 0.281778\n",
      "  Epoch  29 — Loss: 0.272511\n",
      "  Epoch  30 — Loss: 0.265515\n",
      "  Epoch  31 — Loss: 0.255212\n",
      "  Epoch  32 — Loss: 0.247497\n",
      "  Epoch  33 — Loss: 0.239874\n",
      "  Epoch  34 — Loss: 0.232152\n",
      "  Epoch  35 — Loss: 0.225961\n",
      "  Epoch  36 — Loss: 0.220942\n",
      "  Epoch  37 — Loss: 0.214907\n",
      "  Epoch  38 — Loss: 0.205040\n",
      "  Epoch  39 — Loss: 0.202545\n",
      "  Epoch  40 — Loss: 0.196083\n",
      "  Epoch  41 — Loss: 0.190929\n",
      "  Epoch  42 — Loss: 0.185488\n",
      "  Epoch  43 — Loss: 0.180011\n",
      "  Epoch  44 — Loss: 0.174907\n",
      "  Epoch  45 — Loss: 0.171648\n",
      "  Epoch  46 — Loss: 0.165924\n",
      "  Epoch  47 — Loss: 0.162180\n",
      "  Epoch  48 — Loss: 0.159426\n",
      "  Epoch  49 — Loss: 0.155434\n",
      "  Epoch  50 — Loss: 0.153544\n",
      "  Epoch  51 — Loss: 0.147653\n",
      "  Epoch  52 — Loss: 0.147866\n",
      "  ↳ small improvement (Δ=-2.12e-04) stale 1/3\n",
      "  Epoch  53 — Loss: 0.142683\n",
      "  Epoch  54 — Loss: 0.144807\n",
      "  ↳ small improvement (Δ=-2.12e-03) stale 1/3\n",
      "  Epoch  55 — Loss: 0.135798\n",
      "  Epoch  56 — Loss: 0.135247\n",
      "  Epoch  57 — Loss: 0.133095\n",
      "  Epoch  58 — Loss: 0.130936\n",
      "  Epoch  59 — Loss: 0.127467\n",
      "  Epoch  60 — Loss: 0.125543\n",
      "  Epoch  61 — Loss: 0.121845\n",
      "  Epoch  62 — Loss: 0.125681\n",
      "  ↳ small improvement (Δ=-3.84e-03) stale 1/3\n",
      "  Epoch  63 — Loss: 0.119258\n",
      "  Epoch  64 — Loss: 0.117345\n",
      "  Epoch  65 — Loss: 0.117034\n",
      "  Epoch  66 — Loss: 0.114791\n",
      "  Epoch  67 — Loss: 0.109261\n",
      "  Epoch  68 — Loss: 0.113443\n",
      "  ↳ small improvement (Δ=-4.18e-03) stale 1/3\n",
      "  Epoch  69 — Loss: 0.111771\n",
      "  ↳ small improvement (Δ=-2.51e-03) stale 2/3\n",
      "  Epoch  70 — Loss: 0.107617\n",
      "  Epoch  71 — Loss: 0.105926\n",
      "  Epoch  72 — Loss: 0.107925\n",
      "  ↳ small improvement (Δ=-2.00e-03) stale 1/3\n",
      "  Epoch  73 — Loss: 0.106220\n",
      "  ↳ small improvement (Δ=-2.94e-04) stale 2/3\n",
      "  Epoch  74 — Loss: 0.102808\n",
      "  Epoch  75 — Loss: 0.105975\n",
      "  ↳ small improvement (Δ=-3.17e-03) stale 1/3\n",
      "  Epoch  76 — Loss: 0.104693\n",
      "  ↳ small improvement (Δ=-1.88e-03) stale 2/3\n",
      "  Epoch  77 — Loss: 0.099820\n",
      "  Epoch  78 — Loss: 0.098839\n",
      "  Epoch  79 — Loss: 0.098751\n",
      "  ↳ small improvement (Δ=8.78e-05) stale 1/3\n",
      "  Epoch  80 — Loss: 0.099923\n",
      "  ↳ small improvement (Δ=-1.08e-03) stale 2/3\n",
      "  Epoch  81 — Loss: 0.097129\n",
      "  Epoch  82 — Loss: 0.099282\n",
      "  ↳ small improvement (Δ=-2.15e-03) stale 1/3\n",
      "  Epoch  83 — Loss: 0.097198\n",
      "  ↳ small improvement (Δ=-6.91e-05) stale 2/3\n",
      "  Epoch  84 — Loss: 0.097142\n",
      "  ↳ small improvement (Δ=-1.38e-05) stale 3/3\n",
      "  ↳ Early stopping at epoch 84\n",
      "Metrics for group2:\n",
      "  Accuracy    :  58.44%\n",
      "  Micro-F1    :  58.44%\n",
      "  Macro-F1    :  58.04%\n",
      "  Weighted-F1 :  58.37%\n",
      "  Macro-Recall:  58.03%\n",
      "\n",
      "=== Tuning on group3 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f291a7c9f38460ba739d3d89f775c5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2291e1dcc530406c8e2cbafa3db7811d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch   1 — Loss: 0.680906\n",
      "  Epoch   2 — Loss: 0.651838\n",
      "  Epoch   3 — Loss: 0.632582\n",
      "  Epoch   4 — Loss: 0.613767\n",
      "  Epoch   5 — Loss: 0.596141\n",
      "  Epoch   6 — Loss: 0.579071\n",
      "  Epoch   7 — Loss: 0.561556\n",
      "  Epoch   8 — Loss: 0.541865\n",
      "  Epoch   9 — Loss: 0.525246\n",
      "  Epoch  10 — Loss: 0.506970\n",
      "  Epoch  11 — Loss: 0.485707\n",
      "  Epoch  12 — Loss: 0.469680\n",
      "  Epoch  13 — Loss: 0.450482\n",
      "  Epoch  14 — Loss: 0.433289\n",
      "  Epoch  15 — Loss: 0.416671\n",
      "  Epoch  16 — Loss: 0.399856\n",
      "  Epoch  17 — Loss: 0.382653\n",
      "  Epoch  18 — Loss: 0.367167\n",
      "  Epoch  19 — Loss: 0.354555\n",
      "  Epoch  20 — Loss: 0.338210\n",
      "  Epoch  21 — Loss: 0.324430\n",
      "  Epoch  22 — Loss: 0.309983\n",
      "  Epoch  23 — Loss: 0.300754\n",
      "  Epoch  24 — Loss: 0.285679\n",
      "  Epoch  25 — Loss: 0.272687\n",
      "  Epoch  26 — Loss: 0.264264\n",
      "  Epoch  27 — Loss: 0.253202\n",
      "  Epoch  28 — Loss: 0.242305\n",
      "  Epoch  29 — Loss: 0.233397\n",
      "  Epoch  30 — Loss: 0.225520\n",
      "  Epoch  31 — Loss: 0.216467\n",
      "  Epoch  32 — Loss: 0.208381\n",
      "  Epoch  33 — Loss: 0.200821\n",
      "  Epoch  34 — Loss: 0.193841\n",
      "  Epoch  35 — Loss: 0.187713\n",
      "  Epoch  36 — Loss: 0.180025\n",
      "  Epoch  37 — Loss: 0.175095\n",
      "  Epoch  38 — Loss: 0.170118\n",
      "  Epoch  39 — Loss: 0.166456\n",
      "  Epoch  40 — Loss: 0.160190\n",
      "  Epoch  41 — Loss: 0.153717\n",
      "  Epoch  42 — Loss: 0.150584\n",
      "  Epoch  43 — Loss: 0.148217\n",
      "  Epoch  44 — Loss: 0.145835\n",
      "  Epoch  45 — Loss: 0.139650\n",
      "  Epoch  46 — Loss: 0.139079\n",
      "  Epoch  47 — Loss: 0.133716\n",
      "  Epoch  48 — Loss: 0.129972\n",
      "  Epoch  49 — Loss: 0.127521\n",
      "  Epoch  50 — Loss: 0.123087\n",
      "  Epoch  51 — Loss: 0.121007\n",
      "  Epoch  52 — Loss: 0.118758\n",
      "  Epoch  53 — Loss: 0.116715\n",
      "  Epoch  54 — Loss: 0.116373\n",
      "  Epoch  55 — Loss: 0.114269\n",
      "  Epoch  56 — Loss: 0.110334\n",
      "  Epoch  57 — Loss: 0.110705\n",
      "  ↳ small improvement (Δ=-3.71e-04) stale 1/3\n",
      "  Epoch  58 — Loss: 0.108954\n",
      "  Epoch  59 — Loss: 0.107627\n",
      "  Epoch  60 — Loss: 0.105220\n",
      "  Epoch  61 — Loss: 0.103167\n",
      "  Epoch  62 — Loss: 0.102931\n",
      "  Epoch  63 — Loss: 0.102032\n",
      "  Epoch  64 — Loss: 0.102307\n",
      "  ↳ small improvement (Δ=-2.75e-04) stale 1/3\n",
      "  Epoch  65 — Loss: 0.098378\n",
      "  Epoch  66 — Loss: 0.100964\n",
      "  ↳ small improvement (Δ=-2.59e-03) stale 1/3\n",
      "  Epoch  67 — Loss: 0.100607\n",
      "  ↳ small improvement (Δ=-2.23e-03) stale 2/3\n",
      "  Epoch  68 — Loss: 0.095736\n",
      "  Epoch  69 — Loss: 0.096105\n",
      "  ↳ small improvement (Δ=-3.68e-04) stale 1/3\n",
      "  Epoch  70 — Loss: 0.094617\n",
      "  Epoch  71 — Loss: 0.095206\n",
      "  ↳ small improvement (Δ=-5.89e-04) stale 1/3\n",
      "  Epoch  72 — Loss: 0.092048\n",
      "  Epoch  73 — Loss: 0.092352\n",
      "  ↳ small improvement (Δ=-3.04e-04) stale 1/3\n",
      "  Epoch  74 — Loss: 0.092745\n",
      "  ↳ small improvement (Δ=-6.96e-04) stale 2/3\n",
      "  Epoch  75 — Loss: 0.091234\n",
      "  Epoch  76 — Loss: 0.093584\n",
      "  ↳ small improvement (Δ=-2.35e-03) stale 1/3\n",
      "  Epoch  77 — Loss: 0.092754\n",
      "  ↳ small improvement (Δ=-1.52e-03) stale 2/3\n",
      "  Epoch  78 — Loss: 0.088878\n",
      "  Epoch  79 — Loss: 0.088443\n",
      "  Epoch  80 — Loss: 0.088295\n",
      "  Epoch  81 — Loss: 0.089198\n",
      "  ↳ small improvement (Δ=-9.03e-04) stale 1/3\n",
      "  Epoch  82 — Loss: 0.087565\n",
      "  Epoch  83 — Loss: 0.088037\n",
      "  ↳ small improvement (Δ=-4.72e-04) stale 1/3\n",
      "  Epoch  84 — Loss: 0.086823\n",
      "  Epoch  85 — Loss: 0.087236\n",
      "  ↳ small improvement (Δ=-4.13e-04) stale 1/3\n",
      "  Epoch  86 — Loss: 0.086292\n",
      "  Epoch  87 — Loss: 0.085980\n",
      "  Epoch  88 — Loss: 0.084313\n",
      "  Epoch  89 — Loss: 0.086325\n",
      "  ↳ small improvement (Δ=-2.01e-03) stale 1/3\n",
      "  Epoch  90 — Loss: 0.083745\n",
      "  Epoch  91 — Loss: 0.082256\n",
      "  Epoch  92 — Loss: 0.081721\n",
      "  Epoch  93 — Loss: 0.082322\n",
      "  ↳ small improvement (Δ=-6.02e-04) stale 1/3\n",
      "  Epoch  94 — Loss: 0.081694\n",
      "  ↳ small improvement (Δ=2.64e-05) stale 2/3\n",
      "  Epoch  95 — Loss: 0.081169\n",
      "  Epoch  96 — Loss: 0.082381\n",
      "  ↳ small improvement (Δ=-1.21e-03) stale 1/3\n",
      "  Epoch  97 — Loss: 0.083466\n",
      "  ↳ small improvement (Δ=-2.30e-03) stale 2/3\n",
      "  Epoch  98 — Loss: 0.082519\n",
      "  ↳ small improvement (Δ=-1.35e-03) stale 3/3\n",
      "  ↳ Early stopping at epoch 98\n",
      "Metrics for group3:\n",
      "  Accuracy    :  55.94%\n",
      "  Micro-F1    :  55.94%\n",
      "  Macro-F1    :  55.20%\n",
      "  Weighted-F1 :  55.92%\n",
      "  Macro-Recall:  55.20%\n",
      "\n",
      "=== Tuning on group4 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cd61fd58c4149bbbebe46ad669d9a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82b91f06b408433391de80d63c071fde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch   1 — Loss: 0.686065\n",
      "  Epoch   2 — Loss: 0.657601\n",
      "  Epoch   3 — Loss: 0.638733\n",
      "  Epoch   4 — Loss: 0.622019\n",
      "  Epoch   5 — Loss: 0.604479\n",
      "  Epoch   6 — Loss: 0.587428\n",
      "  Epoch   7 — Loss: 0.571560\n",
      "  Epoch   8 — Loss: 0.555145\n",
      "  Epoch   9 — Loss: 0.536093\n",
      "  Epoch  10 — Loss: 0.518895\n",
      "  Epoch  11 — Loss: 0.501014\n",
      "  Epoch  12 — Loss: 0.481923\n",
      "  Epoch  13 — Loss: 0.466531\n",
      "  Epoch  14 — Loss: 0.447624\n",
      "  Epoch  15 — Loss: 0.430480\n",
      "  Epoch  16 — Loss: 0.411713\n",
      "  Epoch  17 — Loss: 0.395971\n",
      "  Epoch  18 — Loss: 0.377812\n",
      "  Epoch  19 — Loss: 0.364514\n",
      "  Epoch  20 — Loss: 0.347185\n",
      "  Epoch  21 — Loss: 0.331965\n",
      "  Epoch  22 — Loss: 0.315839\n",
      "  Epoch  23 — Loss: 0.303425\n",
      "  Epoch  24 — Loss: 0.288428\n",
      "  Epoch  25 — Loss: 0.277847\n",
      "  Epoch  26 — Loss: 0.263011\n",
      "  Epoch  27 — Loss: 0.252525\n",
      "  Epoch  28 — Loss: 0.241455\n",
      "  Epoch  29 — Loss: 0.229429\n",
      "  Epoch  30 — Loss: 0.220189\n",
      "  Epoch  31 — Loss: 0.208129\n",
      "  Epoch  32 — Loss: 0.200499\n",
      "  Epoch  33 — Loss: 0.191533\n",
      "  Epoch  34 — Loss: 0.182055\n",
      "  Epoch  35 — Loss: 0.173931\n",
      "  Epoch  36 — Loss: 0.167314\n",
      "  Epoch  37 — Loss: 0.161209\n",
      "  Epoch  38 — Loss: 0.152652\n",
      "  Epoch  39 — Loss: 0.146029\n",
      "  Epoch  40 — Loss: 0.139625\n",
      "  Epoch  41 — Loss: 0.134909\n",
      "  Epoch  42 — Loss: 0.129268\n",
      "  Epoch  43 — Loss: 0.124147\n",
      "  Epoch  44 — Loss: 0.119951\n",
      "  Epoch  45 — Loss: 0.114851\n",
      "  Epoch  46 — Loss: 0.109192\n",
      "  Epoch  47 — Loss: 0.106547\n",
      "  Epoch  48 — Loss: 0.102794\n",
      "  Epoch  49 — Loss: 0.098249\n",
      "  Epoch  50 — Loss: 0.095879\n",
      "  Epoch  51 — Loss: 0.093300\n",
      "  Epoch  52 — Loss: 0.088610\n",
      "  Epoch  53 — Loss: 0.086584\n",
      "  Epoch  54 — Loss: 0.083435\n",
      "  Epoch  55 — Loss: 0.081478\n",
      "  Epoch  56 — Loss: 0.080679\n",
      "  Epoch  57 — Loss: 0.075797\n",
      "  Epoch  58 — Loss: 0.075560\n",
      "  Epoch  59 — Loss: 0.072865\n",
      "  Epoch  60 — Loss: 0.070552\n",
      "  Epoch  61 — Loss: 0.070396\n",
      "  Epoch  62 — Loss: 0.069381\n",
      "  Epoch  63 — Loss: 0.068748\n",
      "  Epoch  64 — Loss: 0.065623\n",
      "  Epoch  65 — Loss: 0.065493\n",
      "  Epoch  66 — Loss: 0.061878\n",
      "  Epoch  67 — Loss: 0.061977\n",
      "  ↳ small improvement (Δ=-9.99e-05) stale 1/3\n",
      "  Epoch  68 — Loss: 0.060984\n",
      "  Epoch  69 — Loss: 0.060963\n",
      "  ↳ small improvement (Δ=2.10e-05) stale 1/3\n",
      "  Epoch  70 — Loss: 0.061636\n",
      "  ↳ small improvement (Δ=-6.52e-04) stale 2/3\n",
      "  Epoch  71 — Loss: 0.056472\n",
      "  Epoch  72 — Loss: 0.055863\n",
      "  Epoch  73 — Loss: 0.056506\n",
      "  ↳ small improvement (Δ=-6.43e-04) stale 1/3\n",
      "  Epoch  74 — Loss: 0.054601\n",
      "  Epoch  75 — Loss: 0.053135\n",
      "  Epoch  76 — Loss: 0.054070\n",
      "  ↳ small improvement (Δ=-9.36e-04) stale 1/3\n",
      "  Epoch  77 — Loss: 0.053583\n",
      "  ↳ small improvement (Δ=-4.48e-04) stale 2/3\n",
      "  Epoch  78 — Loss: 0.050734\n",
      "  Epoch  79 — Loss: 0.052034\n",
      "  ↳ small improvement (Δ=-1.30e-03) stale 1/3\n",
      "  Epoch  80 — Loss: 0.051489\n",
      "  ↳ small improvement (Δ=-7.55e-04) stale 2/3\n",
      "  Epoch  81 — Loss: 0.049522\n",
      "  Epoch  82 — Loss: 0.051652\n",
      "  ↳ small improvement (Δ=-2.13e-03) stale 1/3\n",
      "  Epoch  83 — Loss: 0.046520\n",
      "  Epoch  84 — Loss: 0.047814\n",
      "  ↳ small improvement (Δ=-1.29e-03) stale 1/3\n",
      "  Epoch  85 — Loss: 0.047043\n",
      "  ↳ small improvement (Δ=-5.23e-04) stale 2/3\n",
      "  Epoch  86 — Loss: 0.047104\n",
      "  ↳ small improvement (Δ=-5.84e-04) stale 3/3\n",
      "  ↳ Early stopping at epoch 86\n",
      "Metrics for group4:\n",
      "  Accuracy    :  53.44%\n",
      "  Micro-F1    :  53.44%\n",
      "  Macro-F1    :  52.81%\n",
      "  Weighted-F1 :  53.42%\n",
      "  Macro-Recall:  52.81%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for group_name, labeled_group in [\n",
    "    (\"group2\", labeled_group2),\n",
    "    (\"group3\", labeled_group3),\n",
    "    (\"group4\", labeled_group4),\n",
    "]:\n",
    "    print(f\"=== Tuning on {group_name} ===\")\n",
    "    df_all = pd.concat(labeled_group.values(), ignore_index=True)\n",
    "    texts = df_all[\"title\"].tolist()\n",
    "    labels = df_all[\"label\"].astype(int).tolist()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        texts, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "    emb_train = model_3.encode(\n",
    "        X_train, device=device, batch_size=16,\n",
    "        convert_to_tensor=True, show_progress_bar=True)\n",
    "    del X_train\n",
    "\n",
    "    emb_test = model_3.encode(\n",
    "        X_test, device=device, batch_size=16,\n",
    "        convert_to_tensor=True, show_progress_bar=True)\n",
    "    del X_test\n",
    "\n",
    "    d = emb_train.size(1)\n",
    "    head = nn.Sequential(\n",
    "        nn.Linear(d, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 2)).to(device)\n",
    "\n",
    "    trainable = list(head.parameters()) + [\n",
    "        p for p in model_3.parameters() if p.requires_grad]\n",
    "\n",
    "    optimizer = torch.optim.Adam(trainable, lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_ds = TensorDataset(emb_train, torch.tensor(y_train, device=device))\n",
    "    train_dl = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "\n",
    "    best_prev_loss = float(\"inf\")\n",
    "    tol = 1e-4\n",
    "    patience = 3\n",
    "    stale_epochs = 0\n",
    "    max_epochs = 100\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        head.train()\n",
    "        epoch_loss = 0.0\n",
    "        for emb_b, lab_b in train_dl:\n",
    "            optimizer.zero_grad()\n",
    "            logits = head(emb_b)\n",
    "            loss = criterion(logits, lab_b)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * emb_b.size(0)\n",
    "        avg_loss = epoch_loss / len(train_ds)\n",
    "        print(f\"  Epoch {epoch:>3} — Loss: {avg_loss:.6f}\")\n",
    "        improvement = best_prev_loss - avg_loss\n",
    "        if improvement < tol:\n",
    "            stale_epochs += 1\n",
    "            print(f\"  ↳ small improvement (Δ={improvement:.2e}) stale {stale_epochs}/{patience}\")\n",
    "        else:\n",
    "            stale_epochs = 0\n",
    "            best_prev_loss = avg_loss\n",
    "        if stale_epochs >= patience:\n",
    "            print(f\"  ↳ Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "    head.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = head(emb_test.to(device))\n",
    "        preds = logits.argmax(dim=1).cpu().tolist()\n",
    "\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    micro_f1 = f1_score(y_test, preds, average=\"micro\")\n",
    "    macro_f1 = f1_score(y_test, preds, average=\"macro\")\n",
    "    weighted_f1 = f1_score(y_test, preds, average=\"weighted\")\n",
    "    macro_recall = recall_score(y_test, preds, average=\"macro\")\n",
    "\n",
    "    print(f\"Metrics for {group_name}:\")\n",
    "    print(f\"  Accuracy    : {acc*100:6.2f}%\")\n",
    "    print(f\"  Micro-F1    : {micro_f1*100:6.2f}%\")\n",
    "    print(f\"  Macro-F1    : {macro_f1*100:6.2f}%\")\n",
    "    print(f\"  Weighted-F1 : {weighted_f1*100:6.2f}%\")\n",
    "    print(f\"  Macro-Recall: {macro_recall*100:6.2f}%\\n\")\n",
    "\n",
    "    results[group_name] = head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 669,
     "referenced_widgets": [
      "0df5bc6177ea4b40a410dd47b1e23992",
      "3174de2f0cf0464c8e875a1855f3ffad",
      "76830801b6ff49f88c37ea3d04dcf72a",
      "5c145212f48840c4b5068605ae2def3f",
      "c6024792bccd4d1689ad4b2003e4b3a0",
      "c8fa2534e7514ae6a46d50956ad1111c",
      "2112388519c6493295e47127c656cf47",
      "008f4e055e0c4ed69e0814d8406f98e1",
      "37169ad49615421687ac19fb8f6c8e92",
      "5e64aa16b0324d698bbf33218c057022",
      "cf912ca743ea4d68acc3d42f0873fe01",
      "1562080926cf4fd59aa5806a0086889f",
      "8cb6525a5ada45cf9c830e2de0bfecaa",
      "b61c01843e6f46518d391a5b1e51af39",
      "9543965dfa574ea08da67bae259d934b",
      "6188cf517cef4d7083f92d2ebdb3f816",
      "bd5dd3a0b81143a1a0473a4abc48bed2",
      "ddb2dcb9e2d94213bd42cf52c6770b21",
      "f2847f6d3917497b85992917a6a59c63",
      "c9c9a027ac7a49dfb4b9f33ab628756c",
      "7d2e54f7f69647c0b7bd9b783f07e4d5",
      "d4680263381545bd8cb0e9f771430ecd",
      "588b089ed9ca44769d4a8c83f9b0b7a9",
      "ee779a1903a34caba54b32e0feecca45",
      "178f755b2f3d4d6bb98730e3c17bd3e1",
      "786b52ad2e3845479db668f42a00d482",
      "e1110c9debc94aa9b2261d02e8dfd88f",
      "b4efbfea80984a73acdce052173e865c",
      "6973c83dc60c47a3a2c141f58b156dcc",
      "0d8944a68b1e404aa5ef26739eb4b936",
      "4e33a8c3f8e5421d9d00c0f1ea1c912f",
      "e14d1ca0a33345549d914f491b75c676",
      "4a88b6436c2b419f9bd2f19e7b224dce"
     ]
    },
    "executionInfo": {
     "elapsed": 6598,
     "status": "ok",
     "timestamp": 1754371936964,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -480
    },
    "id": "VNhnswdlrBzr",
    "outputId": "2481e6d2-545d-4939-df3f-edaab6e7d3b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Re-evaluating group2 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0df5bc6177ea4b40a410dd47b1e23992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for group2:\n",
      "  Accuracy    :  58.44%\n",
      "  Micro-F1    :  58.44%\n",
      "  Macro-F1    :  58.04%\n",
      "  Weighted-F1 :  58.37%\n",
      "  Macro-Recall:  58.03%\n",
      "\n",
      "=== Re-evaluating group3 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1562080926cf4fd59aa5806a0086889f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for group3:\n",
      "  Accuracy    :  55.94%\n",
      "  Micro-F1    :  55.94%\n",
      "  Macro-F1    :  55.20%\n",
      "  Weighted-F1 :  55.92%\n",
      "  Macro-Recall:  55.20%\n",
      "\n",
      "=== Re-evaluating group4 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "588b089ed9ca44769d4a8c83f9b0b7a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for group4:\n",
      "  Accuracy    :  53.44%\n",
      "  Micro-F1    :  53.44%\n",
      "  Macro-F1    :  52.81%\n",
      "  Weighted-F1 :  53.42%\n",
      "  Macro-Recall:  52.81%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Re-evaluate with updated embedding and head weights\n",
    "for group_name, labeled_group in [\n",
    "    (\"group2\", labeled_group2),\n",
    "    (\"group3\", labeled_group3),\n",
    "    (\"group4\", labeled_group4),\n",
    "]:\n",
    "    print(f\"=== Re-evaluating {group_name} ===\")\n",
    "    df_all = pd.concat(labeled_group.values(), ignore_index=True)\n",
    "    texts  = df_all[\"title\"].tolist()\n",
    "    labels = df_all[\"label\"].astype(int).tolist()\n",
    "    _, X_test, _, y_test = train_test_split(\n",
    "        texts, labels,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=labels\n",
    "    )\n",
    "    emb_test_updated = model_3.encode(\n",
    "        X_test,\n",
    "        device=device,\n",
    "        batch_size=16,\n",
    "        convert_to_tensor=True,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    head = results[group_name]\n",
    "    head.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = head(emb_test_updated.to(device))\n",
    "        preds  = logits.argmax(dim=1).cpu().tolist()\n",
    "    acc          = accuracy_score(y_test, preds)\n",
    "    micro_f1     = f1_score(y_test, preds, average=\"micro\")\n",
    "    macro_f1     = f1_score(y_test, preds, average=\"macro\")\n",
    "    weighted_f1  = f1_score(y_test, preds, average=\"weighted\")\n",
    "    macro_recall = recall_score(y_test, preds, average=\"macro\")\n",
    "    print(f\"Metrics for {group_name}:\")\n",
    "    print(f\"  Accuracy    : {acc*100:6.2f}%\")\n",
    "    print(f\"  Micro-F1    : {micro_f1*100:6.2f}%\")\n",
    "    print(f\"  Macro-F1    : {macro_f1*100:6.2f}%\")\n",
    "    print(f\"  Weighted-F1 : {weighted_f1*100:6.2f}%\")\n",
    "    print(f\"  Macro-Recall: {macro_recall*100:6.2f}%\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPDOu/OcMIkYGDzCn+uMVRu",
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
