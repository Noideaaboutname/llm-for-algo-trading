{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36275,
     "status": "ok",
     "timestamp": 1757306689488,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -480
    },
    "id": "qCDnr0lbbZDu",
    "outputId": "11d24702-320c-44bb-a6ef-16bff3d6bbfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "✅ Clean copy saved to /content/drive/MyDrive/colab_saves/RAG for DS and QWEN.ipynb\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import nbformat, os\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "SRC = '/content/drive/MyDrive/Colab Notebooks/Untitled13.ipynb'\n",
    "DST = '/content/drive/MyDrive/colab_saves/RAG for DS and QWEN.ipynb'\n",
    "\n",
    "with open(SRC, 'r', encoding='utf-8') as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "\n",
    "nb.metadata.pop('widgets', None)\n",
    "for cell in nb.cells:\n",
    "    cell.metadata.pop('widgets', None)\n",
    "os.makedirs(os.path.dirname(DST), exist_ok=True)\n",
    "with open(DST, 'w', encoding='utf-8') as f:\n",
    "    nbformat.write(nb, f)\n",
    "\n",
    "print(\"✅ Clean copy saved to\", DST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336,
     "referenced_widgets": [
      "415148d402e64f878b5eaed626fd5307",
      "be1d241e81c84860a90246951cbdbde4",
      "ed13595cf3d040a7a785c91384c1e90e",
      "ce02473c852944c888b8d124eb796e2e",
      "f9f2d00d321340fc99bc26b8151b9eeb",
      "36a992cc33e74fad91e2cabaea9f8c04",
      "42ff515d4da34dac99388602e5a51957",
      "a0403041ac8443b092ad5c651ed86cc6",
      "fec4ee6dba984dd893645b54495821ba",
      "fe89b8f087414aca9be6cd4da185b57c",
      "4b3eed2cdbb941d992655d0ddc9211a3",
      "67e051966c584bae81f42d4f38036336",
      "d3cfa1e872b04d638d5c8bf544969715",
      "6f56906e9d50494989c2794b30c880df",
      "fb5f02a7c2234d28bbc929ed2546eab3",
      "8a947e5881ce41029540ae80c9ffdaf6",
      "ad7e9f9c76e14a249a0631c9f6bd6b8f",
      "e726579cf540433ba3070e88a535a26a",
      "b33e0207cc57495fb2099660752ff60c",
      "a7b87465bec54def9b951fb1ee7c9c41",
      "b254ad69c5584bb8bb0dc36ff4024479",
      "bf8ae371886e40f1a07817c8cbb194e0",
      "979bef25034e40d8822ffaa85bf3ecd2",
      "4826284eca054459bea081ce77045f9f",
      "222d41d77a2240fbaaeb7e73623c3aeb",
      "99a6b97b87094d1fafd5989d2bb40c6b",
      "c51fc7e25d6f489fb199a14c94011953",
      "61e896b7c4294bc892bfcd461c935e65",
      "23d3bc4d11254c0183ae038cd74cfa94",
      "e03e2b4d0fdc41e28d545f20c120a134",
      "0b2a37cc4a404008a006ffbfe9c0af28",
      "1eae52ecd96248f5bf70ab5665627ca4",
      "fface84958e54685a01b8e6c390d050a",
      "d86f85d768134ebd91cc5a7f43252797",
      "1785eb4476f24f93ab86c26eeef54e17",
      "b2ba37d42c724234ac75a357055c4ca2",
      "afdc944c7e3f46ff9b169ff0e99f3b3a",
      "8806bb8bb6064d3fa197e340bafbd4e9",
      "0a6670a01f2b497faf608e8e76e59680",
      "68a1262df8954cb9906703dfec1462ab",
      "4b309df277a54273b54b3510b78fcaf2",
      "0168ed649e0f4b1293fb0cf3c1014f26",
      "669fbbd0b322481d994e62b0558583d9",
      "f5df50280f7c4364a2dc370a796460e4",
      "0ae477b7346a4598a60772ccbbc60a53",
      "e6994eee329f44d990b2d08569b0f4c5",
      "cbcf1ddcba1b4b4d9cfd70ccb9a49a7f",
      "88d4c57073f14b5aaca65b52ed0cc628",
      "3165c1f4f43149048365b13540451419",
      "56853f1e8ed54f3792be210558e0fb7d",
      "6339dae7f9d146918466e4bbb5bf4437",
      "120b434f537c4e7d95fb774e99efb23c",
      "870cbb01ba1c4494a66dd351c968f614",
      "7f3a85cd345e42b2b065a298ee6db48b",
      "900b328eaec54e08b9eb3a93674e58ea",
      "eb48ae5c7e1e4a1282ba88de2267e94e",
      "a3b97f9471e1478980a385153ab20104",
      "75b1f03615f14b98a693172cbfc0b881",
      "3256ffc347c44e5492e97e46fd26d06c",
      "8c0e5358d3e742c9acb0004237533d4f",
      "47de77dd009c464f8a176aa8c5802b12",
      "2dbe63f263f749f0bd56e8336130c0cd",
      "7e11b9b7906e43ac9bc9b1430b8f4a4f",
      "9ef52d47276b47f0896ba4e6f3452366",
      "90cca9d98e5340668e0aeb96bb0d7e59",
      "5157f65bde6247b0b8f999f945d905d6"
     ]
    },
    "executionInfo": {
     "elapsed": 27044,
     "status": "ok",
     "timestamp": 1756093414784,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -480
    },
    "id": "QHcU1oK2aMNt",
    "outputId": "d0b8e4e0-dc73-4e71-90e1-444c42d70ca7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "415148d402e64f878b5eaed626fd5307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e051966c584bae81f42d4f38036336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "979bef25034e40d8822ffaa85bf3ecd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d86f85d768134ebd91cc5a7f43252797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae477b7346a4598a60772ccbbc60a53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb48ae5c7e1e4a1282ba88de2267e94e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.19G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "MODEL_ID = \"Qwen/Qwen3-Embedding-0.6B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModel.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    output_hidden_states=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 65543,
     "status": "ok",
     "timestamp": 1757054985148,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -480
    },
    "id": "eqQphdYCw-4f",
    "outputId": "5b68325a-1e97-4fa1-cf1f-e98612080828"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "\n",
      "group2:\n",
      "C (400, 21) ['id', 'type', 'publishOn', 'attributes.isLockedPro', 'attributes.commentCount']\n",
      "GS (400, 21) ['id', 'type', 'publishOn', 'attributes.isLockedPro', 'attributes.commentCount']\n",
      "MS (400, 21) ['id', 'type', 'publishOn', 'attributes.isLockedPro', 'attributes.commentCount']\n",
      "USB (400, 21) ['id', 'type', 'publishOn', 'attributes.isLockedPro', 'attributes.commentCount']\n",
      "\n",
      "group3:\n",
      "AAPL (400, 21) ['id', 'type', 'publishOn', 'attributes.isLockedPro', 'attributes.commentCount']\n",
      "MSFT (400, 21) ['id', 'type', 'publishOn', 'attributes.isLockedPro', 'attributes.commentCount']\n",
      "AMZN (400, 21) ['id', 'type', 'publishOn', 'attributes.isLockedPro', 'attributes.commentCount']\n",
      "GOOGL (400, 21) ['id', 'type', 'publishOn', 'attributes.isLockedPro', 'attributes.commentCount']\n",
      "\n",
      "group4:\n",
      "META (400, 21) ['id', 'type', 'publishOn', 'attributes.isLockedPro', 'attributes.commentCount']\n",
      "NVDA (400, 21) ['id', 'type', 'publishOn', 'attributes.isLockedPro', 'attributes.commentCount']\n",
      "TSLA (400, 21) ['id', 'type', 'publishOn', 'attributes.isLockedPro', 'attributes.commentCount']\n",
      "NFLX (400, 21) ['id', 'type', 'publishOn', 'attributes.isLockedPro', 'attributes.commentCount']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pickle, pandas as pd\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "with open('/content/drive/My Drive/colab_saves/company_dfs.pkl', 'rb') as f:\n",
    "    labeled_groups = pickle.load(f)\n",
    "\n",
    "labeled_group2 = labeled_groups['group2']\n",
    "labeled_group3 = labeled_groups['group3']\n",
    "labeled_group4 = labeled_groups['group4']\n",
    "\n",
    "for name, group in [('group2', labeled_group2),\n",
    "                    ('group3', labeled_group3),\n",
    "                    ('group4', labeled_group4)]:\n",
    "    print(f\"\\n{name}:\")\n",
    "    for sym, df in group.items():\n",
    "        print(sym, df.shape, list(df.columns)[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Q-kCrH5wwo0"
   },
   "outputs": [],
   "source": [
    "\n",
    "import re, html, pandas as pd\n",
    "\n",
    "def _first_para_from_html(s: str, max_chars=420):\n",
    "    s = str(s)\n",
    "    s = re.sub(r\"<[^>]+>\", \" \", s)         # remove tags\n",
    "    s = html.unescape(\" \".join(s.split())) # remove whitespace\n",
    "    return s[:max_chars]\n",
    "\n",
    "def make_snippet_df(group_dict: dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for tkr, df in group_dict.items():\n",
    "        # keep needed columns\n",
    "        need = [\"symbol\", \"publishOn\", \"title\"]\n",
    "        if \"attributes.content\" in df.columns:\n",
    "            need.append(\"attributes.content\") # add the aim body of news if it exist\n",
    "        sdf = df[need].copy()\n",
    "        sdf[\"publishOn\"] = pd.to_datetime(sdf[\"publishOn\"], utc=True, errors=\"coerce\") #convert to utc time\n",
    "        sdf = sdf.dropna(subset=[\"publishOn\"])\n",
    "\n",
    "        for sym, ts, ttl in sdf[[\"symbol\",\"publishOn\",\"title\"]].itertuples(index=False, name=None):\n",
    "            rows.append({\"ticker\": sym, \"time\": ts, \"text\": str(ttl), \"kind\": \"headline\"})\n",
    "\n",
    "        if \"attributes.content\" in sdf.columns:\n",
    "            mask = sdf[\"attributes.content\"].notna()\n",
    "            for sym, ts, body in sdf.loc[mask, [\"symbol\",\"publishOn\",\"attributes.content\"]].itertuples(index=False, name=None):\n",
    "                rows.append({\"ticker\": sym, \"time\": ts,\n",
    "                             \"text\": _first_para_from_html(body), \"kind\": \"first_para\"})\n",
    "\n",
    "    snips = pd.DataFrame(rows).drop_duplicates(subset=[\"ticker\",\"time\",\"text\"]).reset_index(drop=True)\n",
    "    return snips\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 269,
     "status": "ok",
     "timestamp": 1756093439905,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -480
    },
    "id": "SW5t_3FZyxWL",
    "outputId": "78743e6e-3d77-45cf-b37d-95bd26721a74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ticker                      time  \\\n",
      "0      C 2025-07-15 20:38:20+00:00   \n",
      "1      C 2025-07-15 19:49:24+00:00   \n",
      "2      C 2025-07-15 17:45:07+00:00   \n",
      "3      C 2025-07-15 13:00:54+00:00   \n",
      "4      C 2025-07-15 12:51:29+00:00   \n",
      "\n",
      "                                                text      kind  \n",
      "0  Citi targets $84B revenue for 2025 while advan...  headline  \n",
      "1  Citi's June credit card delinquencies, charge-...  headline  \n",
      "2  Citigroup stock climbs as it ramps up stock bu...  headline  \n",
      "3  5 stocks to watch on Tuesday: C, JPM, WFC, BLK...  headline  \n",
      "4  Citigroup boosts NII guidance after Q2 earning...  headline   (2320, 4)\n",
      "  ticker                      time  \\\n",
      "0   AAPL 2025-07-16 06:07:21+00:00   \n",
      "1   AAPL 2025-07-16 01:25:36+00:00   \n",
      "2   AAPL 2025-07-15 19:22:41+00:00   \n",
      "3   AAPL 2025-07-15 18:07:31+00:00   \n",
      "4   AAPL 2025-07-15 15:05:04+00:00   \n",
      "\n",
      "                                                text      kind  \n",
      "0         Trump eyes drug, chip tariffs by month-end  headline  \n",
      "1  MP Materials rips to all-time high after $500M...  headline  \n",
      "2  Hearing, sleep apnea features for AirPods, App...  headline  \n",
      "3  Tech Voices: Nvidia, AMD China exports; Apple-...  headline  \n",
      "4  Globalstar, U.S. Army collaborate on defense a...  headline   (2440, 4)\n",
      "  ticker                      time  \\\n",
      "0   AAPL 2025-07-16 06:07:21+00:00   \n",
      "1   AAPL 2025-07-16 01:25:36+00:00   \n",
      "2   AAPL 2025-07-15 19:22:41+00:00   \n",
      "3   AAPL 2025-07-15 18:07:31+00:00   \n",
      "4   AAPL 2025-07-15 15:05:04+00:00   \n",
      "\n",
      "                                                text      kind  \n",
      "0         Trump eyes drug, chip tariffs by month-end  headline  \n",
      "1  MP Materials rips to all-time high after $500M...  headline  \n",
      "2  Hearing, sleep apnea features for AirPods, App...  headline  \n",
      "3  Tech Voices: Nvidia, AMD China exports; Apple-...  headline  \n",
      "4  Globalstar, U.S. Army collaborate on defense a...  headline   (2440, 4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "snips_df_2 = make_snippet_df(labeled_group2)\n",
    "print(snips_df_2.head(), snips_df_2.shape)\n",
    "\n",
    "snips_df_3 = make_snippet_df(labeled_group3)\n",
    "print(snips_df_3.head(), snips_df_3.shape)\n",
    "\n",
    "\n",
    "snips_df_4 = make_snippet_df(labeled_group3)\n",
    "print(snips_df_4.head(), snips_df_3.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gbjbn0D5yiUY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "MAX_LEN = 128\n",
    "# generate embeddings\n",
    "@torch.no_grad()\n",
    "def qwen_embed_batched(texts, batch_size=128, pool=\"mean\"):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    vecs = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        chunk = texts[i:i+batch_size]\n",
    "        enc = tokenizer(\n",
    "            chunk,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LEN,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "        out = model(**enc)\n",
    "        H = out.hidden_states[-1] if hasattr(out, \"hidden_states\") else out.last_hidden_state\n",
    "        E = H.mean(dim=1) if pool == \"mean\" else H[:, 0]\n",
    "        E = torch.nn.functional.normalize(E.float(), p=2, dim=1)\n",
    "        vecs.append(E.cpu().numpy().astype(\"float32\"))\n",
    "        del enc, out, H, E\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    return np.vstack(vecs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UoLS96GuymMK"
   },
   "outputs": [],
   "source": [
    "\n",
    "class TickerIndex:\n",
    "    def __init__(self, snips_df: pd.DataFrame, batch_size=128):\n",
    "        self.df_by_tkr, self.vec_by_tkr = {}, {}\n",
    "        for tkr, sdf in snips_df.groupby(\"ticker\"):\n",
    "            texts = sdf[\"text\"].tolist()\n",
    "            vecs  = qwen_embed_batched(texts, batch_size=batch_size)\n",
    "            self.df_by_tkr[tkr]  = sdf.reset_index(drop=True)\n",
    "            self.vec_by_tkr[tkr] = vecs\n",
    "\n",
    "# given a query, search most relavant snipet by cos similarity\n",
    "    def retrieve(self, ticker: str, cutoff_time, query_text: str | None = None,\n",
    "                top_r: int = 2, qvec: np.ndarray | None = None):\n",
    "        if ticker not in self.df_by_tkr:\n",
    "            return []\n",
    "        sdf  = self.df_by_tkr[ticker]\n",
    "        vecs = self.vec_by_tkr[ticker]\n",
    "        mask = sdf[\"time\"] <= pd.to_datetime(cutoff_time, utc=True)\n",
    "        if not mask.any():\n",
    "            return []\n",
    "        sub  = sdf[mask]\n",
    "        subV = vecs[mask.values].astype(\"float32\")\n",
    "        if qvec is None:\n",
    "            assert query_text is not None, \"Need query_text when qvec is None\"\n",
    "            q = qwen_embed_batched([query_text])[0].astype(\"float32\")\n",
    "        else:\n",
    "            q = qvec.astype(\"float32\")\n",
    "\n",
    "        sims = subV @ q\n",
    "        top  = sims.argsort()[-top_r:][::-1]\n",
    "        return sub.iloc[top][\"text\"].tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W9PifvYp1ffi"
   },
   "outputs": [],
   "source": [
    "\n",
    "idx_2 = TickerIndex(snips_df_2)\n",
    "idx_3 = TickerIndex(snips_df_3)\n",
    "idx_4 = TickerIndex(snips_df_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VbgMc3huoMqy"
   },
   "outputs": [],
   "source": [
    "\n",
    "_title_cache: dict[str, np.ndarray] = {}\n",
    "\n",
    "def embed_title_cached(text: str) -> np.ndarray:\n",
    "    k = text.strip()\n",
    "    v = _title_cache.get(k)\n",
    "    if v is None:\n",
    "        v = qwen_embed_batched([k])[0]\n",
    "        _title_cache[k] = v\n",
    "    return v\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evMb55QDzXS8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "def rag_features_from_group(group_dict, idx: TickerIndex, top_r=1): # concatenate snippets to embeddings\n",
    "\n",
    "    X, y, meta = [], [], []\n",
    "\n",
    "    for tkr, df in group_dict.items():\n",
    "        sdf = df[[\"symbol\",\"publishOn\",\"title\",\"label\"]].copy()\n",
    "        sdf[\"publishOn\"] = pd.to_datetime(sdf[\"publishOn\"], utc=True, errors=\"coerce\")\n",
    "        sdf = sdf.dropna(subset=[\"publishOn\"]).reset_index(drop=True)\n",
    "        titles = sdf[\"title\"].astype(str).tolist()\n",
    "        title_vecs = qwen_embed_batched(titles, batch_size=16)\n",
    "\n",
    "\n",
    "        for i, (sym, ts, ttl, lab) in enumerate(sdf.itertuples(index=False, name=None)):\n",
    "            qvec = title_vecs[i]\n",
    "\n",
    "            ctx_texts = idx.retrieve(sym, ts, query_text=None, top_r=top_r, qvec=qvec)\n",
    "\n",
    "\n",
    "            if ctx_texts:\n",
    "                ctx_vec = qwen_embed_batched(ctx_texts, batch_size=16).mean(axis=0)\n",
    "            else:\n",
    "                ctx_vec = np.zeros_like(qvec)\n",
    "\n",
    "            X.append(np.concatenate([qvec, ctx_vec], axis=0))\n",
    "            y.append(int(lab))\n",
    "            meta.append((sym, ts, ttl))\n",
    "\n",
    "            if i % 200 == 0:\n",
    "                print(f\"[{tkr}] row {i}/{len(sdf)}\", flush=True)\n",
    "\n",
    "    return np.vstack(X), np.asarray(y), meta\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 191461,
     "status": "ok",
     "timestamp": 1756093640438,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -480
    },
    "id": "Sqm6dkGu11eQ",
    "outputId": "1bd0928f-d029-4590-eea2-0e1f7b3cc505"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C] row 0/400\n",
      "[C] row 200/400\n",
      "[GS] row 0/400\n",
      "[GS] row 200/400\n",
      "[MS] row 0/400\n",
      "[MS] row 200/400\n",
      "[USB] row 0/400\n",
      "[USB] row 200/400\n",
      "[AAPL] row 0/400\n",
      "[AAPL] row 200/400\n",
      "[MSFT] row 0/400\n",
      "[MSFT] row 200/400\n",
      "[AMZN] row 0/400\n",
      "[AMZN] row 200/400\n",
      "[GOOGL] row 0/400\n",
      "[GOOGL] row 200/400\n",
      "[META] row 0/400\n",
      "[META] row 200/400\n",
      "[NVDA] row 0/400\n",
      "[NVDA] row 200/400\n",
      "[TSLA] row 0/400\n",
      "[TSLA] row 200/400\n",
      "[NFLX] row 0/400\n",
      "[NFLX] row 200/400\n"
     ]
    }
   ],
   "source": [
    "import gc, torch\n",
    "\n",
    "\n",
    "idx_2 = TickerIndex(snips_df_2, batch_size=16)\n",
    "X2, y2, meta2 = rag_features_from_group(labeled_group2, idx_2, top_r=1)\n",
    "del idx_2; gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache(); torch.cuda.ipc_collect()\n",
    "\n",
    "\n",
    "idx_3 = TickerIndex(snips_df_3, batch_size=16)\n",
    "X3, y3, meta3 = rag_features_from_group(labeled_group3, idx_3, top_r=1)\n",
    "del idx_3; gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache(); torch.cuda.ipc_collect()\n",
    "\n",
    "\n",
    "idx_4 = TickerIndex(snips_df_4, batch_size=16)\n",
    "X4, y4, meta4 = rag_features_from_group(labeled_group4, idx_4, top_r=1)\n",
    "del idx_4; gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache(); torch.cuda.ipc_collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 296570,
     "status": "ok",
     "timestamp": 1756094372927,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -480
    },
    "id": "Z7STBXbBWGU1",
    "outputId": "02e1d4b3-86d3-401b-9906-64ef983c6fd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Tuning on group2 ===\n",
      "[C] row 0/400\n",
      "[C] row 200/400\n",
      "[GS] row 0/400\n",
      "[GS] row 200/400\n",
      "[MS] row 0/400\n",
      "[MS] row 200/400\n",
      "[USB] row 0/400\n",
      "[USB] row 200/400\n",
      "  Epoch   1 — Loss: 0.690810\n",
      "  Epoch   2 — Loss: 0.687239\n",
      "  Epoch   3 — Loss: 0.684674\n",
      "  Epoch   4 — Loss: 0.682368\n",
      "  Epoch   5 — Loss: 0.678872\n",
      "  Epoch   6 — Loss: 0.676158\n",
      "  Epoch   7 — Loss: 0.672376\n",
      "  Epoch   8 — Loss: 0.669481\n",
      "  Epoch   9 — Loss: 0.666599\n",
      "  Epoch  10 — Loss: 0.664147\n",
      "  Epoch  11 — Loss: 0.659586\n",
      "  Epoch  12 — Loss: 0.656305\n",
      "  Epoch  13 — Loss: 0.652638\n",
      "  Epoch  14 — Loss: 0.649676\n",
      "  Epoch  15 — Loss: 0.645413\n",
      "  Epoch  16 — Loss: 0.642419\n",
      "  Epoch  17 — Loss: 0.639104\n",
      "  Epoch  18 — Loss: 0.636601\n",
      "  Epoch  19 — Loss: 0.634641\n",
      "  Epoch  20 — Loss: 0.629648\n",
      "  Epoch  21 — Loss: 0.627287\n",
      "  Epoch  22 — Loss: 0.623647\n",
      "  Epoch  23 — Loss: 0.621115\n",
      "  Epoch  24 — Loss: 0.618727\n",
      "  Epoch  25 — Loss: 0.617229\n",
      "  Epoch  26 — Loss: 0.614277\n",
      "  Epoch  27 — Loss: 0.609355\n",
      "  Epoch  28 — Loss: 0.609942\n",
      "  ↳ small improvement (Δ=-5.86e-04) stale 1/3\n",
      "  Epoch  29 — Loss: 0.606443\n",
      "  Epoch  30 — Loss: 0.604199\n",
      "  Epoch  31 — Loss: 0.599685\n",
      "  Epoch  32 — Loss: 0.597271\n",
      "  Epoch  33 — Loss: 0.594517\n",
      "  Epoch  34 — Loss: 0.593783\n",
      "  Epoch  35 — Loss: 0.590897\n",
      "  Epoch  36 — Loss: 0.589091\n",
      "  Epoch  37 — Loss: 0.586509\n",
      "  Epoch  38 — Loss: 0.583884\n",
      "  Epoch  39 — Loss: 0.583241\n",
      "  Epoch  40 — Loss: 0.575331\n",
      "  Epoch  41 — Loss: 0.577828\n",
      "  ↳ small improvement (Δ=-2.50e-03) stale 1/3\n",
      "  Epoch  42 — Loss: 0.574995\n",
      "  Epoch  43 — Loss: 0.576209\n",
      "  ↳ small improvement (Δ=-1.21e-03) stale 1/3\n",
      "  Epoch  44 — Loss: 0.570225\n",
      "  Epoch  45 — Loss: 0.568885\n",
      "  Epoch  46 — Loss: 0.566986\n",
      "  Epoch  47 — Loss: 0.563919\n",
      "  Epoch  48 — Loss: 0.562054\n",
      "  Epoch  49 — Loss: 0.558844\n",
      "  Epoch  50 — Loss: 0.556362\n",
      "  Epoch  51 — Loss: 0.556465\n",
      "  ↳ small improvement (Δ=-1.03e-04) stale 1/3\n",
      "  Epoch  52 — Loss: 0.552630\n",
      "  Epoch  53 — Loss: 0.550919\n",
      "  Epoch  54 — Loss: 0.549066\n",
      "  Epoch  55 — Loss: 0.546197\n",
      "  Epoch  56 — Loss: 0.544383\n",
      "  Epoch  57 — Loss: 0.543231\n",
      "  Epoch  58 — Loss: 0.541664\n",
      "  Epoch  59 — Loss: 0.538565\n",
      "  Epoch  60 — Loss: 0.536179\n",
      "  Epoch  61 — Loss: 0.533901\n",
      "  Epoch  62 — Loss: 0.532573\n",
      "  Epoch  63 — Loss: 0.529940\n",
      "  Epoch  64 — Loss: 0.528389\n",
      "  Epoch  65 — Loss: 0.526744\n",
      "  Epoch  66 — Loss: 0.524911\n",
      "  Epoch  67 — Loss: 0.523514\n",
      "  Epoch  68 — Loss: 0.519442\n",
      "  Epoch  69 — Loss: 0.519752\n",
      "  ↳ small improvement (Δ=-3.10e-04) stale 1/3\n",
      "  Epoch  70 — Loss: 0.516771\n",
      "  Epoch  71 — Loss: 0.514016\n",
      "  Epoch  72 — Loss: 0.512417\n",
      "  Epoch  73 — Loss: 0.509683\n",
      "  Epoch  74 — Loss: 0.507772\n",
      "  Epoch  75 — Loss: 0.506917\n",
      "  Epoch  76 — Loss: 0.503991\n",
      "  Epoch  77 — Loss: 0.503514\n",
      "  Epoch  78 — Loss: 0.501617\n",
      "  Epoch  79 — Loss: 0.500787\n",
      "  Epoch  80 — Loss: 0.498009\n",
      "  Epoch  81 — Loss: 0.495978\n",
      "  Epoch  82 — Loss: 0.494913\n",
      "  Epoch  83 — Loss: 0.491674\n",
      "  Epoch  84 — Loss: 0.489930\n",
      "  Epoch  85 — Loss: 0.489430\n",
      "  Epoch  86 — Loss: 0.488806\n",
      "  Epoch  87 — Loss: 0.484287\n",
      "  Epoch  88 — Loss: 0.481869\n",
      "  Epoch  89 — Loss: 0.482061\n",
      "  ↳ small improvement (Δ=-1.92e-04) stale 1/3\n",
      "  Epoch  90 — Loss: 0.478076\n",
      "  Epoch  91 — Loss: 0.475935\n",
      "  Epoch  92 — Loss: 0.475341\n",
      "  Epoch  93 — Loss: 0.473415\n",
      "  Epoch  94 — Loss: 0.473991\n",
      "  ↳ small improvement (Δ=-5.76e-04) stale 1/3\n",
      "  Epoch  95 — Loss: 0.469744\n",
      "  Epoch  96 — Loss: 0.468098\n",
      "  Epoch  97 — Loss: 0.464920\n",
      "  Epoch  98 — Loss: 0.465567\n",
      "  ↳ small improvement (Δ=-6.47e-04) stale 1/3\n",
      "  Epoch  99 — Loss: 0.462303\n",
      "  Epoch 100 — Loss: 0.461771\n",
      "Metrics for group2:\n",
      "  Accuracy    :  58.44%\n",
      "  Micro-F1    :  58.44%\n",
      "  Macro-F1    :  58.14%\n",
      "  Weighted-F1 :  58.43%\n",
      "  Macro-Recall:  58.14%\n",
      "\n",
      "\n",
      "=== Tuning on group3 ===\n",
      "[AAPL] row 0/400\n",
      "[AAPL] row 200/400\n",
      "[MSFT] row 0/400\n",
      "[MSFT] row 200/400\n",
      "[AMZN] row 0/400\n",
      "[AMZN] row 200/400\n",
      "[GOOGL] row 0/400\n",
      "[GOOGL] row 200/400\n",
      "  Epoch   1 — Loss: 0.688217\n",
      "  Epoch   2 — Loss: 0.682780\n",
      "  Epoch   3 — Loss: 0.681094\n",
      "  Epoch   4 — Loss: 0.678522\n",
      "  Epoch   5 — Loss: 0.675604\n",
      "  Epoch   6 — Loss: 0.673129\n",
      "  Epoch   7 — Loss: 0.670528\n",
      "  Epoch   8 — Loss: 0.666900\n",
      "  Epoch   9 — Loss: 0.663805\n",
      "  Epoch  10 — Loss: 0.660567\n",
      "  Epoch  11 — Loss: 0.657243\n",
      "  Epoch  12 — Loss: 0.652964\n",
      "  Epoch  13 — Loss: 0.650969\n",
      "  Epoch  14 — Loss: 0.647566\n",
      "  Epoch  15 — Loss: 0.644098\n",
      "  Epoch  16 — Loss: 0.640423\n",
      "  Epoch  17 — Loss: 0.636507\n",
      "  Epoch  18 — Loss: 0.633668\n",
      "  Epoch  19 — Loss: 0.630320\n",
      "  Epoch  20 — Loss: 0.627993\n",
      "  Epoch  21 — Loss: 0.622719\n",
      "  Epoch  22 — Loss: 0.619309\n",
      "  Epoch  23 — Loss: 0.617077\n",
      "  Epoch  24 — Loss: 0.613428\n",
      "  Epoch  25 — Loss: 0.610687\n",
      "  Epoch  26 — Loss: 0.606676\n",
      "  Epoch  27 — Loss: 0.604841\n",
      "  Epoch  28 — Loss: 0.600767\n",
      "  Epoch  29 — Loss: 0.597577\n",
      "  Epoch  30 — Loss: 0.593846\n",
      "  Epoch  31 — Loss: 0.592630\n",
      "  Epoch  32 — Loss: 0.589134\n",
      "  Epoch  33 — Loss: 0.587797\n",
      "  Epoch  34 — Loss: 0.582748\n",
      "  Epoch  35 — Loss: 0.580488\n",
      "  Epoch  36 — Loss: 0.577488\n",
      "  Epoch  37 — Loss: 0.575547\n",
      "  Epoch  38 — Loss: 0.572698\n",
      "  Epoch  39 — Loss: 0.571230\n",
      "  Epoch  40 — Loss: 0.567018\n",
      "  Epoch  41 — Loss: 0.563972\n",
      "  Epoch  42 — Loss: 0.561859\n",
      "  Epoch  43 — Loss: 0.561980\n",
      "  ↳ small improvement (Δ=-1.21e-04) stale 1/3\n",
      "  Epoch  44 — Loss: 0.556952\n",
      "  Epoch  45 — Loss: 0.555018\n",
      "  Epoch  46 — Loss: 0.550658\n",
      "  Epoch  47 — Loss: 0.549972\n",
      "  Epoch  48 — Loss: 0.546674\n",
      "  Epoch  49 — Loss: 0.545427\n",
      "  Epoch  50 — Loss: 0.544197\n",
      "  Epoch  51 — Loss: 0.540360\n",
      "  Epoch  52 — Loss: 0.537017\n",
      "  Epoch  53 — Loss: 0.535665\n",
      "  Epoch  54 — Loss: 0.534689\n",
      "  Epoch  55 — Loss: 0.532040\n",
      "  Epoch  56 — Loss: 0.528851\n",
      "  Epoch  57 — Loss: 0.526403\n",
      "  Epoch  58 — Loss: 0.525650\n",
      "  Epoch  59 — Loss: 0.523015\n",
      "  Epoch  60 — Loss: 0.522077\n",
      "  Epoch  61 — Loss: 0.519273\n",
      "  Epoch  62 — Loss: 0.517434\n",
      "  Epoch  63 — Loss: 0.515178\n",
      "  Epoch  64 — Loss: 0.513258\n",
      "  Epoch  65 — Loss: 0.512491\n",
      "  Epoch  66 — Loss: 0.508979\n",
      "  Epoch  67 — Loss: 0.507555\n",
      "  Epoch  68 — Loss: 0.506703\n",
      "  Epoch  69 — Loss: 0.501973\n",
      "  Epoch  70 — Loss: 0.501702\n",
      "  Epoch  71 — Loss: 0.500176\n",
      "  Epoch  72 — Loss: 0.498155\n",
      "  Epoch  73 — Loss: 0.496922\n",
      "  Epoch  74 — Loss: 0.497705\n",
      "  ↳ small improvement (Δ=-7.83e-04) stale 1/3\n",
      "  Epoch  75 — Loss: 0.491442\n",
      "  Epoch  76 — Loss: 0.492011\n",
      "  ↳ small improvement (Δ=-5.70e-04) stale 1/3\n",
      "  Epoch  77 — Loss: 0.487639\n",
      "  Epoch  78 — Loss: 0.486324\n",
      "  Epoch  79 — Loss: 0.486280\n",
      "  ↳ small improvement (Δ=4.45e-05) stale 1/3\n",
      "  Epoch  80 — Loss: 0.486394\n",
      "  ↳ small improvement (Δ=-6.99e-05) stale 2/3\n",
      "  Epoch  81 — Loss: 0.483295\n",
      "  Epoch  82 — Loss: 0.479852\n",
      "  Epoch  83 — Loss: 0.477014\n",
      "  Epoch  84 — Loss: 0.475804\n",
      "  Epoch  85 — Loss: 0.475107\n",
      "  Epoch  86 — Loss: 0.472739\n",
      "  Epoch  87 — Loss: 0.470332\n",
      "  Epoch  88 — Loss: 0.469513\n",
      "  Epoch  89 — Loss: 0.468904\n",
      "  Epoch  90 — Loss: 0.465661\n",
      "  Epoch  91 — Loss: 0.466238\n",
      "  ↳ small improvement (Δ=-5.78e-04) stale 1/3\n",
      "  Epoch  92 — Loss: 0.464862\n",
      "  Epoch  93 — Loss: 0.460722\n",
      "  Epoch  94 — Loss: 0.458971\n",
      "  Epoch  95 — Loss: 0.457288\n",
      "  Epoch  96 — Loss: 0.454384\n",
      "  Epoch  97 — Loss: 0.454564\n",
      "  ↳ small improvement (Δ=-1.80e-04) stale 1/3\n",
      "  Epoch  98 — Loss: 0.457546\n",
      "  ↳ small improvement (Δ=-3.16e-03) stale 2/3\n",
      "  Epoch  99 — Loss: 0.451304\n",
      "  Epoch 100 — Loss: 0.449436\n",
      "Metrics for group3:\n",
      "  Accuracy    :  61.56%\n",
      "  Micro-F1    :  61.56%\n",
      "  Macro-F1    :  60.98%\n",
      "  Weighted-F1 :  61.58%\n",
      "  Macro-Recall:  60.99%\n",
      "\n",
      "\n",
      "=== Tuning on group4 ===\n",
      "[META] row 0/400\n",
      "[META] row 200/400\n",
      "[NVDA] row 0/400\n",
      "[NVDA] row 200/400\n",
      "[TSLA] row 0/400\n",
      "[TSLA] row 200/400\n",
      "[NFLX] row 0/400\n",
      "[NFLX] row 200/400\n",
      "  Epoch   1 — Loss: 0.687507\n",
      "  Epoch   2 — Loss: 0.686055\n",
      "  Epoch   3 — Loss: 0.684959\n",
      "  Epoch   4 — Loss: 0.683430\n",
      "  Epoch   5 — Loss: 0.682109\n",
      "  Epoch   6 — Loss: 0.679608\n",
      "  Epoch   7 — Loss: 0.677134\n",
      "  Epoch   8 — Loss: 0.674898\n",
      "  Epoch   9 — Loss: 0.672784\n",
      "  Epoch  10 — Loss: 0.669426\n",
      "  Epoch  11 — Loss: 0.665994\n",
      "  Epoch  12 — Loss: 0.663296\n",
      "  Epoch  13 — Loss: 0.661282\n",
      "  Epoch  14 — Loss: 0.657167\n",
      "  Epoch  15 — Loss: 0.651629\n",
      "  Epoch  16 — Loss: 0.651098\n",
      "  Epoch  17 — Loss: 0.646495\n",
      "  Epoch  18 — Loss: 0.642265\n",
      "  Epoch  19 — Loss: 0.639762\n",
      "  Epoch  20 — Loss: 0.636746\n",
      "  Epoch  21 — Loss: 0.633424\n",
      "  Epoch  22 — Loss: 0.630645\n",
      "  Epoch  23 — Loss: 0.625916\n",
      "  Epoch  24 — Loss: 0.622885\n",
      "  Epoch  25 — Loss: 0.620092\n",
      "  Epoch  26 — Loss: 0.617844\n",
      "  Epoch  27 — Loss: 0.614611\n",
      "  Epoch  28 — Loss: 0.613008\n",
      "  Epoch  29 — Loss: 0.608421\n",
      "  Epoch  30 — Loss: 0.605318\n",
      "  Epoch  31 — Loss: 0.605964\n",
      "  ↳ small improvement (Δ=-6.47e-04) stale 1/3\n",
      "  Epoch  32 — Loss: 0.600806\n",
      "  Epoch  33 — Loss: 0.598038\n",
      "  Epoch  34 — Loss: 0.596666\n",
      "  Epoch  35 — Loss: 0.593627\n",
      "  Epoch  36 — Loss: 0.591811\n",
      "  Epoch  37 — Loss: 0.590152\n",
      "  Epoch  38 — Loss: 0.586135\n",
      "  Epoch  39 — Loss: 0.583350\n",
      "  Epoch  40 — Loss: 0.581999\n",
      "  Epoch  41 — Loss: 0.580433\n",
      "  Epoch  42 — Loss: 0.577677\n",
      "  Epoch  43 — Loss: 0.575256\n",
      "  Epoch  44 — Loss: 0.573342\n",
      "  Epoch  45 — Loss: 0.570021\n",
      "  Epoch  46 — Loss: 0.567479\n",
      "  Epoch  47 — Loss: 0.569677\n",
      "  ↳ small improvement (Δ=-2.20e-03) stale 1/3\n",
      "  Epoch  48 — Loss: 0.563990\n",
      "  Epoch  49 — Loss: 0.561645\n",
      "  Epoch  50 — Loss: 0.560229\n",
      "  Epoch  51 — Loss: 0.558911\n",
      "  Epoch  52 — Loss: 0.555388\n",
      "  Epoch  53 — Loss: 0.554561\n",
      "  Epoch  54 — Loss: 0.551200\n",
      "  Epoch  55 — Loss: 0.550558\n",
      "  Epoch  56 — Loss: 0.549807\n",
      "  Epoch  57 — Loss: 0.544648\n",
      "  Epoch  58 — Loss: 0.542760\n",
      "  Epoch  59 — Loss: 0.544307\n",
      "  ↳ small improvement (Δ=-1.55e-03) stale 1/3\n",
      "  Epoch  60 — Loss: 0.539013\n",
      "  Epoch  61 — Loss: 0.536277\n",
      "  Epoch  62 — Loss: 0.534412\n",
      "  Epoch  63 — Loss: 0.532534\n",
      "  Epoch  64 — Loss: 0.530117\n",
      "  Epoch  65 — Loss: 0.529629\n",
      "  Epoch  66 — Loss: 0.526252\n",
      "  Epoch  67 — Loss: 0.525155\n",
      "  Epoch  68 — Loss: 0.522386\n",
      "  Epoch  69 — Loss: 0.520654\n",
      "  Epoch  70 — Loss: 0.519984\n",
      "  Epoch  71 — Loss: 0.515981\n",
      "  Epoch  72 — Loss: 0.514515\n",
      "  Epoch  73 — Loss: 0.511880\n",
      "  Epoch  74 — Loss: 0.511374\n",
      "  Epoch  75 — Loss: 0.509159\n",
      "  Epoch  76 — Loss: 0.508013\n",
      "  Epoch  77 — Loss: 0.506072\n",
      "  Epoch  78 — Loss: 0.503791\n",
      "  Epoch  79 — Loss: 0.501336\n",
      "  Epoch  80 — Loss: 0.498582\n",
      "  Epoch  81 — Loss: 0.496549\n",
      "  Epoch  82 — Loss: 0.494372\n",
      "  Epoch  83 — Loss: 0.494675\n",
      "  ↳ small improvement (Δ=-3.03e-04) stale 1/3\n",
      "  Epoch  84 — Loss: 0.491344\n",
      "  Epoch  85 — Loss: 0.488672\n",
      "  Epoch  86 — Loss: 0.487926\n",
      "  Epoch  87 — Loss: 0.488023\n",
      "  ↳ small improvement (Δ=-9.73e-05) stale 1/3\n",
      "  Epoch  88 — Loss: 0.483309\n",
      "  Epoch  89 — Loss: 0.480971\n",
      "  Epoch  90 — Loss: 0.478692\n",
      "  Epoch  91 — Loss: 0.481084\n",
      "  ↳ small improvement (Δ=-2.39e-03) stale 1/3\n",
      "  Epoch  92 — Loss: 0.476648\n",
      "  Epoch  93 — Loss: 0.473082\n",
      "  Epoch  94 — Loss: 0.472915\n",
      "  Epoch  95 — Loss: 0.470047\n",
      "  Epoch  96 — Loss: 0.470348\n",
      "  ↳ small improvement (Δ=-3.01e-04) stale 1/3\n",
      "  Epoch  97 — Loss: 0.468437\n",
      "  Epoch  98 — Loss: 0.465827\n",
      "  Epoch  99 — Loss: 0.463582\n",
      "  Epoch 100 — Loss: 0.460734\n",
      "Metrics for group4:\n",
      "  Accuracy    :  54.69%\n",
      "  Micro-F1    :  54.69%\n",
      "  Macro-F1    :  52.20%\n",
      "  Weighted-F1 :  53.43%\n",
      "  Macro-Recall:  52.79%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gc, torch, numpy as np, pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "results = {}\n",
    "\n",
    "def make_snippet_df(group_dict: dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "    import re, html\n",
    "    def _first_para_from_html(s: str, max_chars=420):\n",
    "        s = str(s)\n",
    "        s = re.sub(r\"<[^>]+>\", \" \", s)\n",
    "        s = html.unescape(\" \".join(s.split()))\n",
    "        return s[:max_chars]\n",
    "\n",
    "    rows = []\n",
    "    for tkr, df in group_dict.items():\n",
    "        need = [\"symbol\", \"publishOn\", \"title\"]\n",
    "        if \"attributes.content\" in df.columns:\n",
    "            need.append(\"attributes.content\")\n",
    "        sdf = df[need].copy()\n",
    "        sdf[\"publishOn\"] = pd.to_datetime(sdf[\"publishOn\"], utc=True, errors=\"coerce\")\n",
    "        sdf = sdf.dropna(subset=[\"publishOn\"])\n",
    "        for sym, ts, ttl in sdf[[\"symbol\",\"publishOn\",\"title\"]].itertuples(index=False, name=None):\n",
    "            rows.append({\"ticker\": sym, \"time\": ts, \"text\": str(ttl), \"kind\": \"headline\"})\n",
    "        if \"attributes.content\" in sdf.columns:\n",
    "            mask = sdf[\"attributes.content\"].notna()\n",
    "            for sym, ts, body in sdf.loc[mask, [\"symbol\",\"publishOn\",\"attributes.content\"]].itertuples(index=False, name=None):\n",
    "                rows.append({\"ticker\": sym, \"time\": ts,\n",
    "                             \"text\": _first_para_from_html(body), \"kind\": \"first_para\"})\n",
    "    snips = pd.DataFrame(rows).drop_duplicates(subset=[\"ticker\",\"time\",\"text\"]).reset_index(drop=True)\n",
    "    return snips\n",
    "\n",
    "\n",
    "for group_name, labeled_group in [\n",
    "    (\"group2\", labeled_group2),\n",
    "    (\"group3\", labeled_group3),\n",
    "    (\"group4\", labeled_group4),\n",
    "]:\n",
    "    print(f\"\\n=== Tuning on {group_name} ===\")\n",
    "\n",
    "    snips_df = make_snippet_df(labeled_group)\n",
    "    idx = TickerIndex(snips_df, batch_size=16)\n",
    "\n",
    "    X, y, meta = rag_features_from_group(labeled_group, idx, top_r=1)\n",
    "\n",
    "\n",
    "    del idx; gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache(); torch.cuda.ipc_collect()\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "\n",
    "    emb_train = torch.from_numpy(X_train).to(device).float()\n",
    "    emb_test  = torch.from_numpy(X_test ).to(device).float()\n",
    "    y_train_t = torch.from_numpy(y_train).to(device).long()\n",
    "\n",
    "\n",
    "    d = emb_train.size(1)\n",
    "    head = nn.Sequential(\n",
    "        nn.Linear(d, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 2)\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.Adam(head.parameters(), lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    train_ds = TensorDataset(emb_train, y_train_t)\n",
    "    train_dl = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "\n",
    "\n",
    "    best_prev_loss = float(\"inf\")\n",
    "    tol = 1e-4\n",
    "    patience = 3\n",
    "    stale_epochs = 0\n",
    "    max_epochs = 100\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        head.train()\n",
    "        epoch_loss = 0.0\n",
    "        for emb_b, lab_b in train_dl:\n",
    "            optimizer.zero_grad()\n",
    "            logits = head(emb_b)\n",
    "            loss = criterion(logits, lab_b)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * emb_b.size(0)\n",
    "\n",
    "        avg_loss = epoch_loss / len(train_ds)\n",
    "        print(f\"  Epoch {epoch:>3} — Loss: {avg_loss:.6f}\")\n",
    "        improvement = best_prev_loss - avg_loss\n",
    "        if improvement < tol:\n",
    "            stale_epochs += 1\n",
    "            print(f\"  ↳ small improvement (Δ={improvement:.2e}) stale {stale_epochs}/{patience}\")\n",
    "        else:\n",
    "            stale_epochs = 0\n",
    "            best_prev_loss = avg_loss\n",
    "        if stale_epochs >= patience:\n",
    "            print(f\"  ↳ Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "\n",
    "    head.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = head(emb_test).argmax(dim=1).detach().cpu().numpy()\n",
    "\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    micro_f1 = f1_score(y_test, preds, average=\"micro\")\n",
    "    macro_f1 = f1_score(y_test, preds, average=\"macro\")\n",
    "    weighted_f1 = f1_score(y_test, preds, average=\"weighted\")\n",
    "    macro_recall = recall_score(y_test, preds, average=\"macro\")\n",
    "\n",
    "    print(f\"Metrics for {group_name}:\")\n",
    "    print(f\"  Accuracy    : {acc*100:6.2f}%\")\n",
    "    print(f\"  Micro-F1    : {micro_f1*100:6.2f}%\")\n",
    "    print(f\"  Macro-F1    : {macro_f1*100:6.2f}%\")\n",
    "    print(f\"  Weighted-F1 : {weighted_f1*100:6.2f}%\")\n",
    "    print(f\"  Macro-Recall: {macro_recall*100:6.2f}%\\n\")\n",
    "\n",
    "    results[group_name] = head\n",
    "\n",
    "\n",
    "    del emb_train, emb_test, y_train_t, head\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache(); torch.cuda.ipc_collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7oy_kVo2VQOS"
   },
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch, numpy as np\n",
    "\n",
    "def build_embedder(model_id, max_len=256, dtype=torch.float16):\n",
    "    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "    mdl = AutoModel.from_pretrained(\n",
    "        model_id,\n",
    "        output_hidden_states=False,\n",
    "        torch_dtype=dtype,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def embed_batched(texts, batch_size=16, pool=\"mean\"):\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        vecs = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            enc = tok(\n",
    "                texts[i:i+batch_size],\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_len,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(mdl.device)\n",
    "            out = mdl(**enc).last_hidden_state\n",
    "            if pool == \"mean\":\n",
    "                E = out.mean(dim=1)\n",
    "            else:\n",
    "                E = out[:, 0]   # CLS token\n",
    "            E = torch.nn.functional.normalize(E, p=2, dim=1)\n",
    "            vecs.append(E.cpu().numpy().astype(\"float32\"))\n",
    "        return np.vstack(vecs)\n",
    "\n",
    "    return tok, mdl, embed_batched\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "a91ce5a5cc1a40e488b75810a409bd43",
      "8ad1b2032dfa4c08a729af1dc9dd27cf",
      "7d8cd19a44a84ea6b7ddc4c29662f1e0",
      "1f04aed3000441be91ea308331b43ec6",
      "903281e2110d4b78a1a2cbaa8a22835c",
      "418b1a037ae8435cac38583e3585b840",
      "3b0fa1ab3e8b442e91194490e248c335",
      "c66f744187ca4e339714152cbd01d242",
      "2969b2d585364f70bb49592eff14047c",
      "cb21cd98a2b148259114408725f238ec",
      "7b5dc2edb0874fd2814f91661217c642",
      "08ff3018313e4f029c53fb1bd8c1280c",
      "dcd08be6f6b54376816a46ab5bac5aa5",
      "85072531b5384546a1d82f01936f2eb5",
      "c0842f6386d744a88cdb2269868b282a",
      "783f0fa398f04565a35d2240a21d7237",
      "0fbac56bae744948baa8abd538f9ca29",
      "c2aede58228741179d028a6fc84b4e17",
      "9fb43136f9854faaaee24a8a35ab6d09",
      "6e3d9eafc86c4a7cb1bbf28bebbc8ec3",
      "74f35cf2a12043e2af31d5309808367f",
      "d12243349fd648b08422c7e8bb5b6eba",
      "2c4bdc3aa9b744769e55d22678850201",
      "5b07336f548a4d34a2d99ec50d906012",
      "658f96f37fc8435c9e720cc7fdc08b39",
      "3545d5c5cfe647d3a1e5e975cf93c3a5",
      "fba0171b63934380ada661203cda8143",
      "4c59720f803c4bb8b8bc23917b6e269f",
      "efdf6c3efabc42e0acf3afa7ede9fbca",
      "d03ec12293864dcea1364c59cf6c7c39",
      "8f42ebeeaf7a4f96a4a38268273193ee",
      "95f345af109542088c1c6b478da9cbf0",
      "ef006719a39c473bb6965775a587971e",
      "315f3b5bf8f64cf082de8944f382a907",
      "6e070155a4c34bfeacb35f046784c555",
      "83569224f39c426aa5a3b817087c4af3",
      "7fb76880dfd64446be3b51a35d1c31db",
      "7b1196c57ed94ac0a854c652c04527a2",
      "0fc6a5ff285b4e23b9ee03519cc601d8",
      "ed9676ee8c4c4c3f85dbc59d380b972d",
      "5f1a4b0f1c6b4544bf2930bc6ed0d30d",
      "db5b1cfa579e4b098fb0dddb2225aa42",
      "8b05c868a42440c9a5ce67bc9275ab45",
      "ba11afcf58734745b3db07bfbf98bba3",
      "9d89329b92ca430daa6a890774d7f63a",
      "677a939cfd3d456f995bd4e9ee45e2ed",
      "5022e481736346beaeeb96ab815f9803",
      "2811305ef84b4c9798a12d3499c7fc72",
      "b072c16342c54e4281d9b6d16905003c",
      "1141081a9ce8480a9fc47c541826fe2b",
      "b6f91a424021458eadcbdaff314eca49",
      "4fa199e4e00d48848b6ab3bc8930384d",
      "0935528f830844bd82a36f6cfc9d4158",
      "b9f502e08d28420d90b99621ff353a8d",
      "1115e21a928e48e1995489b4050b75f7",
      "8711ae230aa144f1a05d499b18f9822b",
      "ac3a41c813ba4b2a9974d9436ce23204",
      "b403fd28263a42939a2bf480235f8ae1",
      "5f326696f456468fa35ada85bf1d3863",
      "e8ffa7bd6c284c128ece4848597225e4",
      "8202fd7422d64ac2b83ef853d31526e7",
      "7eb60f8c89ea4f7b9a4edc1fd4aa6fd5",
      "6fb2bfd7f1ed49549176beea11a4a6d2",
      "93bac6c9f191465b862d1a0703529efc",
      "4f7b6078c98a41fd8220348381535325",
      "52c33be69d9a4b779eb85a3161b417ab",
      "a73c27e697ea431bb7b4ed45ad325b0d",
      "1a4bf1d4c0444c7685cdecf5a16dd505",
      "1bd687de014843daa1bcc601b21ca0e9",
      "7fcaad8157f94ec4a97fb0ad870adf4c",
      "008c25ae99b54fd5a63495d58e94af0d",
      "6b06aac6e1d147129d6f703ac47ea5a7",
      "79709857de2e42888fc96154c3c5bb69",
      "9424a9ed64a54baeb585860addfb73a3",
      "fb35cf90625a4dd28f7ab75f550fcd69",
      "7e942c7334444261b95c1c3762387de5",
      "8cadfc7c97714fa4a9fe7dc0a7b6a3b2",
      "69c60eb82eef42acb65382e3f674be31",
      "54da373529554e439d03fc907dfaf230",
      "1b8dd71ca08949c8926392437e8b6a30",
      "43db3d05746a4121931be3448dcae702",
      "235f1591171c422bae4f5018df724014",
      "218b74ea3ca540f9898337988911ca25",
      "a086e8d1a581421885852dfa0b9a7272",
      "4cdc0a30d63c44c4b15cb8559d896cf7",
      "a3c5068d5d4a4ddca36aa9f59421bc73",
      "a6a07e001e0a4e0e800c6a1d381f0c12",
      "a9ead3864f4a4cc2b56dacf9af51471f",
      "497d1fc679e54a4dbe82ca83ff2723ff",
      "a1ff40d3eeb24777b5316c51255414ca",
      "90df072e8d874a67b0d6c4475ccac08e",
      "298d1db386c04648a32c293f95375333",
      "d6296cd625ab4a9694720198380aa8c4",
      "732b106083754e0dac303c46dd99d8ff",
      "932207ae97664258b6fe9d18b7434910",
      "d3142fc538494065b7678e85130fb815",
      "a6ea35367dbb417984f113066dbf36f0",
      "ce627e52abdc448fa61b2fcfec479efb",
      "206b35a2aca5471087b39bf4b7571d43"
     ]
    },
    "executionInfo": {
     "elapsed": 384834,
     "status": "ok",
     "timestamp": 1756096147757,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -480
    },
    "id": "VcsXiiXOa38x",
    "outputId": "fc628186-e5c8-4f59-c3d9-07327f67eaf7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a91ce5a5cc1a40e488b75810a409bd43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/792 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08ff3018313e4f029c53fb1bd8c1280c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c4bdc3aa9b744769e55d22678850201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/584 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "315f3b5bf8f64cf082de8944f382a907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d89329b92ca430daa6a890774d7f63a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8711ae230aa144f1a05d499b18f9822b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.85G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a73c27e697ea431bb7b4ed45ad325b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c60eb82eef42acb65382e3f674be31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "497d1fc679e54a4dbe82ca83ff2723ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Tuning on group2 ===\n",
      "[C] row 0/400\n",
      "[C] row 200/400\n",
      "[GS] row 0/400\n",
      "[GS] row 200/400\n",
      "[MS] row 0/400\n",
      "[MS] row 200/400\n",
      "[USB] row 0/400\n",
      "[USB] row 200/400\n",
      "  Epoch   1 — Loss: 0.690739\n",
      "  Epoch   2 — Loss: 0.689386\n",
      "  Epoch   3 — Loss: 0.688964\n",
      "  Epoch   4 — Loss: 0.688339\n",
      "  Epoch   5 — Loss: 0.687413\n",
      "  Epoch   6 — Loss: 0.686865\n",
      "  Epoch   7 — Loss: 0.686223\n",
      "  Epoch   8 — Loss: 0.685566\n",
      "  Epoch   9 — Loss: 0.684749\n",
      "  Epoch  10 — Loss: 0.683760\n",
      "  Epoch  11 — Loss: 0.682778\n",
      "  Epoch  12 — Loss: 0.681953\n",
      "  Epoch  13 — Loss: 0.680812\n",
      "  Epoch  14 — Loss: 0.680055\n",
      "  Epoch  15 — Loss: 0.679196\n",
      "  Epoch  16 — Loss: 0.678070\n",
      "  Epoch  17 — Loss: 0.677233\n",
      "  Epoch  18 — Loss: 0.675716\n",
      "  Epoch  19 — Loss: 0.674329\n",
      "  Epoch  20 — Loss: 0.673710\n",
      "  Epoch  21 — Loss: 0.672379\n",
      "  Epoch  22 — Loss: 0.671795\n",
      "  Epoch  23 — Loss: 0.670310\n",
      "  Epoch  24 — Loss: 0.669118\n",
      "  Epoch  25 — Loss: 0.668651\n",
      "  Epoch  26 — Loss: 0.666446\n",
      "  Epoch  27 — Loss: 0.665496\n",
      "  Epoch  28 — Loss: 0.663270\n",
      "  Epoch  29 — Loss: 0.663179\n",
      "  ↳ small improvement (Δ=9.14e-05) stale 1/3\n",
      "  Epoch  30 — Loss: 0.661060\n",
      "  Epoch  31 — Loss: 0.659797\n",
      "  Epoch  32 — Loss: 0.658060\n",
      "  Epoch  33 — Loss: 0.656374\n",
      "  Epoch  34 — Loss: 0.656505\n",
      "  ↳ small improvement (Δ=-1.31e-04) stale 1/3\n",
      "  Epoch  35 — Loss: 0.654145\n",
      "  Epoch  36 — Loss: 0.654025\n",
      "  Epoch  37 — Loss: 0.652189\n",
      "  Epoch  38 — Loss: 0.652578\n",
      "  ↳ small improvement (Δ=-3.89e-04) stale 1/3\n",
      "  Epoch  39 — Loss: 0.648763\n",
      "  Epoch  40 — Loss: 0.648228\n",
      "  Epoch  41 — Loss: 0.646723\n",
      "  Epoch  42 — Loss: 0.645953\n",
      "  Epoch  43 — Loss: 0.643794\n",
      "  Epoch  44 — Loss: 0.642495\n",
      "  Epoch  45 — Loss: 0.640798\n",
      "  Epoch  46 — Loss: 0.640637\n",
      "  Epoch  47 — Loss: 0.638393\n",
      "  Epoch  48 — Loss: 0.637304\n",
      "  Epoch  49 — Loss: 0.635750\n",
      "  Epoch  50 — Loss: 0.634074\n",
      "  Epoch  51 — Loss: 0.632166\n",
      "  Epoch  52 — Loss: 0.631675\n",
      "  Epoch  53 — Loss: 0.630380\n",
      "  Epoch  54 — Loss: 0.629676\n",
      "  Epoch  55 — Loss: 0.627455\n",
      "  Epoch  56 — Loss: 0.628920\n",
      "  ↳ small improvement (Δ=-1.47e-03) stale 1/3\n",
      "  Epoch  57 — Loss: 0.624435\n",
      "  Epoch  58 — Loss: 0.623662\n",
      "  Epoch  59 — Loss: 0.622884\n",
      "  Epoch  60 — Loss: 0.620726\n",
      "  Epoch  61 — Loss: 0.619290\n",
      "  Epoch  62 — Loss: 0.614636\n",
      "  Epoch  63 — Loss: 0.616995\n",
      "  ↳ small improvement (Δ=-2.36e-03) stale 1/3\n",
      "  Epoch  64 — Loss: 0.615027\n",
      "  ↳ small improvement (Δ=-3.90e-04) stale 2/3\n",
      "  Epoch  65 — Loss: 0.613312\n",
      "  Epoch  66 — Loss: 0.613252\n",
      "  ↳ small improvement (Δ=6.05e-05) stale 1/3\n",
      "  Epoch  67 — Loss: 0.614051\n",
      "  ↳ small improvement (Δ=-7.39e-04) stale 2/3\n",
      "  Epoch  68 — Loss: 0.608954\n",
      "  Epoch  69 — Loss: 0.606446\n",
      "  Epoch  70 — Loss: 0.607511\n",
      "  ↳ small improvement (Δ=-1.07e-03) stale 1/3\n",
      "  Epoch  71 — Loss: 0.605998\n",
      "  Epoch  72 — Loss: 0.604264\n",
      "  Epoch  73 — Loss: 0.604012\n",
      "  Epoch  74 — Loss: 0.602421\n",
      "  Epoch  75 — Loss: 0.600950\n",
      "  Epoch  76 — Loss: 0.599740\n",
      "  Epoch  77 — Loss: 0.601085\n",
      "  ↳ small improvement (Δ=-1.35e-03) stale 1/3\n",
      "  Epoch  78 — Loss: 0.595905\n",
      "  Epoch  79 — Loss: 0.595970\n",
      "  ↳ small improvement (Δ=-6.51e-05) stale 1/3\n",
      "  Epoch  80 — Loss: 0.592681\n",
      "  Epoch  81 — Loss: 0.596312\n",
      "  ↳ small improvement (Δ=-3.63e-03) stale 1/3\n",
      "  Epoch  82 — Loss: 0.588431\n",
      "  Epoch  83 — Loss: 0.592320\n",
      "  ↳ small improvement (Δ=-3.89e-03) stale 1/3\n",
      "  Epoch  84 — Loss: 0.587302\n",
      "  Epoch  85 — Loss: 0.591001\n",
      "  ↳ small improvement (Δ=-3.70e-03) stale 1/3\n",
      "  Epoch  86 — Loss: 0.586929\n",
      "  Epoch  87 — Loss: 0.584348\n",
      "  Epoch  88 — Loss: 0.582766\n",
      "  Epoch  89 — Loss: 0.582817\n",
      "  ↳ small improvement (Δ=-5.07e-05) stale 1/3\n",
      "  Epoch  90 — Loss: 0.580043\n",
      "  Epoch  91 — Loss: 0.580197\n",
      "  ↳ small improvement (Δ=-1.54e-04) stale 1/3\n",
      "  Epoch  92 — Loss: 0.580184\n",
      "  ↳ small improvement (Δ=-1.41e-04) stale 2/3\n",
      "  Epoch  93 — Loss: 0.576756\n",
      "  Epoch  94 — Loss: 0.575110\n",
      "  Epoch  95 — Loss: 0.572434\n",
      "  Epoch  96 — Loss: 0.576053\n",
      "  ↳ small improvement (Δ=-3.62e-03) stale 1/3\n",
      "  Epoch  97 — Loss: 0.570471\n",
      "  Epoch  98 — Loss: 0.569398\n",
      "  Epoch  99 — Loss: 0.568017\n",
      "  Epoch 100 — Loss: 0.567514\n",
      "Metrics for group2:\n",
      "  Accuracy    :  55.62%\n",
      "  Micro-F1    :  55.62%\n",
      "  Macro-F1    :  55.33%\n",
      "  Weighted-F1 :  55.62%\n",
      "  Macro-Recall:  55.33%\n",
      "\n",
      "\n",
      "=== Tuning on group3 ===\n",
      "[AAPL] row 0/400\n",
      "[AAPL] row 200/400\n",
      "[MSFT] row 0/400\n",
      "[MSFT] row 200/400\n",
      "[AMZN] row 0/400\n",
      "[AMZN] row 200/400\n",
      "[GOOGL] row 0/400\n",
      "[GOOGL] row 200/400\n",
      "  Epoch   1 — Loss: 0.688851\n",
      "  Epoch   2 — Loss: 0.685807\n",
      "  Epoch   3 — Loss: 0.684516\n",
      "  Epoch   4 — Loss: 0.684118\n",
      "  Epoch   5 — Loss: 0.683297\n",
      "  Epoch   6 — Loss: 0.682694\n",
      "  Epoch   7 — Loss: 0.682100\n",
      "  Epoch   8 — Loss: 0.681798\n",
      "  Epoch   9 — Loss: 0.680959\n",
      "  Epoch  10 — Loss: 0.680635\n",
      "  Epoch  11 — Loss: 0.680006\n",
      "  Epoch  12 — Loss: 0.679221\n",
      "  Epoch  13 — Loss: 0.678391\n",
      "  Epoch  14 — Loss: 0.677624\n",
      "  Epoch  15 — Loss: 0.677151\n",
      "  Epoch  16 — Loss: 0.675909\n",
      "  Epoch  17 — Loss: 0.674855\n",
      "  Epoch  18 — Loss: 0.674157\n",
      "  Epoch  19 — Loss: 0.673004\n",
      "  Epoch  20 — Loss: 0.672205\n",
      "  Epoch  21 — Loss: 0.671039\n",
      "  Epoch  22 — Loss: 0.670362\n",
      "  Epoch  23 — Loss: 0.669076\n",
      "  Epoch  24 — Loss: 0.667585\n",
      "  Epoch  25 — Loss: 0.666624\n",
      "  Epoch  26 — Loss: 0.665095\n",
      "  Epoch  27 — Loss: 0.664574\n",
      "  Epoch  28 — Loss: 0.663966\n",
      "  Epoch  29 — Loss: 0.661612\n",
      "  Epoch  30 — Loss: 0.660534\n",
      "  Epoch  31 — Loss: 0.658909\n",
      "  Epoch  32 — Loss: 0.657538\n",
      "  Epoch  33 — Loss: 0.656737\n",
      "  Epoch  34 — Loss: 0.655047\n",
      "  Epoch  35 — Loss: 0.653913\n",
      "  Epoch  36 — Loss: 0.652965\n",
      "  Epoch  37 — Loss: 0.650214\n",
      "  Epoch  38 — Loss: 0.650716\n",
      "  ↳ small improvement (Δ=-5.02e-04) stale 1/3\n",
      "  Epoch  39 — Loss: 0.649383\n",
      "  Epoch  40 — Loss: 0.646972\n",
      "  Epoch  41 — Loss: 0.645830\n",
      "  Epoch  42 — Loss: 0.643840\n",
      "  Epoch  43 — Loss: 0.641919\n",
      "  Epoch  44 — Loss: 0.640235\n",
      "  Epoch  45 — Loss: 0.640913\n",
      "  ↳ small improvement (Δ=-6.78e-04) stale 1/3\n",
      "  Epoch  46 — Loss: 0.637814\n",
      "  Epoch  47 — Loss: 0.635862\n",
      "  Epoch  48 — Loss: 0.634139\n",
      "  Epoch  49 — Loss: 0.633267\n",
      "  Epoch  50 — Loss: 0.631577\n",
      "  Epoch  51 — Loss: 0.629758\n",
      "  Epoch  52 — Loss: 0.628232\n",
      "  Epoch  53 — Loss: 0.626654\n",
      "  Epoch  54 — Loss: 0.625447\n",
      "  Epoch  55 — Loss: 0.623327\n",
      "  Epoch  56 — Loss: 0.621895\n",
      "  Epoch  57 — Loss: 0.619741\n",
      "  Epoch  58 — Loss: 0.618234\n",
      "  Epoch  59 — Loss: 0.617956\n",
      "  Epoch  60 — Loss: 0.616296\n",
      "  Epoch  61 — Loss: 0.613677\n",
      "  Epoch  62 — Loss: 0.612152\n",
      "  Epoch  63 — Loss: 0.610623\n",
      "  Epoch  64 — Loss: 0.608965\n",
      "  Epoch  65 — Loss: 0.607462\n",
      "  Epoch  66 — Loss: 0.605417\n",
      "  Epoch  67 — Loss: 0.603024\n",
      "  Epoch  68 — Loss: 0.602939\n",
      "  ↳ small improvement (Δ=8.47e-05) stale 1/3\n",
      "  Epoch  69 — Loss: 0.600459\n",
      "  Epoch  70 — Loss: 0.598106\n",
      "  Epoch  71 — Loss: 0.596218\n",
      "  Epoch  72 — Loss: 0.594992\n",
      "  Epoch  73 — Loss: 0.593381\n",
      "  Epoch  74 — Loss: 0.591224\n",
      "  Epoch  75 — Loss: 0.589220\n",
      "  Epoch  76 — Loss: 0.587411\n",
      "  Epoch  77 — Loss: 0.585537\n",
      "  Epoch  78 — Loss: 0.584925\n",
      "  Epoch  79 — Loss: 0.582897\n",
      "  Epoch  80 — Loss: 0.580344\n",
      "  Epoch  81 — Loss: 0.575117\n",
      "  Epoch  82 — Loss: 0.580289\n",
      "  ↳ small improvement (Δ=-5.17e-03) stale 1/3\n",
      "  Epoch  83 — Loss: 0.574819\n",
      "  Epoch  84 — Loss: 0.573886\n",
      "  Epoch  85 — Loss: 0.570567\n",
      "  Epoch  86 — Loss: 0.570884\n",
      "  ↳ small improvement (Δ=-3.16e-04) stale 1/3\n",
      "  Epoch  87 — Loss: 0.567393\n",
      "  Epoch  88 — Loss: 0.567550\n",
      "  ↳ small improvement (Δ=-1.57e-04) stale 1/3\n",
      "  Epoch  89 — Loss: 0.564391\n",
      "  Epoch  90 — Loss: 0.563208\n",
      "  Epoch  91 — Loss: 0.561650\n",
      "  Epoch  92 — Loss: 0.558647\n",
      "  Epoch  93 — Loss: 0.555159\n",
      "  Epoch  94 — Loss: 0.555370\n",
      "  ↳ small improvement (Δ=-2.11e-04) stale 1/3\n",
      "  Epoch  95 — Loss: 0.554019\n",
      "  Epoch  96 — Loss: 0.555660\n",
      "  ↳ small improvement (Δ=-1.64e-03) stale 1/3\n",
      "  Epoch  97 — Loss: 0.549430\n",
      "  Epoch  98 — Loss: 0.549588\n",
      "  ↳ small improvement (Δ=-1.58e-04) stale 1/3\n",
      "  Epoch  99 — Loss: 0.545699\n",
      "  Epoch 100 — Loss: 0.542908\n",
      "Metrics for group3:\n",
      "  Accuracy    :  62.81%\n",
      "  Micro-F1    :  62.81%\n",
      "  Macro-F1    :  58.88%\n",
      "  Weighted-F1 :  60.47%\n",
      "  Macro-Recall:  59.88%\n",
      "\n",
      "\n",
      "=== Tuning on group4 ===\n",
      "[META] row 0/400\n",
      "[META] row 200/400\n",
      "[NVDA] row 0/400\n",
      "[NVDA] row 200/400\n",
      "[TSLA] row 0/400\n",
      "[TSLA] row 200/400\n",
      "[NFLX] row 0/400\n",
      "[NFLX] row 200/400\n",
      "  Epoch   1 — Loss: 0.689395\n",
      "  Epoch   2 — Loss: 0.687335\n",
      "  Epoch   3 — Loss: 0.686664\n",
      "  Epoch   4 — Loss: 0.686378\n",
      "  Epoch   5 — Loss: 0.686020\n",
      "  Epoch   6 — Loss: 0.685581\n",
      "  Epoch   7 — Loss: 0.685692\n",
      "  ↳ small improvement (Δ=-1.11e-04) stale 1/3\n",
      "  Epoch   8 — Loss: 0.685182\n",
      "  Epoch   9 — Loss: 0.684548\n",
      "  Epoch  10 — Loss: 0.684314\n",
      "  Epoch  11 — Loss: 0.684168\n",
      "  Epoch  12 — Loss: 0.683586\n",
      "  Epoch  13 — Loss: 0.683220\n",
      "  Epoch  14 — Loss: 0.682742\n",
      "  Epoch  15 — Loss: 0.682430\n",
      "  Epoch  16 — Loss: 0.681812\n",
      "  Epoch  17 — Loss: 0.681590\n",
      "  Epoch  18 — Loss: 0.681456\n",
      "  Epoch  19 — Loss: 0.680365\n",
      "  Epoch  20 — Loss: 0.679941\n",
      "  Epoch  21 — Loss: 0.679289\n",
      "  Epoch  22 — Loss: 0.678870\n",
      "  Epoch  23 — Loss: 0.678091\n",
      "  Epoch  24 — Loss: 0.677497\n",
      "  Epoch  25 — Loss: 0.677179\n",
      "  Epoch  26 — Loss: 0.676457\n",
      "  Epoch  27 — Loss: 0.676007\n",
      "  Epoch  28 — Loss: 0.675742\n",
      "  Epoch  29 — Loss: 0.674938\n",
      "  Epoch  30 — Loss: 0.673439\n",
      "  Epoch  31 — Loss: 0.673617\n",
      "  ↳ small improvement (Δ=-1.78e-04) stale 1/3\n",
      "  Epoch  32 — Loss: 0.672574\n",
      "  Epoch  33 — Loss: 0.672195\n",
      "  Epoch  34 — Loss: 0.671082\n",
      "  Epoch  35 — Loss: 0.670240\n",
      "  Epoch  36 — Loss: 0.669943\n",
      "  Epoch  37 — Loss: 0.668830\n",
      "  Epoch  38 — Loss: 0.668102\n",
      "  Epoch  39 — Loss: 0.667771\n",
      "  Epoch  40 — Loss: 0.666600\n",
      "  Epoch  41 — Loss: 0.665952\n",
      "  Epoch  42 — Loss: 0.664789\n",
      "  Epoch  43 — Loss: 0.664053\n",
      "  Epoch  44 — Loss: 0.662825\n",
      "  Epoch  45 — Loss: 0.662086\n",
      "  Epoch  46 — Loss: 0.662033\n",
      "  ↳ small improvement (Δ=5.32e-05) stale 1/3\n",
      "  Epoch  47 — Loss: 0.660589\n",
      "  Epoch  48 — Loss: 0.659784\n",
      "  Epoch  49 — Loss: 0.658613\n",
      "  Epoch  50 — Loss: 0.658386\n",
      "  Epoch  51 — Loss: 0.657018\n",
      "  Epoch  52 — Loss: 0.655820\n",
      "  Epoch  53 — Loss: 0.655417\n",
      "  Epoch  54 — Loss: 0.654358\n",
      "  Epoch  55 — Loss: 0.652379\n",
      "  Epoch  56 — Loss: 0.652154\n",
      "  Epoch  57 — Loss: 0.650918\n",
      "  Epoch  58 — Loss: 0.649476\n",
      "  Epoch  59 — Loss: 0.649219\n",
      "  Epoch  60 — Loss: 0.647718\n",
      "  Epoch  61 — Loss: 0.646424\n",
      "  Epoch  62 — Loss: 0.645502\n",
      "  Epoch  63 — Loss: 0.645103\n",
      "  Epoch  64 — Loss: 0.643112\n",
      "  Epoch  65 — Loss: 0.643663\n",
      "  ↳ small improvement (Δ=-5.51e-04) stale 1/3\n",
      "  Epoch  66 — Loss: 0.641323\n",
      "  Epoch  67 — Loss: 0.639841\n",
      "  Epoch  68 — Loss: 0.638575\n",
      "  Epoch  69 — Loss: 0.637165\n",
      "  Epoch  70 — Loss: 0.636515\n",
      "  Epoch  71 — Loss: 0.635368\n",
      "  Epoch  72 — Loss: 0.633861\n",
      "  Epoch  73 — Loss: 0.633793\n",
      "  ↳ small improvement (Δ=6.74e-05) stale 1/3\n",
      "  Epoch  74 — Loss: 0.631489\n",
      "  Epoch  75 — Loss: 0.630025\n",
      "  Epoch  76 — Loss: 0.629645\n",
      "  Epoch  77 — Loss: 0.627938\n",
      "  Epoch  78 — Loss: 0.626380\n",
      "  Epoch  79 — Loss: 0.625809\n",
      "  Epoch  80 — Loss: 0.624267\n",
      "  Epoch  81 — Loss: 0.624935\n",
      "  ↳ small improvement (Δ=-6.68e-04) stale 1/3\n",
      "  Epoch  82 — Loss: 0.621134\n",
      "  Epoch  83 — Loss: 0.620647\n",
      "  Epoch  84 — Loss: 0.620116\n",
      "  Epoch  85 — Loss: 0.618435\n",
      "  Epoch  86 — Loss: 0.616000\n",
      "  Epoch  87 — Loss: 0.615928\n",
      "  ↳ small improvement (Δ=7.22e-05) stale 1/3\n",
      "  Epoch  88 — Loss: 0.614415\n",
      "  Epoch  89 — Loss: 0.612439\n",
      "  Epoch  90 — Loss: 0.611463\n",
      "  Epoch  91 — Loss: 0.610295\n",
      "  Epoch  92 — Loss: 0.608543\n",
      "  Epoch  93 — Loss: 0.607448\n",
      "  Epoch  94 — Loss: 0.606307\n",
      "  Epoch  95 — Loss: 0.604744\n",
      "  Epoch  96 — Loss: 0.603724\n",
      "  Epoch  97 — Loss: 0.602047\n",
      "  Epoch  98 — Loss: 0.600549\n",
      "  Epoch  99 — Loss: 0.598685\n",
      "  Epoch 100 — Loss: 0.600901\n",
      "  ↳ small improvement (Δ=-2.22e-03) stale 1/3\n",
      "Metrics for group4:\n",
      "  Accuracy    :  60.94%\n",
      "  Micro-F1    :  60.94%\n",
      "  Macro-F1    :  55.81%\n",
      "  Weighted-F1 :  57.50%\n",
      "  Macro-Recall:  57.84%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MODEL_ID = \"deepseek-ai/deepseek-llm-7b-base\"\n",
    "tokenizer, model, deepseek_embed_batched = build_embedder(MODEL_ID, max_len=256)\n",
    "\n",
    "qwen_embed_batched = deepseek_embed_batched\n",
    "\n",
    "for group_name, labeled_group in [\n",
    "    (\"group2\", labeled_group2),\n",
    "    (\"group3\", labeled_group3),\n",
    "    (\"group4\", labeled_group4),\n",
    "]:\n",
    "    print(f\"\\n=== Tuning on {group_name} ===\")\n",
    "\n",
    "    snips_df = make_snippet_df(labeled_group)\n",
    "    idx = TickerIndex(snips_df, batch_size=16)\n",
    "\n",
    "    X, y, meta = rag_features_from_group(labeled_group, idx, top_r=1)\n",
    "\n",
    "\n",
    "    del idx; gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache(); torch.cuda.ipc_collect()\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "\n",
    "    emb_train = torch.from_numpy(X_train).to(device).float()\n",
    "    emb_test  = torch.from_numpy(X_test ).to(device).float()\n",
    "    y_train_t = torch.from_numpy(y_train).to(device).long()\n",
    "\n",
    "\n",
    "    d = emb_train.size(1)\n",
    "    head = nn.Sequential(\n",
    "        nn.Linear(d, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 2)\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.Adam(head.parameters(), lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    train_ds = TensorDataset(emb_train, y_train_t)\n",
    "    train_dl = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "\n",
    "\n",
    "    best_prev_loss = float(\"inf\")\n",
    "    tol = 1e-4\n",
    "    patience = 3\n",
    "    stale_epochs = 0\n",
    "    max_epochs = 100\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        head.train()\n",
    "        epoch_loss = 0.0\n",
    "        for emb_b, lab_b in train_dl:\n",
    "            optimizer.zero_grad()\n",
    "            logits = head(emb_b)\n",
    "            loss = criterion(logits, lab_b)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * emb_b.size(0)\n",
    "\n",
    "        avg_loss = epoch_loss / len(train_ds)\n",
    "        print(f\"  Epoch {epoch:>3} — Loss: {avg_loss:.6f}\")\n",
    "        improvement = best_prev_loss - avg_loss\n",
    "        if improvement < tol:\n",
    "            stale_epochs += 1\n",
    "            print(f\"  ↳ small improvement (Δ={improvement:.2e}) stale {stale_epochs}/{patience}\")\n",
    "        else:\n",
    "            stale_epochs = 0\n",
    "            best_prev_loss = avg_loss\n",
    "        if stale_epochs >= patience:\n",
    "            print(f\"  ↳ Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "\n",
    "    head.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = head(emb_test).argmax(dim=1).detach().cpu().numpy()\n",
    "\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    micro_f1 = f1_score(y_test, preds, average=\"micro\")\n",
    "    macro_f1 = f1_score(y_test, preds, average=\"macro\")\n",
    "    weighted_f1 = f1_score(y_test, preds, average=\"weighted\")\n",
    "    macro_recall = recall_score(y_test, preds, average=\"macro\")\n",
    "\n",
    "    print(f\"Metrics for {group_name}:\")\n",
    "    print(f\"  Accuracy    : {acc*100:6.2f}%\")\n",
    "    print(f\"  Micro-F1    : {micro_f1*100:6.2f}%\")\n",
    "    print(f\"  Macro-F1    : {macro_f1*100:6.2f}%\")\n",
    "    print(f\"  Weighted-F1 : {weighted_f1*100:6.2f}%\")\n",
    "    print(f\"  Macro-Recall: {macro_recall*100:6.2f}%\\n\")\n",
    "\n",
    "    results[group_name] = head\n",
    "\n",
    "\n",
    "    del emb_train, emb_test, y_train_t, head\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache(); torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D_SfAfq1p2RN"
   },
   "outputs": [],
   "source": [
    "# do embedding without rag\n",
    "def title_only_features_from_group(group_dict, batch_size=16):\n",
    "\n",
    "    X, y, meta = [], [], []\n",
    "    for tkr, df in group_dict.items():\n",
    "        sdf = df[[\"symbol\", \"publishOn\", \"title\", \"label\"]].copy()\n",
    "        sdf[\"publishOn\"] = pd.to_datetime(sdf[\"publishOn\"], utc=True, errors=\"coerce\")\n",
    "        sdf = sdf.dropna(subset=[\"publishOn\"]).reset_index(drop=True)\n",
    "\n",
    "        titles = sdf[\"title\"].astype(str).tolist()\n",
    "        title_vecs = qwen_embed_batched(titles, batch_size=batch_size)\n",
    "\n",
    "        X.append(title_vecs)\n",
    "        y.extend(sdf[\"label\"].astype(int).tolist())\n",
    "        meta.extend(list(sdf[[\"symbol\",\"publishOn\",\"title\"]].itertuples(index=False, name=None)))\n",
    "\n",
    "    return np.vstack(X), np.asarray(y), meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "71d310687afa478bb27777f7625abee1",
      "a927e95219d3421aafc9c065b91e1779",
      "68a2da7d7afd49a6803e5cb5f5dd4f9d",
      "f44a08cf64d3434a8012835f8b1691af",
      "4647dcb648344ecda3ae3c2d8b7b8643",
      "5329b9c01c6343fcb5b218dcb24f29d7",
      "118cf1c52bbf48d8ae041fcd136a4a7f",
      "61076627e6c84928994441475653de22",
      "48237b8cc4c54ed8a1d2e1500d31cdd2",
      "0a2a8c3fc7944a84bcf0dcc37ac2d4be",
      "cce4f14da6bf4fd9ba245dcf690f9c68"
     ]
    },
    "executionInfo": {
     "elapsed": 54069,
     "status": "ok",
     "timestamp": 1757056357313,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -480
    },
    "id": "_iB0Wbr4m3bw",
    "outputId": "748c2aa8-554d-4701-8968-44dfddac15a8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71d310687afa478bb27777f7625abee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Tuning on group2 (DeepSeek title-only) ===\n",
      "  Epoch   1 — Loss: 0.691579\n",
      "  Epoch   2 — Loss: 0.690042\n",
      "  Epoch   3 — Loss: 0.689540\n",
      "  Epoch   4 — Loss: 0.689319\n",
      "  Epoch   5 — Loss: 0.688854\n",
      "  Epoch   6 — Loss: 0.688698\n",
      "  Epoch   7 — Loss: 0.688381\n",
      "  Epoch   8 — Loss: 0.687995\n",
      "  Epoch   9 — Loss: 0.687813\n",
      "  Epoch  10 — Loss: 0.687371\n",
      "  Epoch  11 — Loss: 0.686933\n",
      "  Epoch  12 — Loss: 0.686604\n",
      "  Epoch  13 — Loss: 0.686286\n",
      "  Epoch  14 — Loss: 0.686043\n",
      "  Epoch  15 — Loss: 0.685431\n",
      "  Epoch  16 — Loss: 0.685193\n",
      "  Epoch  17 — Loss: 0.684723\n",
      "  Epoch  18 — Loss: 0.684401\n",
      "  Epoch  19 — Loss: 0.683933\n",
      "  Epoch  20 — Loss: 0.683255\n",
      "  Epoch  21 — Loss: 0.683031\n",
      "  Epoch  22 — Loss: 0.682438\n",
      "  Epoch  23 — Loss: 0.681942\n",
      "  Epoch  24 — Loss: 0.681587\n",
      "  Epoch  25 — Loss: 0.681243\n",
      "  Epoch  26 — Loss: 0.680888\n",
      "  Epoch  27 — Loss: 0.680315\n",
      "  Epoch  28 — Loss: 0.679765\n",
      "  Epoch  29 — Loss: 0.678868\n",
      "  Epoch  30 — Loss: 0.678604\n",
      "  Epoch  31 — Loss: 0.677926\n",
      "  Epoch  32 — Loss: 0.677327\n",
      "  Epoch  33 — Loss: 0.677068\n",
      "  Epoch  34 — Loss: 0.676234\n",
      "  Epoch  35 — Loss: 0.675971\n",
      "  Epoch  36 — Loss: 0.674617\n",
      "  Epoch  37 — Loss: 0.674486\n",
      "  Epoch  38 — Loss: 0.673521\n",
      "  Epoch  39 — Loss: 0.673210\n",
      "  Epoch  40 — Loss: 0.672124\n",
      "  Epoch  41 — Loss: 0.671570\n",
      "  Epoch  42 — Loss: 0.671099\n",
      "  Epoch  43 — Loss: 0.670834\n",
      "  Epoch  44 — Loss: 0.669683\n",
      "  Epoch  45 — Loss: 0.668672\n",
      "  Epoch  46 — Loss: 0.667978\n",
      "  Epoch  47 — Loss: 0.667568\n",
      "  Epoch  48 — Loss: 0.666411\n",
      "  Epoch  49 — Loss: 0.665785\n",
      "  Epoch  50 — Loss: 0.664796\n",
      "  Epoch  51 — Loss: 0.664165\n",
      "  Epoch  52 — Loss: 0.663335\n",
      "  Epoch  53 — Loss: 0.662731\n",
      "  Epoch  54 — Loss: 0.661844\n",
      "  Epoch  55 — Loss: 0.661684\n",
      "  Epoch  56 — Loss: 0.660409\n",
      "  Epoch  57 — Loss: 0.659891\n",
      "  Epoch  58 — Loss: 0.658107\n",
      "  Epoch  59 — Loss: 0.657972\n",
      "  Epoch  60 — Loss: 0.656484\n",
      "  Epoch  61 — Loss: 0.656225\n",
      "  Epoch  62 — Loss: 0.655224\n",
      "  Epoch  63 — Loss: 0.654540\n",
      "  Epoch  64 — Loss: 0.653943\n",
      "  Epoch  65 — Loss: 0.653159\n",
      "  Epoch  66 — Loss: 0.652139\n",
      "  Epoch  67 — Loss: 0.651262\n",
      "  Epoch  68 — Loss: 0.650683\n",
      "  Epoch  69 — Loss: 0.649660\n",
      "  Epoch  70 — Loss: 0.648868\n",
      "  Epoch  71 — Loss: 0.648039\n",
      "  Epoch  72 — Loss: 0.647890\n",
      "  Epoch  73 — Loss: 0.646283\n",
      "  Epoch  74 — Loss: 0.646014\n",
      "  Epoch  75 — Loss: 0.645007\n",
      "  Epoch  76 — Loss: 0.644192\n",
      "  Epoch  77 — Loss: 0.643387\n",
      "  Epoch  78 — Loss: 0.642578\n",
      "  Epoch  79 — Loss: 0.641853\n",
      "  Epoch  80 — Loss: 0.640793\n",
      "  Epoch  81 — Loss: 0.640517\n",
      "  Epoch  82 — Loss: 0.639551\n",
      "  Epoch  83 — Loss: 0.638058\n",
      "  Epoch  84 — Loss: 0.637619\n",
      "  Epoch  85 — Loss: 0.636872\n",
      "  Epoch  86 — Loss: 0.635739\n",
      "  Epoch  87 — Loss: 0.635905\n",
      "  ↳ small improvement (Δ=-1.66e-04) stale 1/3\n",
      "  Epoch  88 — Loss: 0.634174\n",
      "  Epoch  89 — Loss: 0.633738\n",
      "  Epoch  90 — Loss: 0.632825\n",
      "  Epoch  91 — Loss: 0.632139\n",
      "  Epoch  92 — Loss: 0.632106\n",
      "  ↳ small improvement (Δ=3.38e-05) stale 1/3\n",
      "  Epoch  93 — Loss: 0.630787\n",
      "  Epoch  94 — Loss: 0.629782\n",
      "  Epoch  95 — Loss: 0.628818\n",
      "  Epoch  96 — Loss: 0.627965\n",
      "  Epoch  97 — Loss: 0.627422\n",
      "  Epoch  98 — Loss: 0.626449\n",
      "  Epoch  99 — Loss: 0.625037\n",
      "  Epoch 100 — Loss: 0.624364\n",
      "Metrics for group2:\n",
      "  Accuracy    :  58.44%\n",
      "  Micro-F1    :  58.44%\n",
      "  Macro-F1    :  57.17%\n",
      "  Weighted-F1 :  57.77%\n",
      "  Macro-Recall:  57.42%\n",
      "\n",
      "\n",
      "=== Tuning on group3 (DeepSeek title-only) ===\n",
      "  Epoch   1 — Loss: 0.690333\n",
      "  Epoch   2 — Loss: 0.685548\n",
      "  Epoch   3 — Loss: 0.684734\n",
      "  Epoch   4 — Loss: 0.684217\n",
      "  Epoch   5 — Loss: 0.683661\n",
      "  Epoch   6 — Loss: 0.683189\n",
      "  Epoch   7 — Loss: 0.682951\n",
      "  Epoch   8 — Loss: 0.682284\n",
      "  Epoch   9 — Loss: 0.681894\n",
      "  Epoch  10 — Loss: 0.681343\n",
      "  Epoch  11 — Loss: 0.680869\n",
      "  Epoch  12 — Loss: 0.680232\n",
      "  Epoch  13 — Loss: 0.679727\n",
      "  Epoch  14 — Loss: 0.678990\n",
      "  Epoch  15 — Loss: 0.678753\n",
      "  Epoch  16 — Loss: 0.678221\n",
      "  Epoch  17 — Loss: 0.677527\n",
      "  Epoch  18 — Loss: 0.676878\n",
      "  Epoch  19 — Loss: 0.676068\n",
      "  Epoch  20 — Loss: 0.675623\n",
      "  Epoch  21 — Loss: 0.675005\n",
      "  Epoch  22 — Loss: 0.673982\n",
      "  Epoch  23 — Loss: 0.673472\n",
      "  Epoch  24 — Loss: 0.672447\n",
      "  Epoch  25 — Loss: 0.671925\n",
      "  Epoch  26 — Loss: 0.671261\n",
      "  Epoch  27 — Loss: 0.670644\n",
      "  Epoch  28 — Loss: 0.669534\n",
      "  Epoch  29 — Loss: 0.668706\n",
      "  Epoch  30 — Loss: 0.667795\n",
      "  Epoch  31 — Loss: 0.666706\n",
      "  Epoch  32 — Loss: 0.666172\n",
      "  Epoch  33 — Loss: 0.665240\n",
      "  Epoch  34 — Loss: 0.664491\n",
      "  Epoch  35 — Loss: 0.663640\n",
      "  Epoch  36 — Loss: 0.662526\n",
      "  Epoch  37 — Loss: 0.661734\n",
      "  Epoch  38 — Loss: 0.660217\n",
      "  Epoch  39 — Loss: 0.658896\n",
      "  Epoch  40 — Loss: 0.658278\n",
      "  Epoch  41 — Loss: 0.657375\n",
      "  Epoch  42 — Loss: 0.655979\n",
      "  Epoch  43 — Loss: 0.655590\n",
      "  Epoch  44 — Loss: 0.653705\n",
      "  Epoch  45 — Loss: 0.652960\n",
      "  Epoch  46 — Loss: 0.651463\n",
      "  Epoch  47 — Loss: 0.651042\n",
      "  Epoch  48 — Loss: 0.650298\n",
      "  Epoch  49 — Loss: 0.648730\n",
      "  Epoch  50 — Loss: 0.648250\n",
      "  Epoch  51 — Loss: 0.645927\n",
      "  Epoch  52 — Loss: 0.645063\n",
      "  Epoch  53 — Loss: 0.643548\n",
      "  Epoch  54 — Loss: 0.642521\n",
      "  Epoch  55 — Loss: 0.641219\n",
      "  Epoch  56 — Loss: 0.640229\n",
      "  Epoch  57 — Loss: 0.639123\n",
      "  Epoch  58 — Loss: 0.637417\n",
      "  Epoch  59 — Loss: 0.636541\n",
      "  Epoch  60 — Loss: 0.634113\n",
      "  Epoch  61 — Loss: 0.633213\n",
      "  Epoch  62 — Loss: 0.632675\n",
      "  Epoch  63 — Loss: 0.630881\n",
      "  Epoch  64 — Loss: 0.629906\n",
      "  Epoch  65 — Loss: 0.628552\n",
      "  Epoch  66 — Loss: 0.627060\n",
      "  Epoch  67 — Loss: 0.626076\n",
      "  Epoch  68 — Loss: 0.624761\n",
      "  Epoch  69 — Loss: 0.622738\n",
      "  Epoch  70 — Loss: 0.621988\n",
      "  Epoch  71 — Loss: 0.620713\n",
      "  Epoch  72 — Loss: 0.618905\n",
      "  Epoch  73 — Loss: 0.617100\n",
      "  Epoch  74 — Loss: 0.617187\n",
      "  ↳ small improvement (Δ=-8.65e-05) stale 1/3\n",
      "  Epoch  75 — Loss: 0.614965\n",
      "  Epoch  76 — Loss: 0.613348\n",
      "  Epoch  77 — Loss: 0.612697\n",
      "  Epoch  78 — Loss: 0.610304\n",
      "  Epoch  79 — Loss: 0.609533\n",
      "  Epoch  80 — Loss: 0.607786\n",
      "  Epoch  81 — Loss: 0.606934\n",
      "  Epoch  82 — Loss: 0.605385\n",
      "  Epoch  83 — Loss: 0.603618\n",
      "  Epoch  84 — Loss: 0.602437\n",
      "  Epoch  85 — Loss: 0.601083\n",
      "  Epoch  86 — Loss: 0.600994\n",
      "  ↳ small improvement (Δ=8.92e-05) stale 1/3\n",
      "  Epoch  87 — Loss: 0.598195\n",
      "  Epoch  88 — Loss: 0.596944\n",
      "  Epoch  89 — Loss: 0.595525\n",
      "  Epoch  90 — Loss: 0.594292\n",
      "  Epoch  91 — Loss: 0.592224\n",
      "  Epoch  92 — Loss: 0.591529\n",
      "  Epoch  93 — Loss: 0.589709\n",
      "  Epoch  94 — Loss: 0.587743\n",
      "  Epoch  95 — Loss: 0.587707\n",
      "  ↳ small improvement (Δ=3.63e-05) stale 1/3\n",
      "  Epoch  96 — Loss: 0.585296\n",
      "  Epoch  97 — Loss: 0.583612\n",
      "  Epoch  98 — Loss: 0.582779\n",
      "  Epoch  99 — Loss: 0.581270\n",
      "  Epoch 100 — Loss: 0.579608\n",
      "Metrics for group3:\n",
      "  Accuracy    :  61.25%\n",
      "  Micro-F1    :  61.25%\n",
      "  Macro-F1    :  59.42%\n",
      "  Weighted-F1 :  60.50%\n",
      "  Macro-Recall:  59.52%\n",
      "\n",
      "\n",
      "=== Tuning on group4 (DeepSeek title-only) ===\n",
      "  Epoch   1 — Loss: 0.694314\n",
      "  Epoch   2 — Loss: 0.688255\n",
      "  Epoch   3 — Loss: 0.686983\n",
      "  Epoch   4 — Loss: 0.686469\n",
      "  Epoch   5 — Loss: 0.686298\n",
      "  Epoch   6 — Loss: 0.685823\n",
      "  Epoch   7 — Loss: 0.685575\n",
      "  Epoch   8 — Loss: 0.685266\n",
      "  Epoch   9 — Loss: 0.685212\n",
      "  ↳ small improvement (Δ=5.35e-05) stale 1/3\n",
      "  Epoch  10 — Loss: 0.684729\n",
      "  Epoch  11 — Loss: 0.684112\n",
      "  Epoch  12 — Loss: 0.683779\n",
      "  Epoch  13 — Loss: 0.683622\n",
      "  Epoch  14 — Loss: 0.683143\n",
      "  Epoch  15 — Loss: 0.683079\n",
      "  ↳ small improvement (Δ=6.37e-05) stale 1/3\n",
      "  Epoch  16 — Loss: 0.682366\n",
      "  Epoch  17 — Loss: 0.681817\n",
      "  Epoch  18 — Loss: 0.681513\n",
      "  Epoch  19 — Loss: 0.681036\n",
      "  Epoch  20 — Loss: 0.680681\n",
      "  Epoch  21 — Loss: 0.680126\n",
      "  Epoch  22 — Loss: 0.680126\n",
      "  ↳ small improvement (Δ=1.65e-07) stale 1/3\n",
      "  Epoch  23 — Loss: 0.679168\n",
      "  Epoch  24 — Loss: 0.678700\n",
      "  Epoch  25 — Loss: 0.678223\n",
      "  Epoch  26 — Loss: 0.677762\n",
      "  Epoch  27 — Loss: 0.677387\n",
      "  Epoch  28 — Loss: 0.676667\n",
      "  Epoch  29 — Loss: 0.676484\n",
      "  Epoch  30 — Loss: 0.675786\n",
      "  Epoch  31 — Loss: 0.674824\n",
      "  Epoch  32 — Loss: 0.674481\n",
      "  Epoch  33 — Loss: 0.673941\n",
      "  Epoch  34 — Loss: 0.673861\n",
      "  ↳ small improvement (Δ=7.91e-05) stale 1/3\n",
      "  Epoch  35 — Loss: 0.672414\n",
      "  Epoch  36 — Loss: 0.672378\n",
      "  ↳ small improvement (Δ=3.53e-05) stale 1/3\n",
      "  Epoch  37 — Loss: 0.671569\n",
      "  Epoch  38 — Loss: 0.670588\n",
      "  Epoch  39 — Loss: 0.670044\n",
      "  Epoch  40 — Loss: 0.669673\n",
      "  Epoch  41 — Loss: 0.668777\n",
      "  Epoch  42 — Loss: 0.668015\n",
      "  Epoch  43 — Loss: 0.667130\n",
      "  Epoch  44 — Loss: 0.667028\n",
      "  Epoch  45 — Loss: 0.666126\n",
      "  Epoch  46 — Loss: 0.665260\n",
      "  Epoch  47 — Loss: 0.664366\n",
      "  Epoch  48 — Loss: 0.663771\n",
      "  Epoch  49 — Loss: 0.663026\n",
      "  Epoch  50 — Loss: 0.661694\n",
      "  Epoch  51 — Loss: 0.661370\n",
      "  Epoch  52 — Loss: 0.660272\n",
      "  Epoch  53 — Loss: 0.660346\n",
      "  ↳ small improvement (Δ=-7.39e-05) stale 1/3\n",
      "  Epoch  54 — Loss: 0.659083\n",
      "  Epoch  55 — Loss: 0.657517\n",
      "  Epoch  56 — Loss: 0.656605\n",
      "  Epoch  57 — Loss: 0.655729\n",
      "  Epoch  58 — Loss: 0.654544\n",
      "  Epoch  59 — Loss: 0.653797\n",
      "  Epoch  60 — Loss: 0.652768\n",
      "  Epoch  61 — Loss: 0.651593\n",
      "  Epoch  62 — Loss: 0.650493\n",
      "  Epoch  63 — Loss: 0.649625\n",
      "  Epoch  64 — Loss: 0.648477\n",
      "  Epoch  65 — Loss: 0.647445\n",
      "  Epoch  66 — Loss: 0.646349\n",
      "  Epoch  67 — Loss: 0.645189\n",
      "  Epoch  68 — Loss: 0.643942\n",
      "  Epoch  69 — Loss: 0.642870\n",
      "  Epoch  70 — Loss: 0.641277\n",
      "  Epoch  71 — Loss: 0.640769\n",
      "  Epoch  72 — Loss: 0.639746\n",
      "  Epoch  73 — Loss: 0.638421\n",
      "  Epoch  74 — Loss: 0.636420\n",
      "  Epoch  75 — Loss: 0.635424\n",
      "  Epoch  76 — Loss: 0.634684\n",
      "  Epoch  77 — Loss: 0.632989\n",
      "  Epoch  78 — Loss: 0.631843\n",
      "  Epoch  79 — Loss: 0.630350\n",
      "  Epoch  80 — Loss: 0.629349\n",
      "  Epoch  81 — Loss: 0.627318\n",
      "  Epoch  82 — Loss: 0.625826\n",
      "  Epoch  83 — Loss: 0.625650\n",
      "  Epoch  84 — Loss: 0.623376\n",
      "  Epoch  85 — Loss: 0.621820\n",
      "  Epoch  86 — Loss: 0.620640\n",
      "  Epoch  87 — Loss: 0.619915\n",
      "  Epoch  88 — Loss: 0.618241\n",
      "  Epoch  89 — Loss: 0.616124\n",
      "  Epoch  90 — Loss: 0.614633\n",
      "  Epoch  91 — Loss: 0.613592\n",
      "  Epoch  92 — Loss: 0.611934\n",
      "  Epoch  93 — Loss: 0.612344\n",
      "  ↳ small improvement (Δ=-4.10e-04) stale 1/3\n",
      "  Epoch  94 — Loss: 0.609406\n",
      "  Epoch  95 — Loss: 0.607466\n",
      "  Epoch  96 — Loss: 0.606206\n",
      "  Epoch  97 — Loss: 0.603169\n",
      "  Epoch  98 — Loss: 0.603412\n",
      "  ↳ small improvement (Δ=-2.43e-04) stale 1/3\n",
      "  Epoch  99 — Loss: 0.602170\n",
      "  Epoch 100 — Loss: 0.601751\n",
      "Metrics for group4:\n",
      "  Accuracy    :  59.69%\n",
      "  Micro-F1    :  59.69%\n",
      "  Macro-F1    :  58.27%\n",
      "  Weighted-F1 :  59.13%\n",
      "  Macro-Recall:  58.35%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gc, torch, numpy as np, pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_ID = \"deepseek-ai/deepseek-llm-7b-base\"\n",
    "tokenizer, model, deepseek_embed_batched = build_embedder(MODEL_ID, max_len=256)\n",
    "qwen_embed_batched = deepseek_embed_batched\n",
    "\n",
    "results = {}\n",
    "for group_name, labeled_group in [\n",
    "    (\"group2\", labeled_group2),\n",
    "    (\"group3\", labeled_group3),\n",
    "    (\"group4\", labeled_group4),\n",
    "]:\n",
    "    print(f\"\\n=== Tuning on {group_name} (DeepSeek title-only) ===\")\n",
    "\n",
    "    X, y, meta = title_only_features_from_group(labeled_group, batch_size=16)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    emb_train = torch.from_numpy(X_train).to(device).float()\n",
    "    emb_test  = torch.from_numpy(X_test ).to(device).float()\n",
    "    y_train_t = torch.from_numpy(y_train).to(device).long()\n",
    "\n",
    "    d = emb_train.size(1)\n",
    "    head = nn.Sequential(\n",
    "        nn.Linear(d, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 2)\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(head.parameters(), lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_ds = TensorDataset(emb_train, y_train_t)\n",
    "    train_dl = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "\n",
    "    best_prev_loss = float(\"inf\")\n",
    "    tol = 1e-4\n",
    "    patience = 3\n",
    "    stale_epochs = 0\n",
    "    max_epochs = 100\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        head.train()\n",
    "        epoch_loss = 0.0\n",
    "        for emb_b, lab_b in train_dl:\n",
    "            optimizer.zero_grad()\n",
    "            logits = head(emb_b)\n",
    "            loss = criterion(logits, lab_b)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * emb_b.size(0)\n",
    "\n",
    "        avg_loss = epoch_loss / len(train_ds)\n",
    "        print(f\"  Epoch {epoch:>3} — Loss: {avg_loss:.6f}\")\n",
    "        improvement = best_prev_loss - avg_loss\n",
    "        if improvement < tol:\n",
    "            stale_epochs += 1\n",
    "            print(f\"  ↳ small improvement (Δ={improvement:.2e}) stale {stale_epochs}/{patience}\")\n",
    "        else:\n",
    "            stale_epochs = 0\n",
    "            best_prev_loss = avg_loss\n",
    "        if stale_epochs >= patience:\n",
    "            print(f\"  ↳ Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "    head.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = head(emb_test).argmax(dim=1).detach().cpu().numpy()\n",
    "\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    micro_f1 = f1_score(y_test, preds, average=\"micro\")\n",
    "    macro_f1 = f1_score(y_test, preds, average=\"macro\")\n",
    "    weighted_f1 = f1_score(y_test, preds, average=\"weighted\")\n",
    "    macro_recall = recall_score(y_test, preds, average=\"macro\")\n",
    "\n",
    "    print(f\"Metrics for {group_name}:\")\n",
    "    print(f\"  Accuracy    : {acc*100:6.2f}%\")\n",
    "    print(f\"  Micro-F1    : {micro_f1*100:6.2f}%\")\n",
    "    print(f\"  Macro-F1    : {macro_f1*100:6.2f}%\")\n",
    "    print(f\"  Weighted-F1 : {weighted_f1*100:6.2f}%\")\n",
    "    print(f\"  Macro-Recall: {macro_recall*100:6.2f}%\\n\")\n",
    "\n",
    "    results[group_name] = head\n",
    "\n",
    "    del emb_train, emb_test, y_train_t, head\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache(); torch.cuda.ipc_collect()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPFvDtwt3d+cNPXoBu1P2oj",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
