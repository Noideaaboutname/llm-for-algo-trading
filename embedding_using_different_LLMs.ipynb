{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4lfzrGaLVxCG"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os, nbformat as nbf, shutil, json, mimetypes\n",
    "src  = \"/content/drive/My Drive/Sentimental Analysis for Algo trading/different_extracting_methods_superclean_version (2).ipynb\"\n",
    "dst  = \"/content/drive/My Drive/Sentimental Analysis for Algo trading/different_extracting_methods_superclean_version_clean.ipynb\"\n",
    "\n",
    "\n",
    "nb = nbf.read(src, as_version=nbf.NO_CONVERT)\n",
    "nb.metadata.pop(\"widgets\", None)\n",
    "for c in nb.cells:\n",
    "    c.metadata.pop(\"widgets\", None)\n",
    "\n",
    "nbf.write(nb, dst)\n",
    "\n",
    "# 3) remove the “bad” file so only the fresh one is left\n",
    "os.remove(src)\n",
    "\n",
    "print(\"✅  Clean file written:\", dst)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2089,
     "status": "aborted",
     "timestamp": 1751552698564,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "dFZMSN91tR92"
   },
   "outputs": [],
   "source": [
    "!pip install -q --upgrade transformers sentence-transformers accelerate\n",
    "!pip install mteb\n",
    "!pip install -U --no-cache-dir --force-reinstall \"datasets>=2.19.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10360,
     "status": "ok",
     "timestamp": 1751559868606,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "yPe9NVRbRmPb",
    "outputId": "fbc6085f-5115-4ab0-c7e6-0075d3d4ef28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mteb in /usr/local/lib/python3.11/dist-packages (1.38.33)\n",
      "Requirement already satisfied: datasets>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from mteb) (3.6.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from mteb) (2.0.2)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from mteb) (2.32.3)\n",
      "Requirement already satisfied: scikit_learn>=1.0.2 in /usr/local/lib/python3.11/dist-packages (from mteb) (1.6.1)\n",
      "Requirement already satisfied: scipy>=0.0.0 in /usr/local/lib/python3.11/dist-packages (from mteb) (1.15.3)\n",
      "Requirement already satisfied: sentence_transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from mteb) (4.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from mteb) (4.14.0)\n",
      "Requirement already satisfied: torch>1.0.0 in /usr/local/lib/python3.11/dist-packages (from mteb) (2.6.0+cu124)\n",
      "Requirement already satisfied: tqdm>1.0.0 in /usr/local/lib/python3.11/dist-packages (from mteb) (4.67.1)\n",
      "Requirement already satisfied: rich>=0.0.0 in /usr/local/lib/python3.11/dist-packages (from mteb) (13.9.4)\n",
      "Requirement already satisfied: pytrec-eval-terrier>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from mteb) (0.5.7)\n",
      "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from mteb) (2.11.7)\n",
      "Requirement already satisfied: eval_type_backport>=0.0.0 in /usr/local/lib/python3.11/dist-packages (from mteb) (0.2.2)\n",
      "Requirement already satisfied: polars>=0.20.22 in /usr/local/lib/python3.11/dist-packages (from mteb) (1.21.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19.0->mteb) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19.0->mteb) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19.0->mteb) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19.0->mteb) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19.0->mteb) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19.0->mteb) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->mteb) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19.0->mteb) (0.33.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19.0->mteb) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19.0->mteb) (6.0.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->mteb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->mteb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->mteb) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->mteb) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->mteb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->mteb) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->mteb) (2025.6.15)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=0.0.0->mteb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=0.0.0->mteb) (2.19.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit_learn>=1.0.2->mteb) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit_learn>=1.0.2->mteb) (3.6.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers>=3.0.0->mteb) (4.53.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence_transformers>=3.0.0->mteb) (11.2.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>1.0.0->mteb) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>1.0.0->mteb) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>1.0.0->mteb) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>1.0.0->mteb) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>1.0.0->mteb) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>1.0.0->mteb) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>1.0.0->mteb) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>1.0.0->mteb) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>1.0.0->mteb) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>1.0.0->mteb) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>1.0.0->mteb) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>1.0.0->mteb) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>1.0.0->mteb) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>1.0.0->mteb) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>1.0.0->mteb) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>1.0.0->mteb) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>1.0.0->mteb) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>1.0.0->mteb) (1.3.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->mteb) (3.11.15)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets>=2.19.0->mteb) (1.1.5)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=0.0.0->mteb) (0.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers>=3.0.0->mteb) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers>=3.0.0->mteb) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers>=3.0.0->mteb) (0.5.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>1.0.0->mteb) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.19.0->mteb) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.19.0->mteb) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.19.0->mteb) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->mteb) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->mteb) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->mteb) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->mteb) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->mteb) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->mteb) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->mteb) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.19.0->mteb) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install mteb\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "from sentence_transformers.models import Transformer, Pooling\n",
    "from mteb import get_tasks, MTEB\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import scipy.stats as st\n",
    "import numpy as np, time, torch\n",
    "from datasets  import load_dataset\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report,f1_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV , StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree        import DecisionTreeRegressor\n",
    "from sklearn.metrics     import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.tree        import DecisionTreeRegressor\n",
    "from sklearn.metrics     import mean_squared_error, mean_absolute_error, r2_score\n",
    "import yfinance as yf\n",
    "import openai\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3475,
     "status": "ok",
     "timestamp": 1751559892881,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "_2gUawud889_",
    "outputId": "80405b44-82ac-48ea-d042-db14ac0f93ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "-rw------- 1 root root 2.4M Jun 29 14:49 /content/drive/MyDrive/colab_saves/company_dfs.pkl\n",
      "<class 'dict'>\n",
      "dict_keys(['JPM', 'BAC', 'WFC', 'RY'])\n"
     ]
    }
   ],
   "source": [
    "# Download the data from my google drive so that I could avoid rerun the function below\n",
    "from google.colab import drive\n",
    "import pickle, os\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "path = '/content/drive/MyDrive/colab_saves/company_dfs.pkl'\n",
    "\n",
    "!ls -lh \"{path}\"\n",
    "with open(path, 'rb') as f:\n",
    "    company_dfs = pickle.load(f)\n",
    "print(type(company_dfs))\n",
    "print(company_dfs.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2164,
     "status": "aborted",
     "timestamp": 1751552698654,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "740G82DBBXZZ"
   },
   "outputs": [],
   "source": [
    "import os, time, requests, pandas as pd\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "API_KEY = os.getenv(\"RAPIDAPI_KEY\") or \"ad973560c8msh96909b7fb3cc3fdp1c1065jsn5a49e6b93474\"\n",
    "HEADERS = {\n",
    "    \"X-RapidAPI-Key\":  API_KEY,\n",
    "    \"X-RapidAPI-Host\": \"seeking-alpha.p.rapidapi.com\",\n",
    "}\n",
    "\n",
    "session = requests.Session()\n",
    "retry_cfg = Retry(\n",
    "    total=5,\n",
    "    backoff_factor=1,\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    allowed_methods=[\"GET\"],\n",
    ")\n",
    "session.mount(\"https://\", HTTPAdapter(max_retries=retry_cfg))\n",
    "\n",
    "\n",
    "def _respect_rate_limit(resp):\n",
    "    \"\"\"Sleep if remaining-requests header is low.\"\"\"\n",
    "    left = resp.headers.get(\"X-RateLimit-Requests-Remaining\")\n",
    "    reset = resp.headers.get(\"X-RateLimit-Requests-Reset\")\n",
    "    if left is not None and reset is not None:\n",
    "        left, reset = int(left), int(reset)\n",
    "        if left < 2:\n",
    "            sleep_s = max(reset - int(time.time()), 1)\n",
    "            print(f\"[rate-limit] only {left} calls left → sleeping {sleep_s}s\")\n",
    "            time.sleep(sleep_s)\n",
    "\n",
    "# extracting the news with particular symbols\n",
    "\n",
    "def fetch_symbol_news(symbol: str,\n",
    "                      max_items: int = 200,\n",
    "                      page_size: int = 40,\n",
    "                      since: int = 0,\n",
    "                      until: int = 0) -> pd.DataFrame:\n",
    "    url   = \"https://seeking-alpha.p.rapidapi.com/news/v2/list-by-symbol\"\n",
    "    items = []\n",
    "    page  = 1\n",
    "\n",
    "    while len(items) < max_items:\n",
    "        p = {\"id\": symbol, \"size\": page_size, \"number\": page}\n",
    "        if since: p[\"since\"] = since\n",
    "        if until: p[\"until\"] = until\n",
    "\n",
    "        r = session.get(url, headers=HEADERS, params=p, timeout=20)\n",
    "        r.raise_for_status()\n",
    "        batch = r.json().get(\"data\", [])\n",
    "        if not batch:\n",
    "            break\n",
    "\n",
    "        items.extend(batch)\n",
    "        if len(batch) < page_size:\n",
    "            break\n",
    "\n",
    "        page += 1\n",
    "        time.sleep(0.25 + 0.25 * os.urandom(1)[0] / 255)\n",
    "\n",
    "    if not items:\n",
    "        return pd.DataFrame(columns=[\"title\", \"publishOn\", \"symbol\"])\n",
    "\n",
    "    df = pd.json_normalize(items[:max_items])\n",
    "\n",
    "    for c in [\"attributes.title\", \"attributes.headline\", \"attributes.teaser\"]:\n",
    "        if c in df.columns:\n",
    "            df = df.rename(columns={c: \"title\"})\n",
    "            break\n",
    "    if \"title\" not in df.columns:\n",
    "        df[\"title\"] = \"\"\n",
    "\n",
    "    if \"publishOn\" in df.columns:\n",
    "        df[\"publishOn\"] = pd.to_datetime(df[\"publishOn\"], utc=True)\n",
    "    elif \"publishedAt\" in df.columns:\n",
    "        df[\"publishOn\"] = pd.to_datetime(df[\"publishedAt\"], utc=True)\n",
    "\n",
    "    df[\"symbol\"] = symbol\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2164,
     "status": "aborted",
     "timestamp": 1751552698654,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "LjyNU2SsaiKn"
   },
   "outputs": [],
   "source": [
    "# collecting the news relating to JPM, BAC, WFC and RY\n",
    "df_jpm = fetch_symbol_news(\"JPM\", max_items=200, since=0, until=0)\n",
    "df_bac = fetch_symbol_news(\"BAC\", max_items=200, since=0, until=0)\n",
    "df_wfc = fetch_symbol_news(\"WFC\", max_items=200, since=0, until=0)\n",
    "df_ry  = fetch_symbol_news(\"RY\",  max_items=200, since=0, until=0)\n",
    "company_dfs = {\n",
    "    \"JPM\": df_jpm,\n",
    "    \"BAC\": df_bac,\n",
    "    \"WFC\": df_wfc,\n",
    "    \"RY\":  df_ry,}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WkhkQHe6tRxU"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2162,
     "status": "aborted",
     "timestamp": 1751552698655,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "TpodW_o2woXc"
   },
   "outputs": [],
   "source": [
    "# try 6 LLM transformers to do embedding\n",
    "transformer_1 = Transformer(\"sentence-transformers/paraphrase-MiniLM-L6-v2\",\n",
    "                          max_seq_length=128)\n",
    "\n",
    "transformer_2 = Transformer(\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    max_seq_length=128)\n",
    "\n",
    "transformer_3 = Transformer(\"sentence-transformers/gtr-t5-large\",\n",
    "                          max_seq_length=128)\n",
    "\n",
    "transformer_4 = Transformer(\"intfloat/e5-large-v2\",\n",
    "    max_seq_length=128)\n",
    "\n",
    "transformer_5 = Transformer(\"sentence-transformers/all-MiniLM-L12-v2\")\n",
    "\n",
    "transformer_6 = Transformer(\"sentence-transformers/stsb-roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2160,
     "status": "aborted",
     "timestamp": 1751552698656,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "uRibgj1xM_EN"
   },
   "outputs": [],
   "source": [
    "# testing on the LLMs embedding results\n",
    "backbones = [\n",
    "    \"sentence-transformers/paraphrase-MiniLM-L6-v2\",\n",
    "    \"sentence-transformers/all-mpnet-base-v2\",\n",
    "    \"sentence-transformers/gtr-t5-large\",\n",
    "    \"intfloat/e5-large-v2\",\n",
    "    \"sentence-transformers/all-MiniLM-L12-v2\",\n",
    "    \"sentence-transformers/stsb-roberta-large\"]\n",
    "\n",
    "def quick_sts(model_name: str) -> float:\n",
    "    model = SentenceTransformer(model_name, device=\"cuda\")\n",
    "    sts   = load_dataset(\"stsb_multi_mt\", name=\"en\", split=\"dev[:1500]\")\n",
    "    emb1  = model.encode(sts[\"sentence1\"], batch_size=64, convert_to_numpy=True)\n",
    "    emb2  = model.encode(sts[\"sentence2\"], batch_size=64, convert_to_numpy=True)\n",
    "    cos   = util.cos_sim(emb1, emb2).diagonal()\n",
    "    return st.spearmanr(cos, sts[\"similarity_score\"]).correlation\n",
    "\n",
    "def quick_banking(model_name: str,\n",
    "                  n_train: int = 1000,\n",
    "                  n_test: int  = 2000) -> float:\n",
    "    ds   = load_dataset(\"PolyAI/banking77\", split=\"train\").shuffle(seed=42)\n",
    "    text = ds[\"text\"][:n_train + n_test]\n",
    "    y    = ds[\"label\"][:n_train + n_test]\n",
    "\n",
    "    X_tr_raw, X_te_raw, y_tr, y_te = train_test_split(\n",
    "        text, y, train_size=n_train, stratify=y, random_state=42)\n",
    "\n",
    "    model = SentenceTransformer(model_name, device=\"cuda\")\n",
    "    X_tr  = model.encode(X_tr_raw, convert_to_numpy=True)\n",
    "    X_te  = model.encode(X_te_raw, convert_to_numpy=True)\n",
    "\n",
    "    clf   = LogisticRegression(max_iter=1000).fit(X_tr, y_tr)\n",
    "    return accuracy_score(y_te, clf.predict(X_te))\n",
    "\n",
    "def quick_speed(model_name: str,\n",
    "                n_sent: int = 2048,\n",
    "                sent_len: int = 16) -> tuple[float, int]:\n",
    "    dummy  = [\"hello world\"] * n_sent\n",
    "    model  = SentenceTransformer(model_name, device=\"cuda\")\n",
    "    t0     = time.time()\n",
    "    _      = model.encode(dummy, batch_size=128, convert_to_numpy=True)\n",
    "    sec    = time.time() - t0\n",
    "    dim    = model.get_sentence_embedding_dimension()\n",
    "    return n_sent / sec, dim\n",
    "\n",
    "for mdl in backbones:\n",
    "    short = mdl.split('/')[-1]\n",
    "    sts_rho        = quick_sts(mdl)\n",
    "    banking_acc    = quick_banking(mdl)\n",
    "    toks_per_sec, d = quick_speed(mdl)\n",
    "\n",
    "    print(f\"{short:30s} | STS ρ  {sts_rho: .4f}\"\n",
    "          f\" | Banking acc {banking_acc: .3f}\"\n",
    "          f\" | {int(toks_per_sec):5d} sents/s\"\n",
    "          f\" | {d:4d} dims\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NKG_cJtlX1BQ"
   },
   "source": [
    "In the previous example, I tested the embedding performance of 6 sentence‐embedding backbones by measuring (1) spearman's rho comparing to human scoring (2) classification accuracy on a simple logistic classifier (3) number of embeddings that could be proceeded per second. Considering the task onward, I choose (2) as main criterion for model selection because I aim to classify the stock price movement in the next step. Overall, the embedding of LLM paraphrase-MiniLM-L6-v2 and stsb-roberta-large has highest classification accuracy, which I choose as the optimal models in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ydf04xCZXxAV"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2158,
     "status": "aborted",
     "timestamp": 1751552698656,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "GTb9ymkddD8c"
   },
   "outputs": [],
   "source": [
    "# use stsb-roberta-large to do embedding\n",
    "backbone     = \"sentence-transformers/stsb-roberta-large\"\n",
    "transformer  = Transformer(backbone, max_seq_length=128)\n",
    "dim          = transformer.get_word_embedding_dimension()\n",
    "\n",
    "mean_pool    = Pooling(dim, pooling_mode_mean_tokens=True)\n",
    "max_pool     = Pooling(dim, pooling_mode_mean_tokens=False, pooling_mode_max_tokens=True)\n",
    "maxmean_pool = Pooling(dim, pooling_mode_mean_tokens=True,  pooling_mode_max_tokens=True)\n",
    "pseudo_pool  = Pooling(dim, pooling_mode_mean_tokens=False, pooling_mode_max_tokens=False,\n",
    "                       pooling_mode_cls_token=True)\n",
    "\n",
    "model_mean    = SentenceTransformer(modules=[transformer, mean_pool],    device=\"cuda\")\n",
    "model_max     = SentenceTransformer(modules=[transformer, max_pool],     device=\"cuda\")\n",
    "model_maxmean = SentenceTransformer(modules=[transformer, maxmean_pool], device=\"cuda\")\n",
    "model_pseudo  = SentenceTransformer(modules=[transformer, pseudo_pool],  device=\"cuda\")\n",
    "\n",
    "variants = {\n",
    "    \"mean\":    model_mean,\n",
    "    \"max\":     model_max,\n",
    "    \"maxmean\": model_maxmean,\n",
    "    \"pseudo\":  model_pseudo,\n",
    "}\n",
    "\n",
    "buffers = {name: [] for name in variants}\n",
    "\n",
    "for sym, df in company_dfs.items():\n",
    "    if \"attributes.publishOn\" in df.columns:\n",
    "        df = df.rename(columns={\"attributes.publishOn\": \"publishOn\"})\n",
    "\n",
    "    texts = df[\"title\"].fillna(\"\").tolist()\n",
    "\n",
    "    for name, model in variants.items():\n",
    "        emb = model.encode(texts, batch_size=32, convert_to_numpy=True)\n",
    "        print(f\"{name:7s} → {emb.shape[0]} samples × {emb.shape[1]}-dim embeddings\")\n",
    "        df[f\"emb_{name}\"] = list(emb)\n",
    "\n",
    "\n",
    "        buffers[name].append(df.assign(symbol=sym))\n",
    "\n",
    "X_store = {}\n",
    "\n",
    "for name, parts in buffers.items():\n",
    "    big_df = pd.concat(parts, ignore_index=True)\n",
    "    X = np.vstack(big_df[f\"emb_{name}\"].values)\n",
    "\n",
    "    X_store[name] = X\n",
    "    globals()[f\"X_{name}\"] = X\n",
    "\n",
    "    print(f\"{name:7s} → X_{name}.shape = {X.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2158,
     "status": "aborted",
     "timestamp": 1751552698657,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "hfBTkx1-LppK"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report,f1_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV , StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree        import DecisionTreeRegressor\n",
    "from sklearn.metrics     import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.tree        import DecisionTreeRegressor\n",
    "from sklearn.metrics     import mean_squared_error, mean_absolute_error, r2_score\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1751559951974,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "Ava47OwQ49HA"
   },
   "outputs": [],
   "source": [
    "# collect the return data on the publish day of news and convert to 1,0\n",
    "\n",
    "def label_news_df(df: pd.DataFrame, symbol: str) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if \"attributes.publishOn\" in df.columns:\n",
    "        df = df.rename(columns={\"attributes.publishOn\": \"publishOn\"})\n",
    "\n",
    "\n",
    "    df[\"publishOn\"] = (\n",
    "        pd.to_datetime(df[\"publishOn\"], utc=True, errors=\"coerce\")\n",
    "          .dt.tz_localize(None)\n",
    "    )\n",
    "    df = df.dropna(subset=[\"publishOn\"])\n",
    "\n",
    "    # 1) build price window\n",
    "    earliest = df[\"publishOn\"].dt.date.min()\n",
    "    latest   = df[\"publishOn\"].dt.date.max()\n",
    "    start_dt = (pd.Timestamp(earliest) - pd.Timedelta(days=7)).strftime(\"%Y-%m-%d\")\n",
    "    end_dt   = (pd.Timestamp(latest)   + pd.Timedelta(days=7)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    px = yf.download(symbol, start=start_dt, end=end_dt, interval=\"1d\")\n",
    "\n",
    "    # extract the Close price\n",
    "    closes = px[\"Close\"]\n",
    "    if isinstance(closes, pd.DataFrame):\n",
    "        closes = closes.squeeze()\n",
    "\n",
    "    # normalise to midnight dates\n",
    "    closes.index = closes.index.tz_localize(None).normalize()\n",
    "    trading_dates = closes.index\n",
    "\n",
    "    ret_vals, labels = [], []\n",
    "    for ts in df[\"publishOn\"]:\n",
    "        pub = pd.to_datetime(ts).tz_localize(None).normalize()\n",
    "\n",
    "        prev_days = trading_dates[trading_dates < pub]\n",
    "        next_days = trading_dates[trading_dates > pub]\n",
    "        if prev_days.empty or next_days.empty:\n",
    "            ret_vals.append(np.nan)\n",
    "            labels.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        d0, d1 = prev_days.max(), next_days.min()\n",
    "        p0, p1 = closes.at[d0], closes.at[d1]\n",
    "\n",
    "        if pd.isna(p0) or pd.isna(p1) or p0 == 0:\n",
    "            ret_vals.append(np.nan)\n",
    "            labels.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        r = p1 / p0 - 1\n",
    "        ret_vals.append(r)\n",
    "        labels.append(int(r > 0))\n",
    "\n",
    "    df[\"return_1d\"] = ret_vals\n",
    "    df[\"label\"]     = labels\n",
    "    df = df.dropna(subset=[\"label\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2226,
     "status": "aborted",
     "timestamp": 1751552698729,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "b2AepaoI2gR2"
   },
   "outputs": [],
   "source": [
    "# collect the embeddings\n",
    "labeled_dfs = {\n",
    "    sym: label_news_df(df, sym)\n",
    "    for sym, df in company_dfs.items()\n",
    "}\n",
    "\n",
    "buffers = {name: [] for name in variants}\n",
    "\n",
    "for sym, df in labeled_dfs.items():\n",
    "    texts = df[\"title\"].fillna(\"\").tolist()\n",
    "    for name, model in variants.items():\n",
    "        df[f\"emb_{name}\"] = list(\n",
    "            model.encode(texts, batch_size=32, convert_to_numpy=True)\n",
    "        )\n",
    "        buffers[name].append(df.assign(symbol=sym))\n",
    "\n",
    "X_store = {}\n",
    "y_vec   = None\n",
    "\n",
    "for name, parts in buffers.items():\n",
    "    big_df = pd.concat(parts, ignore_index=True)\n",
    "    X = np.vstack(big_df[f\"emb_{name}\"].values)\n",
    "    X_store[name] = X\n",
    "    globals()[f\"X_{name}\"] = X\n",
    "\n",
    "    if y_vec is None:\n",
    "        y_vec = big_df[\"label\"].astype(int).values\n",
    "\n",
    "    print(f\"{name:7s} → X_{name}.shape = {X.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2228,
     "status": "aborted",
     "timestamp": 1751552698733,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "akH3zfee8GJq"
   },
   "outputs": [],
   "source": [
    "# fit PCA on the mean embeddings\n",
    "pca = PCA(n_components=788)\n",
    "pca.fit(X_mean)\n",
    "explained = pca.explained_variance_ratio_\n",
    "cumulative = np.cumsum(explained)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(np.arange(1, len(cumulative)+1), cumulative, marker='o')\n",
    "plt.xlabel(\"Number of principal components\")\n",
    "plt.ylabel(\"Cumulative explained variance\")\n",
    "plt.title(\"PCA: Cumulative Explained Variance of News Embeddings\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2228,
     "status": "aborted",
     "timestamp": 1751552698735,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "Svrq2GnW93G-"
   },
   "outputs": [],
   "source": [
    "# fit PCA on the max embeddings\n",
    "pca = PCA(n_components=788)\n",
    "pca.fit(X_max)\n",
    "explained = pca.explained_variance_ratio_\n",
    "cumulative = np.cumsum(explained)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(np.arange(1, len(cumulative)+1), cumulative, marker='o')\n",
    "plt.xlabel(\"Number of principal components\")\n",
    "plt.ylabel(\"Cumulative explained variance\")\n",
    "plt.title(\"PCA: Cumulative Explained Variance of News Embeddings\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2227,
     "status": "aborted",
     "timestamp": 1751552698736,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "FEo5bZ969814"
   },
   "outputs": [],
   "source": [
    "# fit PCA on the concatenation embeddings\n",
    "pca = PCA(n_components=788)\n",
    "pca.fit(X_maxmean)\n",
    "explained = pca.explained_variance_ratio_\n",
    "cumulative = np.cumsum(explained)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(np.arange(1, len(cumulative)+1), cumulative, marker='o')\n",
    "plt.xlabel(\"Number of principal components\")\n",
    "plt.ylabel(\"Cumulative explained variance\")\n",
    "plt.title(\"PCA: Cumulative Explained Variance of News Embeddings\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2226,
     "status": "aborted",
     "timestamp": 1751552698737,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "YVDehxVO-GtE"
   },
   "outputs": [],
   "source": [
    "# fit PCA on the pseudo embeddings\n",
    "pca = PCA(n_components=788)\n",
    "pca.fit(X_pseudo)\n",
    "explained = pca.explained_variance_ratio_\n",
    "cumulative = np.cumsum(explained)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(np.arange(1, len(cumulative)+1), cumulative, marker='o')\n",
    "plt.xlabel(\"Number of principal components\")\n",
    "plt.ylabel(\"Cumulative explained variance\")\n",
    "plt.title(\"PCA: Cumulative Explained Variance of News Embeddings\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2225,
     "status": "aborted",
     "timestamp": 1751552698738,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "BAATP8QZAEI-"
   },
   "outputs": [],
   "source": [
    "# fit logistic regression on 100 PC of mean embeddings\n",
    "pca = PCA(n_components=100, random_state=42)\n",
    "X_mean_pca = pca.fit_transform(X_mean)\n",
    "\n",
    "X_train_pca, X_test_pca, y_train, y_test = train_test_split(\n",
    "    X_mean_pca, y_vec,\n",
    "    test_size=0.2,\n",
    "    random_state=42)\n",
    "\n",
    "logit = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logit.fit(X_train_pca, y_train)\n",
    "\n",
    "y_pred = logit.predict(X_test_pca)\n",
    "\n",
    "acc      = accuracy_score(y_test, y_pred)\n",
    "micro_f1 = f1_score(y_test, y_pred, average=\"micro\")\n",
    "macro_f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "w_f1     = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "mac_rec  = recall_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "print(f\"Accuracy:         {acc:.4f}\")\n",
    "print(f\"Micro F1:         {micro_f1:.4f}\")\n",
    "print(f\"Macro F1:         {macro_f1:.4f}\")\n",
    "print(f\"Weighted F1:      {w_f1:.4f}\")\n",
    "print(f\"Macro Recall:     {mac_rec:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2248,
     "status": "aborted",
     "timestamp": 1751552698763,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "tdqs7ONID0CP"
   },
   "outputs": [],
   "source": [
    "# fit logistic regression on 100 PC of max embeddings\n",
    "pca_max = PCA(n_components=100, random_state=42)\n",
    "X_max_pca = pca_max.fit_transform(X_max)\n",
    "\n",
    "X_train_max, X_test_max, y_train, y_test = train_test_split(\n",
    "    X_max_pca, y_vec,\n",
    "    test_size=0.2,\n",
    "    random_state=42)\n",
    "\n",
    "logit_max = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logit_max.fit(X_train_max, y_train)\n",
    "\n",
    "y_pred_max = logit_max.predict(X_test_max)\n",
    "\n",
    "acc_max      = accuracy_score(y_test, y_pred_max)\n",
    "micro_f1_max = f1_score(y_test, y_pred_max, average=\"micro\")\n",
    "macro_f1_max = f1_score(y_test, y_pred_max, average=\"macro\")\n",
    "w_f1_max     = f1_score(y_test, y_pred_max, average=\"weighted\")\n",
    "mac_rec_max  = recall_score(y_test, y_pred_max, average=\"macro\")\n",
    "\n",
    "print(f\"Accuracy:         {acc_max:.4f}\")\n",
    "print(f\"Micro F1:         {micro_f1_max:.4f}\")\n",
    "print(f\"Macro F1:         {macro_f1_max:.4f}\")\n",
    "print(f\"Weighted F1:      {w_f1_max:.4f}\")\n",
    "print(f\"Macro Recall:     {mac_rec_max:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2247,
     "status": "aborted",
     "timestamp": 1751552698764,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "ccyM_EgpBZ7Q"
   },
   "outputs": [],
   "source": [
    "# fit logistic regression on 100 PC of concatenation embeddings\n",
    "pca_maxmean = PCA(n_components=100, random_state=42)\n",
    "X_maxmean_pca = pca_maxmean.fit_transform(X_maxmean)\n",
    "X_train_maxmean, X_test_maxmean, y_train, y_test = train_test_split(\n",
    "    X_maxmean_pca, y_vec,\n",
    "    test_size=0.2,\n",
    "    random_state=42)\n",
    "\n",
    "\n",
    "logit_maxmean = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logit_maxmean.fit(X_train_maxmean, y_train)\n",
    "\n",
    "y_pred_maxmean = logit_maxmean.predict(X_test_maxmean)\n",
    "\n",
    "acc_maxmean      = accuracy_score(y_test, y_pred_maxmean)\n",
    "micro_f1_maxmean = f1_score(y_test, y_pred_maxmean, average=\"micro\")\n",
    "macro_f1_maxmean = f1_score(y_test, y_pred_maxmean, average=\"macro\")\n",
    "w_f1_maxmean     = f1_score(y_test, y_pred_maxmean, average=\"weighted\")\n",
    "mac_rec_maxmean  = recall_score(y_test, y_pred_maxmean, average=\"macro\")\n",
    "\n",
    "print(f\"Accuracy:         {acc_maxmean:.4f}\")\n",
    "print(f\"Micro F1:         {micro_f1_maxmean:.4f}\")\n",
    "print(f\"Macro F1:         {macro_f1_maxmean:.4f}\")\n",
    "print(f\"Weighted F1:      {w_f1_maxmean:.4f}\")\n",
    "print(f\"Macro Recall:     {mac_rec_maxmean:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2247,
     "status": "aborted",
     "timestamp": 1751552698765,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "XS71YUJxGexo"
   },
   "outputs": [],
   "source": [
    "# fit logistic regression on 100 PC of pseudo embeddings\n",
    "pca_pseudo = PCA(n_components=100, random_state=42)\n",
    "X_pseudo_pca = pca_pseudo.fit_transform(X_pseudo)\n",
    "\n",
    "X_train_pseudo, X_test_pseudo, y_train, y_test = train_test_split(\n",
    "    X_pseudo_pca, y_vec,\n",
    "    test_size=0.2,\n",
    "    random_state=42)\n",
    "\n",
    "logit_pseudo = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logit_pseudo.fit(X_train_pseudo, y_train)\n",
    "\n",
    "y_pred_pseudo = logit_pseudo.predict(X_test_pseudo)\n",
    "\n",
    "acc_pseudo      = accuracy_score(y_test, y_pred_pseudo)\n",
    "micro_f1_pseudo = f1_score(y_test, y_pred_pseudo, average=\"micro\")\n",
    "macro_f1_pseudo = f1_score(y_test, y_pred_pseudo, average=\"macro\")\n",
    "w_f1_pseudo     = f1_score(y_test, y_pred_pseudo, average=\"weighted\")\n",
    "mac_rec_pseudo  = recall_score(y_test, y_pred_pseudo, average=\"macro\")\n",
    "print(f\"Accuracy:         {acc_pseudo:.4f}\")\n",
    "print(f\"Micro F1:         {micro_f1_pseudo:.4f}\")\n",
    "print(f\"Macro F1:         {macro_f1_pseudo:.4f}\")\n",
    "print(f\"Weighted F1:      {w_f1_pseudo:.4f}\")\n",
    "print(f\"Macro Recall:     {mac_rec_pseudo:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Be69MDcObJr"
   },
   "source": [
    "In the next part, I will use paraphrase-MiniLM-L6-v2 to construct the embeddings, and use such embeddings to classify the stock price movements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2246,
     "status": "aborted",
     "timestamp": 1751552698766,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "yKqJ8x3mOFn4"
   },
   "outputs": [],
   "source": [
    "# Try paraphrase-MiniLM-L6-v2 to do embedding\n",
    "backbone     = \"sentence-transformers/paraphrase-MiniLM-L6-v2\"\n",
    "transformer  = Transformer(backbone, max_seq_length=128)\n",
    "dim          = transformer.get_word_embedding_dimension()\n",
    "\n",
    "mean_pool    = Pooling(dim, pooling_mode_mean_tokens=True)\n",
    "max_pool     = Pooling(dim, pooling_mode_mean_tokens=False, pooling_mode_max_tokens=True)\n",
    "maxmean_pool = Pooling(dim, pooling_mode_mean_tokens=True,  pooling_mode_max_tokens=True)\n",
    "pseudo_pool  = Pooling(dim, pooling_mode_mean_tokens=False, pooling_mode_max_tokens=False,\n",
    "                       pooling_mode_cls_token=True)\n",
    "\n",
    "model_mean    = SentenceTransformer(modules=[transformer, mean_pool],    device=\"cuda\")\n",
    "model_max     = SentenceTransformer(modules=[transformer, max_pool],     device=\"cuda\")\n",
    "model_maxmean = SentenceTransformer(modules=[transformer, maxmean_pool], device=\"cuda\")\n",
    "model_pseudo  = SentenceTransformer(modules=[transformer, pseudo_pool],  device=\"cuda\")\n",
    "\n",
    "variants = {\n",
    "    \"mean\":    model_mean,\n",
    "    \"max\":     model_max,\n",
    "    \"maxmean\": model_maxmean,\n",
    "    \"pseudo\":  model_pseudo,\n",
    "}\n",
    "\n",
    "buffers = {name: [] for name in variants}\n",
    "\n",
    "for sym, df in company_dfs.items():\n",
    "    if \"attributes.publishOn\" in df.columns:\n",
    "        df = df.rename(columns={\"attributes.publishOn\": \"publishOn\"})\n",
    "\n",
    "    texts = df[\"title\"].fillna(\"\").tolist()\n",
    "\n",
    "    for name, model in variants.items():\n",
    "        emb = model.encode(texts, batch_size=32, convert_to_numpy=True)\n",
    "        print(f\"{name:7s} → {emb.shape[0]} samples × {emb.shape[1]}-dim embeddings\")\n",
    "        df[f\"emb_{name}\"] = list(emb)\n",
    "\n",
    "\n",
    "        buffers[name].append(df.assign(symbol=sym))\n",
    "\n",
    "X_store = {}\n",
    "\n",
    "for name, parts in buffers.items():\n",
    "    big_df = pd.concat(parts, ignore_index=True)\n",
    "    X = np.vstack(big_df[f\"emb_{name}\"].values)\n",
    "    X_store[name + \"_1\"] = X\n",
    "    globals()[f\"X_{name}_1\"] = X\n",
    "\n",
    "    print(f\"{name:7s} → X_{name}_1.shape = {X.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2246,
     "status": "aborted",
     "timestamp": 1751552698767,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "1YSL37ADTN01"
   },
   "outputs": [],
   "source": [
    "# solve out mean, max, concatenation and pseudo results\n",
    "\n",
    "variants = {\n",
    "    \"mean\":    model_mean,\n",
    "    \"max\":     model_max,\n",
    "    \"maxmean\": model_maxmean,\n",
    "    \"pseudo\":  model_pseudo,\n",
    "}\n",
    "\n",
    "buffers1 = {name: [] for name in variants}\n",
    "\n",
    "for sym, df in labeled_dfs.items():\n",
    "    texts = df[\"title\"].fillna(\"\").tolist()\n",
    "    for name, model in variants.items():\n",
    "        emb = model.encode(texts, batch_size=32, convert_to_numpy=True)\n",
    "        df1 = df.copy()\n",
    "        df1[f\"emb_{name}_1\"] = list(emb)\n",
    "        buffers1[name].append(df1.assign(symbol=sym))\n",
    "\n",
    "\n",
    "X_store_1 = {}\n",
    "y_vec_1   = None\n",
    "\n",
    "for name, parts in buffers1.items():\n",
    "    big_df = pd.concat(parts, ignore_index=True)\n",
    "    X1 = np.vstack(big_df[f\"emb_{name}_1\"].values)\n",
    "\n",
    "    X_store_1[name + \"_1\"] = X1\n",
    "    globals()[f\"X_{name}_1\"] = X1\n",
    "\n",
    "    if y_vec_1 is None:\n",
    "        y_vec_1 = big_df[\"label\"].astype(int).values\n",
    "\n",
    "    print(f\"{name:7s}_1 → X_{name}_1.shape = {X1.shape}\")\n",
    "\n",
    "for name in variants:\n",
    "    assert X_store_1[name + \"_1\"].shape[0] == len(y_vec_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2244,
     "status": "aborted",
     "timestamp": 1751552698767,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "j8vcYHswUoXC"
   },
   "outputs": [],
   "source": [
    "# do PCA on mean pooling embedding and fit logistic regression\n",
    "pca_mean_1 = PCA(n_components=100, random_state=42)\n",
    "X_mean_1_pca = pca_mean_1.fit_transform(X_mean_1)\n",
    "\n",
    "X_train_mean_1, X_test_mean_1, y_train_1, y_test_1 = train_test_split(\n",
    "    X_mean_1_pca, y_vec_1,\n",
    "    test_size=0.2,\n",
    "    random_state=42)\n",
    "\n",
    "logit_mean_1 = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logit_mean_1.fit(X_train_mean_1, y_train_1)\n",
    "y_pred_mean_1 = logit_mean_1.predict(X_test_mean_1)\n",
    "\n",
    "acc_mean_1      = accuracy_score(y_test_1, y_pred_mean_1)\n",
    "micro_f1_mean_1  = f1_score(y_test_1, y_pred_mean_1, average=\"micro\")\n",
    "macro_f1_mean_1  = f1_score(y_test_1, y_pred_mean_1, average=\"macro\")\n",
    "w_f1_mean_1      = f1_score(y_test_1, y_pred_mean_1, average=\"weighted\")\n",
    "mac_rec_mean_1   = recall_score(y_test_1, y_pred_mean_1, average=\"macro\")\n",
    "\n",
    "print(f\"Accuracy:         {acc_mean_1 :.4f}\")\n",
    "print(f\"Micro F1:         {micro_f1_mean_1 :.4f}\")\n",
    "print(f\"Macro F1:         {macro_f1_mean_1 :.4f}\")\n",
    "print(f\"Weighted F1:      {w_f1_mean_1 :.4f}\")\n",
    "print(f\"Macro Recall:     {mac_rec_mean_1 :.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2244,
     "status": "aborted",
     "timestamp": 1751552698768,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "bRuEO-fAYa8h"
   },
   "outputs": [],
   "source": [
    "# do PCA on max pooling embedding and fit logistic regression\n",
    "pca_max_1 = PCA(n_components=100, random_state=42)\n",
    "X_max_1_pca = pca_max_1.fit_transform(X_max_1)\n",
    "\n",
    "X_train_max_1, X_test_max_1, y_train_1, y_test_1 = train_test_split(\n",
    "    X_max_1_pca, y_vec_1,\n",
    "    test_size=0.2,\n",
    "    random_state=42)\n",
    "\n",
    "\n",
    "logit_max_1 = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logit_max_1.fit(X_train_max_1, y_train_1)\n",
    "y_pred_max_1 = logit_max_1.predict(X_test_max_1)\n",
    "\n",
    "acc_max_1      = accuracy_score(y_test_1, y_pred_max_1)\n",
    "micro_f1_max_1 = f1_score(y_test_1, y_pred_max_1, average=\"micro\")\n",
    "macro_f1_max_1 = f1_score(y_test_1, y_pred_max_1, average=\"macro\")\n",
    "w_f1_max_1     = f1_score(y_test_1, y_pred_max_1, average=\"weighted\")\n",
    "mac_rec_max_1  = recall_score(y_test_1, y_pred_max_1, average=\"macro\")\n",
    "print(f\"Accuracy:         {acc_max_1:.4f}\")\n",
    "print(f\"Micro F1:         {micro_f1_max_1:.4f}\")\n",
    "print(f\"Macro F1:         {macro_f1_max_1:.4f}\")\n",
    "print(f\"Weighted F1:      {w_f1_max_1:.4f}\")\n",
    "print(f\"Macro Recall:     {mac_rec_max_1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2244,
     "status": "aborted",
     "timestamp": 1751552698769,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "WX5RxjTrYrm8"
   },
   "outputs": [],
   "source": [
    "# do PCA on concatenation embedding and fit logistic regression\n",
    "pca_maxmean_1 = PCA(n_components=100, random_state=42)\n",
    "X_maxmean_1_pca = pca_maxmean_1.fit_transform(X_maxmean_1)\n",
    "\n",
    "X_train_maxmean_1, X_test_maxmean_1 = train_test_split(\n",
    "    X_maxmean_1_pca,test_size=0.2,random_state=42)\n",
    "\n",
    "logit_maxmean_1 = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logit_maxmean_1.fit(X_train_maxmean_1, y_train_1)\n",
    "\n",
    "y_pred_maxmean_1 = logit_maxmean_1.predict(X_test_maxmean_1)\n",
    "\n",
    "acc_maxmean_1      = accuracy_score(y_test_1, y_pred_maxmean_1)\n",
    "micro_f1_maxmean_1 = f1_score(y_test_1, y_pred_maxmean_1, average=\"micro\")\n",
    "macro_f1_maxmean_1 = f1_score(y_test_1, y_pred_maxmean_1, average=\"macro\")\n",
    "w_f1_maxmean_1     = f1_score(y_test_1, y_pred_maxmean_1, average=\"weighted\")\n",
    "mac_rec_maxmean_1  = recall_score(y_test_1, y_pred_maxmean_1, average=\"macro\")\n",
    "\n",
    "print(f\"Accuracy:         {acc_maxmean_1:.4f}\")\n",
    "print(f\"Micro F1:         {micro_f1_maxmean_1:.4f}\")\n",
    "print(f\"Macro F1:         {macro_f1_maxmean_1:.4f}\")\n",
    "print(f\"Weighted F1:      {w_f1_maxmean_1:.4f}\")\n",
    "print(f\"Macro Recall:     {mac_rec_maxmean_1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2243,
     "status": "aborted",
     "timestamp": 1751552698769,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "bHakvc38ZB9r"
   },
   "outputs": [],
   "source": [
    "# do PCA on pseudo pooling embedding and fit logistic regression\n",
    "pca_pseudo_1 = PCA(n_components=100, random_state=42)\n",
    "X_pseudo_1_pca = pca_pseudo_1.fit_transform(X_pseudo_1)\n",
    "\n",
    "X_train_pseudo_1, X_test_pseudo_1= train_test_split(\n",
    "    X_pseudo_1_pca,\n",
    "    test_size=0.2,\n",
    "    random_state=42)\n",
    "\n",
    "logit_pseudo_1 = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logit_pseudo_1.fit(X_train_pseudo_1, y_train_1)\n",
    "\n",
    "y_pred_pseudo_1 = logit_pseudo_1.predict(X_test_pseudo_1)\n",
    "\n",
    "acc_pseudo_1      = accuracy_score(y_test_1, y_pred_pseudo_1)\n",
    "micro_f1_pseudo_1 = f1_score(y_test_1, y_pred_pseudo_1, average=\"micro\")\n",
    "macro_f1_pseudo_1 = f1_score(y_test_1, y_pred_pseudo_1, average=\"macro\")\n",
    "w_f1_pseudo_1     = f1_score(y_test_1, y_pred_pseudo_1, average=\"weighted\")\n",
    "mac_rec_pseudo_1  = recall_score(y_test_1, y_pred_pseudo_1, average=\"macro\")\n",
    "print(f\"Accuracy:         {acc_pseudo_1:.4f}\")\n",
    "print(f\"Micro F1:         {micro_f1_pseudo_1:.4f}\")\n",
    "print(f\"Macro F1:         {macro_f1_pseudo_1:.4f}\")\n",
    "print(f\"Weighted F1:      {w_f1_pseudo_1:.4f}\")\n",
    "print(f\"Macro Recall:     {mac_rec_pseudo_1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2243,
     "status": "aborted",
     "timestamp": 1751552698770,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "IvZHuiIzsALU"
   },
   "outputs": [],
   "source": [
    "# Try open ai family LLM\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "def openai_embed(texts,\n",
    "                 model=\"text-embedding-ada-002\",\n",
    "                 batch_size=32):\n",
    "    all_emb = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        resp = openai.embeddings.create(model=model, input=batch)\n",
    "        emb = [d.embedding for d in resp.data]\n",
    "        all_emb.extend(emb)\n",
    "    return np.array(all_emb, dtype=np.float32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2242,
     "status": "aborted",
     "timestamp": 1751552698770,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "U1dZHcka3sDW"
   },
   "outputs": [],
   "source": [
    "# Solve out embedding\n",
    "emb_buffers = []\n",
    "for sym, df in company_dfs.items():\n",
    "    df = df.copy()\n",
    "    texts = df[\"title\"].fillna(\"\").tolist()\n",
    "\n",
    "    emb = openai_embed(texts)\n",
    "    df[\"emb_ada_2\"] = list(emb)\n",
    "    emb_buffers.append(df.assign(symbol=sym))\n",
    "\n",
    "big_df   = pd.concat(emb_buffers, ignore_index=True)\n",
    "X_ada_2  = np.vstack(big_df[\"emb_ada_2\"].values)\n",
    "globals()[\"X_ada_2\"] = X_ada_2\n",
    "print(\"ADA         → X_ada_2.shape =\", X_ada_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2242,
     "status": "aborted",
     "timestamp": 1751552698771,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "lNELFun6Aid7"
   },
   "outputs": [],
   "source": [
    "labeled_dfs = {\n",
    "    sym: label_news_df(df, sym)\n",
    "    for sym, df in company_dfs.items()\n",
    "}\n",
    "\n",
    "emb_buffers = []\n",
    "for sym, df in company_dfs.items():\n",
    "    df_lab = label_news_df(df, sym)\n",
    "    if df_lab.empty:\n",
    "        continue\n",
    "\n",
    "    texts = df_lab[\"title\"].fillna(\"\").tolist()\n",
    "    emb   = openai_embed(texts)\n",
    "\n",
    "    df_lab[\"emb_ada_2\"] = list(emb)\n",
    "    emb_buffers.append(df_lab.assign(symbol=sym))\n",
    "\n",
    "big_df   = pd.concat(emb_buffers, ignore_index=True)\n",
    "X_ada_2  = np.vstack(big_df[\"emb_ada_2\"].values)\n",
    "y_vec_2  = big_df[\"label\"].astype(int).values\n",
    "\n",
    "print(\"X_ada_2.shape =\", X_ada_2.shape)\n",
    "print(\"y_vec_2.shape =\", y_vec_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2240,
     "status": "aborted",
     "timestamp": 1751552698771,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "0gW-shsW6nPs"
   },
   "outputs": [],
   "source": [
    "# do PCA based on embedding and fit logistic regression\n",
    "pca_ada    = PCA(n_components=100, random_state=42)\n",
    "X_ada_pca  = pca_ada.fit_transform(X_ada_2)\n",
    "\n",
    "X_train_ada, X_test_ada, y_train_ada, y_test_ada = train_test_split(\n",
    "    X_ada_pca,y_vec_2,test_size=0.2,random_state=42)\n",
    "\n",
    "logit_ada = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logit_ada.fit(X_train_ada, y_train_ada)\n",
    "\n",
    "\n",
    "y_pred_ada = logit_ada.predict(X_test_ada)\n",
    "\n",
    "acc_ada         = accuracy_score(y_test_ada, y_pred_ada)\n",
    "micro_f1_ada    = f1_score(y_test_ada, y_pred_ada, average=\"micro\")\n",
    "macro_f1_ada    = f1_score(y_test_ada, y_pred_ada, average=\"macro\")\n",
    "weighted_f1_ada = f1_score(y_test_ada, y_pred_ada, average=\"weighted\")\n",
    "macro_rec_ada   = recall_score(y_test_ada, y_pred_ada, average=\"macro\")\n",
    "\n",
    "print(f\"Accuracy:      {acc_ada:.4f}\")\n",
    "print(f\"Micro F1:      {micro_f1_ada:.4f}\")\n",
    "print(f\"Macro F1:      {macro_f1_ada:.4f}\")\n",
    "print(f\"Weighted F1:   {weighted_f1_ada:.4f}\")\n",
    "print(f\"Macro Recall:  {macro_rec_ada:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "7df834e2b8f347c7b7ebe67beb16b700",
      "3b0e5ba1c0fb4d319fc86de6008787cb",
      "cc2becacc4db40d0a31677783b40cc0c",
      "e71d8076143847c88ae5264827695249",
      "61f76e7fac5d49df9182adab45e4c32a",
      "5657353a6d264ee0b538740712cf4b61",
      "80238423f7634f08bdee6f01ed146aa3",
      "37d64e7fe8064f2e9ef833c4c3cbac90",
      "e061f6e35371443ea30ec080dff6b9d5",
      "2483f0ca19dd470c9fc371d6a4c66fdd",
      "6a980ad893704110a5a47f4d90dd078a",
      "eb0554294a1940c991a1c72cee97a309",
      "f9331e41cd5047ff97293da99bb7f269",
      "e877ff5b830e48bd8e442092f27ace57",
      "e3739fa919f54c73867c40b5d74831fc",
      "0c0db223a7314055a4cd455581073aef",
      "e27aaea389e84750a0330731eb74af3b",
      "af3c45d120cd4f6297bbb28a78ddb6c6",
      "568e6271805a48e6a7e70a7bccd4ea96",
      "9487252ba40240caa4a25dde7f75e04c"
     ]
    },
    "executionInfo": {
     "elapsed": 65,
     "status": "ok",
     "timestamp": 1751558822645,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "Yzd92y28Q29L",
    "outputId": "5ea48ba3-6f68-4676-8445-7d9cf9485c25"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7df834e2b8f347c7b7ebe67beb16b700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(new_session=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401,
     "referenced_widgets": [
      "e49ff761eedd4542a06bd64c3e87ff0d",
      "0aa9b73c6ae846eba623c7760064ac71",
      "93cd69ab37354bda87faa427f8483f90",
      "0d3e782029c94c778921f5a9a04bc275",
      "c91680af47d84dc3991f948ec4492d0e",
      "9aa6d7d6ffc240a88971117423532c24",
      "6e197ac3e71a4e0d90b30cced700a8ca",
      "df3489fccff8488dbe3aae0f4f632822",
      "54a7ab5cb2684a16be0df5072ec0ee6c",
      "285888c2031645049734649aa07cac9c",
      "380e48b353dd476a90ce50bd6981149d",
      "b52b30d8e3744f14ada12bb3249b80ac",
      "c606a072073d42ce9ce2129fafcbe414",
      "619bcbf858e34b9f857ab8114c318f5e",
      "3f8da254ba9c411a853516380b33353c",
      "ae0ea54a9cca4377ad71c90b44a2f429",
      "402cc13c49ed4c8d902c3b0b8d3a7a4b",
      "7820018f12f1423dad512c132c84866e",
      "085a3b4e690d4b6aac0255965bdb9bd7",
      "ef45ccb786f8479c916854f422ff0afd",
      "0692e98c5c1c4b0fa9e614c92e351cf3",
      "8e7b0c9c4d564036a96a399e238a1f3c",
      "c3681156d02a4039a052da7e28bfb9a4",
      "96b2887ede0c486188a8d938f0108c80",
      "af27bc7257ee4c2d9ca12ab6c011c5ce",
      "a3a60a35c8b645ad8db4aab78fd71247",
      "70227a66d99c4210a80da1a711039862",
      "0cc4da06a76e4c2082c4430c25eb34bb",
      "3808a4ca3da34679ab76e8b0698dc3d9",
      "95691009658e4a7780b18fc633196cee",
      "053de57ee5ad4623bad03add3f6c09ee",
      "42f7a44fdfd443db920e8d426f6b2043",
      "280fa831ae6b4c7f8e422bc9e13e238e",
      "b653a7b459294f2e99296270439b894c",
      "11c04f1b43c04a759d97fd0b14602b48",
      "e73dba02a2344cc09dde47b88c806277",
      "df91539f1dda47da948f82643b8bf078",
      "042fcceb333b4d8c92a38fbeefd6ea8b",
      "5c98e31906ed4d3cb75534af4e5de8c9",
      "0b9352afacab4faca6eb99bfcd74f42a",
      "903eab5ddf204d499fbee1c7371f524a",
      "5f68ca188d9e44d8a83f2e55a616f0ec",
      "774aa9ee84ba489f865784fcbe77f656",
      "d32929004b33431ebd1d9503795fed5a",
      "1aa6593c97ad4b4784187bf34c9db4b9",
      "8fee73575b2c4875a098311abd6acd5d",
      "5fb36663c52e4a4081c5cb84db4d6b19",
      "61349bd3b0f9466db58840cd7c1e3a45",
      "40f4497760934ffead7749e56879b210",
      "e2415c0f0b674dd187003dc38712a124",
      "254378359e42437abf02f4db5f8a1784",
      "dc99c4568d8549eeb55259097f1438d7",
      "6bb2daa12a41473382cedfd51b182904",
      "0fcf2f3eb8f749f690ea09658e557cb8",
      "da4ac6cb4e0d4735a697a5d906d5dbc3",
      "be902839196d4913993281c22d820771",
      "a4938a4c09174629a4502457a1d4121b",
      "436e54ce0f254bbaaf35d72da2c500a1",
      "aacc675f7b5e4eaba41c9bb70e8e4b41",
      "ed3c6b90945b4e639e7703024ab53538",
      "f55fe1b732f44353a28d11dea3c1fcab",
      "34a53c51de514c838c893642442a0bfa",
      "f490e278104c470aa212e66b24d6bcba",
      "aa8adccc39e34ab6a4316ccc7c67b5fb",
      "b49cbc867df94c02a3703c915265f8be",
      "e48f9288d70445f5a92705dde451cc05",
      "f563dfa290a04647a80544dfe1e236c2",
      "590bf5a396864fd785bf7da92dd728e4",
      "7e60a10c00b240aab68f6562d8813a8b",
      "f7179de1b83e40d38b381bfafcb79051",
      "103a195611ea4f4e876b0a801721ca0a",
      "0a58db9f8d3c476b82ecdd5fd9e5a307",
      "c2ad85962f664e24b2c32da28574c701",
      "fc10996f29db4b5991456cc45a5ca3c7",
      "6ffa312e8b6840e990fd7e0d32c1b888",
      "9a380058130b4a3ba5959e896ff1ad26",
      "5548060113034f308b0127d155a3982a",
      "116561e567cd427e965a5b3fad4fb516",
      "71493db096f449118563b42d884a3d6c",
      "abd8112edb1047758ab731104ed32b8a",
      "e059b0d824634421b9abc61c9c1a0e84",
      "d7933861b52e422aa640921bb9167dcf",
      "9a8d5a0b64184ff6bb4e7832ad9fba56",
      "dea13731eaf24f638fa17eea8c2d57e2",
      "461132ea37b94936b934493779036593",
      "683f9cd7a9b24f2d95b5bbc3825ead60",
      "4eee92d5ba4f46dea1c664e60ef247da",
      "97a3964c7f6a4f21a0f825197174a1b1",
      "1846b5e724944078ba75d5f080b8a4f8",
      "1996ddc9f62d4ee39a440eec71cf60cb",
      "3d0156f3450647499d90599be1c10eaa",
      "6ad7bfdd2ee1407a96650906be93f75a",
      "c9acae7117c34e548f1d10b0a6f2b2b9",
      "26ec319395d34abc97f7654a13e74e3b",
      "50d4ea3da41b4b66bccf8c7ee12137b3",
      "88e9bcc9e23a432fac769aa372e6f49f",
      "c5c439c5ea914d3ba22c9c037cce46dc",
      "752a9548e29345d0bfb8ed7069589e61",
      "36c4155ea2bc41dbbb84c9f35b560b77",
      "10a0c3cbf6b1475c86af1ac9a96598c9",
      "c8038471eeba4bbbb40deecceb1fa940",
      "15ae9aaf5abd407aa155b3028d857103",
      "ca4362037f18465595d3ebcea1dd7ec4",
      "18a0af47f24342729dde9f42a2122c40",
      "223dac1331534f96a547f897194d2395",
      "477fe6736f9f43dc91846f47e2f82409",
      "2ec207ffa7c84da09ef61b2b23d7294b",
      "ee079c7d7bcc4177b88d55472f92ebcb",
      "3780053c3ea941dfade0c805014f1669",
      "bc1318d56ef940a792d601282519686c",
      "a0d6df1e16754001935449dd626decf1",
      "ef1e0888cac5493aa8a5916ef47634e8",
      "da91033cb5bb41bca778f5a613b4b11d",
      "3d27d68953064f269c1e27137e224271",
      "d4c43885741a4d2889038986dede910a",
      "027f69fa85c346bc84b0f00edf96e484",
      "dacae7d34bb7428aacedbc6680bd83ca",
      "4bf6795b6cc44999a4e34a026710d80e",
      "1cf3ecba1a8b4f5f88c584ed0f01cb22",
      "cff3913ce6ac4d18bd68c5f98449eb3f",
      "977a12dcd8a64c56807b1f35c2c192d9",
      "dc306ffa3515498b9344b19b53ea0f83",
      "8c75304d621d4e3d9843e3e623af7130",
      "fa34435324c645a19a20c38d98add093",
      "8889602fb38d4474a41b46292e394ecc",
      "d4b9c27d037342029f39a1892cc80f66",
      "f9bb4b7025ec4f6f809cc5936aef732b",
      "b4013f249df44cf7a061dd0af9c1ffc7",
      "192d258dd6f4441dbe9c2349326d8950",
      "025b7339c04f49e39472b0f64a1af426",
      "2b1dc5e0b99644fe88f2042bf8929792",
      "05e0ae7ec16d4e508341320bf474394e"
     ]
    },
    "executionInfo": {
     "elapsed": 131487,
     "status": "ok",
     "timestamp": 1751558962600,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "Z4zigSrnUSme",
    "outputId": "a2b6b727-415c-4eb3-d0ef-4db4d1da6714"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e49ff761eedd4542a06bd64c3e87ff0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b52b30d8e3744f14ada12bb3249b80ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3681156d02a4039a052da7e28bfb9a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b653a7b459294f2e99296270439b894c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa6593c97ad4b4784187bf34c9db4b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be902839196d4913993281c22d820771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f563dfa290a04647a80544dfe1e236c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "116561e567cd427e965a5b3fad4fb516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1846b5e724944078ba75d5f080b8a4f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10a0c3cbf6b1475c86af1ac9a96598c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0d6df1e16754001935449dd626decf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc306ffa3515498b9344b19b53ea0f83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Try Llama-3.1-8B-Instruct to do embedding\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1751560146649,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "1J00pDQifZEz"
   },
   "outputs": [],
   "source": [
    "# solve out mean, max, concatenation and pseudo results\n",
    "model.config.output_hidden_states = True\n",
    "tokenizer.pad_token    = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "\n",
    "def encode_texts(texts, pool=\"mean\", batch_size=16, max_length=128):\n",
    "    all_embs = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            out = model(\n",
    "                **inputs,\n",
    "                use_cache=False,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "\n",
    "        hs   = out.hidden_states[-1]\n",
    "        mask = inputs.attention_mask.bool().unsqueeze(-1)\n",
    "        emb  = pools[pool](hs, mask)\n",
    "\n",
    "        all_embs.append(emb.cpu().numpy())\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return np.vstack(all_embs)\n",
    "\n",
    "\n",
    "def pool_mean(hs, mask):\n",
    "    s = (hs * mask).sum(1)\n",
    "    c = mask.sum(1).clamp(min=1)\n",
    "    return s / c\n",
    "\n",
    "def pool_max(hs, mask):\n",
    "    neg_inf = torch.finfo(hs.dtype).min\n",
    "    return hs.masked_fill(~mask, neg_inf).max(1).values\n",
    "\n",
    "def pool_meanmax(hs, mask):\n",
    "    return torch.cat([pool_mean(hs, mask), pool_max(hs, mask)], dim=1)\n",
    "\n",
    "def pool_cls(hs, mask):\n",
    "    return hs[:, 0, :]\n",
    "\n",
    "pools = {\n",
    "    \"mean\":    pool_mean,\n",
    "    \"max\":     pool_max,\n",
    "    \"meanmax\": pool_meanmax,\n",
    "    \"pseudo\":  pool_cls,\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2560358,
     "status": "ok",
     "timestamp": 1751562709044,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "Q9F_VWNYdeIv",
    "outputId": "816d6fba-c030-4974-854a-74813e30a718"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-9-2069372175.py:21: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  px = yf.download(symbol, start=start_dt, end=end_dt, interval=\"1d\")\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing symbol 'JPM': 200 articles\n",
      "  → pooling mean  ..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " done\n",
      "  → pooling max   ... done\n",
      "  → pooling meanmax... done\n",
      "  → pooling pseudo... done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-9-2069372175.py:21: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  px = yf.download(symbol, start=start_dt, end=end_dt, interval=\"1d\")\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing symbol 'BAC': 200 articles\n",
      "  → pooling mean  ..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " done\n",
      "  → pooling max   ... done\n",
      "  → pooling meanmax... done\n",
      "  → pooling pseudo... done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-9-2069372175.py:21: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  px = yf.download(symbol, start=start_dt, end=end_dt, interval=\"1d\")\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing symbol 'WFC': 200 articles\n",
      "  → pooling mean  ..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " done\n",
      "  → pooling max   ... done\n",
      "  → pooling meanmax... done\n",
      "  → pooling pseudo... done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-9-2069372175.py:21: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  px = yf.download(symbol, start=start_dt, end=end_dt, interval=\"1d\")\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing symbol 'RY': 200 articles\n",
      "  → pooling mean  ..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " done\n",
      "  → pooling max   ... done\n",
      "  → pooling meanmax... done\n",
      "  → pooling pseudo... done\n",
      "\n",
      "Concatenating embeddings for pool='mean' (4 symbols)... done → shape (800, 4096)\n",
      "\n",
      "Concatenating embeddings for pool='max' (4 symbols)... done → shape (800, 4096)\n",
      "\n",
      "Concatenating embeddings for pool='meanmax' (4 symbols)... done → shape (800, 8192)\n",
      "\n",
      "Concatenating embeddings for pool='pseudo' (4 symbols)... done → shape (800, 4096)\n",
      "\n",
      "Final y_vec_3d.shape = (800,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "buffers = {name: [] for name in pools}\n",
    "y_vec_3d = None\n",
    "\n",
    "for sym, raw_df in company_dfs.items():\n",
    "    df_lab = label_news_df(raw_df, sym)\n",
    "    if df_lab.empty:\n",
    "        print(f\"[{sym}] no data after labeling, skipping\")\n",
    "        continue\n",
    "\n",
    "    texts = df_lab[\"title\"].fillna(\"\").tolist()\n",
    "    print(f\"\\nProcessing symbol {sym!r}: {len(texts)} articles\")\n",
    "\n",
    "    for name in pools:\n",
    "        print(f\"  → pooling {name:6s}...\", end=\"\", flush=True)\n",
    "        embs = encode_texts(texts, pool=name)\n",
    "        df_tmp = df_lab.assign(**{f\"emb_{name}_3\": list(embs), \"symbol\": sym})\n",
    "        buffers[name].append(df_tmp)\n",
    "        print(\" done\")\n",
    "\n",
    "\n",
    "#   Concatenate into X_*_3 and build y_vec_3d\n",
    "for name, parts in buffers.items():\n",
    "    print(f\"\\nConcatenating embeddings for pool={name!r} ({len(parts)} symbols)...\", end=\"\", flush=True)\n",
    "    big = pd.concat(parts, ignore_index=True)\n",
    "    X   = np.vstack(big[f\"emb_{name}_3\"].values)\n",
    "    globals()[f\"X_{name}_3\"] = X\n",
    "    print(f\" done → shape {X.shape}\")\n",
    "\n",
    "    if y_vec_3d is None:\n",
    "        y_vec_3d = big[\"label\"].astype(int).values\n",
    "    else:\n",
    "        assert len(y_vec_3d) == X.shape[0]\n",
    "\n",
    "print(f\"\\nFinal y_vec_3d.shape = {y_vec_3d.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1032,
     "status": "ok",
     "timestamp": 1751562810612,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "sa622x4nd4F1",
    "outputId": "eb2bd9ed-37e2-4b1c-e724-c44112d63bed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "✅ Saved all embeddings to /content/drive/My Drive/Sentimental Analysis for Algo trading\n"
     ]
    }
   ],
   "source": [
    "# 1) Mount Google Drive (only needs to run once per session)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 2) Set up the target path\n",
    "import os\n",
    "base = '/content/drive/My Drive/Sentimental Analysis for Algo trading'\n",
    "os.makedirs(base, exist_ok=True)   # creates the folder if it doesn’t already exist\n",
    "\n",
    "# 3) Save each embedding matrix (and labels) as .npy or .npz\n",
    "import numpy as np\n",
    "\n",
    "# Option A: separate .npy files\n",
    "np.save(os.path.join(base, 'X_mean_3.npy'),   X_mean_3)\n",
    "np.save(os.path.join(base, 'X_max_3.npy'),    X_max_3)\n",
    "np.save(os.path.join(base, 'X_meanmax_3.npy'),X_meanmax_3)\n",
    "np.save(os.path.join(base, 'X_pseudo_3.npy'), X_pseudo_3)\n",
    "np.save(os.path.join(base, 'y_vec_3d.npy'),   y_vec_3d)\n",
    "\n",
    "# Option B: one .npz container\n",
    "np.savez(os.path.join(base, 'embeddings_3day.npz'),\n",
    "         X_mean_3=X_mean_3,\n",
    "         X_max_3=X_max_3,\n",
    "         X_meanmax_3=X_meanmax_3,\n",
    "         X_pseudo_3=X_pseudo_3,\n",
    "         y_vec_3d=y_vec_3d)\n",
    "\n",
    "print(\"✅ Saved all embeddings to\", base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1615,
     "status": "ok",
     "timestamp": 1751562825808,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "yywy8X8iHUiV",
    "outputId": "ba8f7d84-6305-4e26-ea92-5cc27fb0a296"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:      0.5687\n",
      "Micro F1:      0.5687\n",
      "Macro F1:      0.5156\n",
      "Weighted F1:   0.5477\n",
      "Macro Recall:  0.5234\n"
     ]
    }
   ],
   "source": [
    "# do PCA based on mean pooling embedding and fit logistic regression\n",
    "pca_mean    = PCA(n_components=100, random_state=42)\n",
    "X_mean_pca  = pca_mean.fit_transform(X_mean_3)\n",
    "\n",
    "X_tr_mean_3, X_te_mean_3, y_tr_mean_3, y_te_mean_3 = train_test_split(\n",
    "    X_mean_pca, y_vec_3d,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_vec_3d\n",
    ")\n",
    "\n",
    "logit = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logit.fit(X_tr_mean_3, y_tr_mean_3)\n",
    "\n",
    "y_pred_mean_3 = logit.predict(X_te_mean_3)\n",
    "\n",
    "acc_mean_3        = accuracy_score(y_te_mean_3, y_pred_mean_3)\n",
    "micro_f1_mean_3      = f1_score(y_te_mean_3, y_pred_mean_3, average=\"micro\")\n",
    "macro_f1_mean_3      = f1_score(y_te_mean_3, y_pred_mean_3, average=\"macro\")\n",
    "weighted_f1_mean_3   = f1_score(y_te_mean_3, y_pred_mean_3, average=\"weighted\")\n",
    "macro_recall_mean_3  = recall_score(y_te_mean_3, y_pred_mean_3, average=\"macro\")\n",
    "\n",
    "print(f\"Accuracy:      {acc_mean_3:.4f}\")\n",
    "print(f\"Micro F1:      {micro_f1_mean_3:.4f}\")\n",
    "print(f\"Macro F1:      {macro_f1_mean_3:.4f}\")\n",
    "print(f\"Weighted F1:   {weighted_f1_mean_3:.4f}\")\n",
    "print(f\"Macro Recall:  {macro_recall_mean_3:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1849,
     "status": "ok",
     "timestamp": 1751562846699,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "h6AiJVu0JJVR",
    "outputId": "38216033-d70e-4796-b469-a0b3e0a20c16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MAX Pooling ===\n",
      "Accuracy:      0.5625\n",
      "Micro F1:      0.5625\n",
      "Macro F1:      0.5267\n",
      "Weighted F1:   0.5527\n",
      "Macro Recall:  0.5286\n"
     ]
    }
   ],
   "source": [
    "# do PCA based on max pooling embedding and fit logistic regression\n",
    "pca_max     = PCA(n_components=100, random_state=42)\n",
    "X_max_pca   = pca_max.fit_transform(X_max_3)\n",
    "\n",
    "\n",
    "X_tr_max_3, X_te_max_3, y_tr_max_3, y_te_max_3 = train_test_split(\n",
    "    X_max_pca, y_vec_3d,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_vec_3d)\n",
    "\n",
    "logit_max = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logit_max.fit(X_tr_max_3, y_tr_max_3)\n",
    "\n",
    "y_pred_max_3 = logit_max.predict(X_te_max_3)\n",
    "\n",
    "acc_max_3        = accuracy_score(y_te_max_3, y_pred_max_3)\n",
    "micro_f1_max_3   = f1_score(y_te_max_3, y_pred_max_3, average=\"micro\")\n",
    "macro_f1_max_3   = f1_score(y_te_max_3, y_pred_max_3, average=\"macro\")\n",
    "weighted_f1_max_3= f1_score(y_te_max_3, y_pred_max_3, average=\"weighted\")\n",
    "macro_recall_max_3 = recall_score(y_te_max_3, y_pred_max_3, average=\"macro\")\n",
    "\n",
    "print(\"\\n=== MAX Pooling ===\")\n",
    "print(f\"Accuracy:      {acc_max_3:.4f}\")\n",
    "print(f\"Micro F1:      {micro_f1_max_3:.4f}\")\n",
    "print(f\"Macro F1:      {macro_f1_max_3:.4f}\")\n",
    "print(f\"Weighted F1:   {weighted_f1_max_3:.4f}\")\n",
    "print(f\"Macro Recall:  {macro_recall_max_3:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3583,
     "status": "ok",
     "timestamp": 1751562963587,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "HD_2gXqdJNkm",
    "outputId": "97f0ab1e-5843-4897-b12b-982c01b0e4fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:      0.5563\n",
      "Micro F1:      0.5563\n",
      "Macro F1:      0.5143\n",
      "Weighted F1:   0.5429\n",
      "Macro Recall:  0.5182\n"
     ]
    }
   ],
   "source": [
    "# do PCA based on concatenation pooling embedding and fit logistic regression\n",
    "pca_maxmean     = PCA(n_components=100, random_state=42)\n",
    "X_meanmax_pca   = pca_maxmean.fit_transform(X_meanmax_3)\n",
    "\n",
    "X_tr_maxmean_3, X_te_maxmean_3, y_tr_maxmean_3, y_te_maxmean_3 = train_test_split(\n",
    "    X_meanmax_pca, y_vec_3d,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_vec_3d)\n",
    "\n",
    "logit_maxmean = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logit_maxmean.fit(X_tr_maxmean_3, y_tr_maxmean_3)\n",
    "\n",
    "y_pred_maxmean_3 = logit_maxmean.predict(X_te_maxmean_3)\n",
    "\n",
    "acc_maxmean_3         = accuracy_score(y_te_maxmean_3, y_pred_maxmean_3)\n",
    "micro_f1_maxmean_3    = f1_score(y_te_maxmean_3, y_pred_maxmean_3, average=\"micro\")\n",
    "macro_f1_maxmean_3    = f1_score(y_te_maxmean_3, y_pred_maxmean_3, average=\"macro\")\n",
    "weighted_f1_maxmean_3 = f1_score(y_te_maxmean_3, y_pred_maxmean_3, average=\"weighted\")\n",
    "macro_recall_maxmean_3= recall_score(y_te_maxmean_3, y_pred_maxmean_3, average=\"macro\")\n",
    "\n",
    "print(f\"Accuracy:      {acc_maxmean_3:.4f}\")\n",
    "print(f\"Micro F1:      {micro_f1_maxmean_3:.4f}\")\n",
    "print(f\"Macro F1:      {macro_f1_maxmean_3:.4f}\")\n",
    "print(f\"Weighted F1:   {weighted_f1_maxmean_3:.4f}\")\n",
    "print(f\"Macro Recall:  {macro_recall_maxmean_3:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1974,
     "status": "ok",
     "timestamp": 1751562973395,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "ngGri3wIJRtz",
    "outputId": "5da6f9de-046e-4977-e63e-d67ce49c4d03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:      0.6000\n",
      "Micro F1:      0.6000\n",
      "Macro F1:      0.3750\n",
      "Weighted F1:   0.4500\n",
      "Macro Recall:  0.5000\n"
     ]
    }
   ],
   "source": [
    "# do PCA based on pseudo pooling embedding and fit logistic regression\n",
    "pca_pseudo    = PCA(n_components=100, random_state=42)\n",
    "X_pseudo_pca  = pca_pseudo.fit_transform(X_pseudo_3)\n",
    "\n",
    "X_tr_pseudo_3, X_te_pseudo_3, y_tr_pseudo_3, y_te_pseudo_3 = train_test_split(\n",
    "    X_pseudo_pca, y_vec_3d,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_vec_3d)\n",
    "\n",
    "\n",
    "logit_pseudo = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logit_pseudo.fit(X_tr_pseudo_3, y_tr_pseudo_3)\n",
    "\n",
    "\n",
    "y_pred_pseudo_3 = logit_pseudo.predict(X_te_pseudo_3)\n",
    "\n",
    "acc_pseudo_3         = accuracy_score(y_te_pseudo_3, y_pred_pseudo_3)\n",
    "micro_f1_pseudo_3    = f1_score(y_te_pseudo_3, y_pred_pseudo_3, average=\"micro\")\n",
    "macro_f1_pseudo_3    = f1_score(y_te_pseudo_3, y_pred_pseudo_3, average=\"macro\")\n",
    "weighted_f1_pseudo_3 = f1_score(y_te_pseudo_3, y_pred_pseudo_3, average=\"weighted\")\n",
    "macro_recall_pseudo_3= recall_score(y_te_pseudo_3, y_pred_pseudo_3, average=\"macro\")\n",
    "\n",
    "print(f\"Accuracy:      {acc_pseudo_3:.4f}\")\n",
    "print(f\"Micro F1:      {micro_f1_pseudo_3:.4f}\")\n",
    "print(f\"Macro F1:      {macro_f1_pseudo_3:.4f}\")\n",
    "print(f\"Weighted F1:   {weighted_f1_pseudo_3:.4f}\")\n",
    "print(f\"Macro Recall:  {macro_recall_pseudo_3:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 17478,
     "status": "ok",
     "timestamp": 1751564485955,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "weA9tBQNJoGe"
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install --quiet gspread google-auth-oauthlib\n",
    "\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "import google.auth\n",
    "import gspread\n",
    "import pandas as pd\n",
    "\n",
    "creds, _ = google.auth.default(\n",
    "    scopes=[\"https://www.googleapis.com/auth/spreadsheets.readonly\"]\n",
    ")\n",
    "gc = gspread.authorize(creds)\n",
    "\n",
    "\n",
    "SHEET_ID = \"1WtZNbGdqRiG4M17yL3gvYIlWMvUEYkpdcKc27RLbzcM\"\n",
    "sh       = gc.open_by_key(SHEET_ID)\n",
    "ws       = sh.get_worksheet(0)\n",
    "\n",
    "records = ws.get_all_records(head=3)\n",
    "sheet = pd.DataFrame(records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "executionInfo": {
     "elapsed": 100,
     "status": "ok",
     "timestamp": 1751564489698,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -60
    },
    "id": "V8nnmO3WSKnz",
    "outputId": "cefd4f43-e020-4a93-e7b4-7ca796bd5692"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"sheet\",\n  \"rows\": 14,\n  \"fields\": [\n    {\n      \"column\": \"Embedding Methods\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"Mean embedding\",\n          \"Max embedding\",\n          \"paraphrase-MiniLM-L6-v2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.6456,\n          0.6076,\n          \"Accuracy\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Micro F1\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.6456,\n          0.6076,\n          \"Micro F1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Macro F1\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.6186,\n          0.5835,\n          \"Macro F1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Weighted F1\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.634,\n          0.5987,\n          \"Weighted F1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Macro recall\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.6195,\n          0.5845,\n          \"Macro recall\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "sheet"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-c847443f-f523-4ea0-ad68-33aaaf2dd3a2\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Embedding Methods</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Weighted F1</th>\n",
       "      <th>Macro recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mean embedding</td>\n",
       "      <td>0.5823</td>\n",
       "      <td>0.5823</td>\n",
       "      <td>0.562</td>\n",
       "      <td>0.5763</td>\n",
       "      <td>0.5626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Max embedding</td>\n",
       "      <td>0.6076</td>\n",
       "      <td>0.6076</td>\n",
       "      <td>0.5835</td>\n",
       "      <td>0.5987</td>\n",
       "      <td>0.5845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Max mean Concatenation</td>\n",
       "      <td>0.5696</td>\n",
       "      <td>0.5696</td>\n",
       "      <td>0.5461</td>\n",
       "      <td>0.5618</td>\n",
       "      <td>0.5476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pseudo embedding</td>\n",
       "      <td>0.6013</td>\n",
       "      <td>0.6013</td>\n",
       "      <td>0.5725</td>\n",
       "      <td>0.5893</td>\n",
       "      <td>0.5751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>paraphrase-MiniLM-L6-v2</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Embedding Methods</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>Micro F1</td>\n",
       "      <td>Macro F1</td>\n",
       "      <td>Weighted F1</td>\n",
       "      <td>Macro recall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Mean embedding</td>\n",
       "      <td>0.6582</td>\n",
       "      <td>0.6582</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.6331</td>\n",
       "      <td>0.6206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Max embedding</td>\n",
       "      <td>0.5759</td>\n",
       "      <td>0.5759</td>\n",
       "      <td>0.5453</td>\n",
       "      <td>0.5633</td>\n",
       "      <td>0.5492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Max mean Concatenation</td>\n",
       "      <td>0.6456</td>\n",
       "      <td>0.6456</td>\n",
       "      <td>0.6186</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.6195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Pseudo embedding</td>\n",
       "      <td>0.6203</td>\n",
       "      <td>0.6203</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.5983</td>\n",
       "      <td>0.5857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c847443f-f523-4ea0-ad68-33aaaf2dd3a2')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-c847443f-f523-4ea0-ad68-33aaaf2dd3a2 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-c847443f-f523-4ea0-ad68-33aaaf2dd3a2');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-e2849c95-5af0-4e98-9550-ba754605c2a9\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e2849c95-5af0-4e98-9550-ba754605c2a9')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-e2849c95-5af0-4e98-9550-ba754605c2a9 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "  <div id=\"id_a2e72916-fc48-46d3-a1fa-b970e1f6845c\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('sheet')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_a2e72916-fc48-46d3-a1fa-b970e1f6845c button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('sheet');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "          Embedding Methods  Accuracy  Micro F1  Macro F1  Weighted F1  \\\n",
       "0            Mean embedding    0.5823    0.5823     0.562       0.5763   \n",
       "1             Max embedding    0.6076    0.6076    0.5835       0.5987   \n",
       "2    Max mean Concatenation    0.5696    0.5696    0.5461       0.5618   \n",
       "3          Pseudo embedding    0.6013    0.6013    0.5725       0.5893   \n",
       "4                                                                        \n",
       "5                                                                        \n",
       "6                                                                        \n",
       "7                                                                        \n",
       "8   paraphrase-MiniLM-L6-v2                                              \n",
       "9         Embedding Methods  Accuracy  Micro F1  Macro F1  Weighted F1   \n",
       "10           Mean embedding    0.6582    0.6582     0.613       0.6331   \n",
       "11            Max embedding    0.5759    0.5759    0.5453       0.5633   \n",
       "12   Max mean Concatenation    0.6456    0.6456    0.6186        0.634   \n",
       "13         Pseudo embedding    0.6203    0.6203     0.578       0.5983   \n",
       "\n",
       "    Macro recall  \n",
       "0         0.5626  \n",
       "1         0.5845  \n",
       "2         0.5476  \n",
       "3         0.5751  \n",
       "4                 \n",
       "5                 \n",
       "6                 \n",
       "7                 \n",
       "8                 \n",
       "9   Macro recall  \n",
       "10        0.6206  \n",
       "11        0.5492  \n",
       "12        0.6195  \n",
       "13        0.5857  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3S--xzD1YV_U"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QrHqnXf6HXTR"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
