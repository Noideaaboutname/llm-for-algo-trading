{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 0,
     "status": "error",
     "timestamp": 1757306014373,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -480
    },
    "id": "WnlXLlREl0Yn",
    "outputId": "b8741847-8891-4464-d4a4-bf972f84b7a1"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "mount failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1451802130.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnbformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mSRC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Colab Notebooks/Untitled14.ipynb'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mDST\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/colab_saves/baseline_model.ipynb'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: mount failed"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import nbformat, os\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "SRC = '/content/drive/MyDrive/Colab Notebooks/Untitled14.ipynb'\n",
    "DST = '/content/drive/MyDrive/colab_saves/baseline_model.ipynb'\n",
    "\n",
    "with open(SRC, 'r', encoding='utf-8') as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "\n",
    "nb.metadata.pop('widgets', None)\n",
    "for cell in nb.cells:\n",
    "    cell.metadata.pop('widgets', None)\n",
    "os.makedirs(os.path.dirname(DST), exist_ok=True)\n",
    "with open(DST, 'w', encoding='utf-8') as f:\n",
    "    nbformat.write(nb, f)\n",
    "\n",
    "print(\"✅ Clean copy saved to\", DST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3384,
     "status": "ok",
     "timestamp": 1757130582987,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -480
    },
    "id": "gM6U1u2NG6bc",
    "outputId": "5124d447-6a04-44b3-d426-30ba97179554"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "\n",
      "group2:\n",
      "C (400, 21) ['id', 'type', 'publishOn', 'attributes.isLockedPro', 'attributes.commentCount']\n",
      "GS (400, 21) ['id', 'type', 'publishOn', 'attributes.isLockedPro', 'attributes.commentCount']\n",
      "MS (400, 21) ['id', 'type', 'publishOn', 'attributes.isLockedPro', 'attributes.commentCount']\n",
      "USB (400, 21) ['id', 'type', 'publishOn', 'attributes.isLockedPro', 'attributes.commentCount']\n",
      "\n",
      "group3:\n",
      "AAPL (400, 21) ['id', 'type', 'publishOn', 'attributes.isLockedPro', 'attributes.commentCount']\n",
      "MSFT (400, 21) ['id', 'type', 'publishOn', 'attributes.isLockedPro', 'attributes.commentCount']\n",
      "AMZN (400, 21) ['id', 'type', 'publishOn', 'attributes.isLockedPro', 'attributes.commentCount']\n",
      "GOOGL (400, 21) ['id', 'type', 'publishOn', 'attributes.isLockedPro', 'attributes.commentCount']\n",
      "\n",
      "group4:\n",
      "META (400, 21) ['id', 'type', 'publishOn', 'attributes.isLockedPro', 'attributes.commentCount']\n",
      "NVDA (400, 21) ['id', 'type', 'publishOn', 'attributes.isLockedPro', 'attributes.commentCount']\n",
      "TSLA (400, 21) ['id', 'type', 'publishOn', 'attributes.isLockedPro', 'attributes.commentCount']\n",
      "NFLX (400, 21) ['id', 'type', 'publishOn', 'attributes.isLockedPro', 'attributes.commentCount']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pickle, pandas as pd\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "with open('/content/drive/My Drive/colab_saves/company_dfs.pkl', 'rb') as f:\n",
    "    labeled_groups = pickle.load(f)\n",
    "\n",
    "labeled_group2 = labeled_groups['group2']\n",
    "labeled_group3 = labeled_groups['group3']\n",
    "labeled_group4 = labeled_groups['group4']\n",
    "\n",
    "for name, group in [('group2', labeled_group2),\n",
    "                    ('group3', labeled_group3),\n",
    "                    ('group4', labeled_group4)]:\n",
    "    print(f\"\\n{name}:\")\n",
    "    for sym, df in group.items():\n",
    "        print(sym, df.shape, list(df.columns)[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42513,
     "status": "ok",
     "timestamp": 1757130629505,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -480
    },
    "id": "0CWTFJgLG30E",
    "outputId": "82ff3c2b-06c5-4d00-fb8b-866b75bb96fb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 1536)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "qwen25_model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "qwen25_tok = AutoTokenizer.from_pretrained(qwen25_model_id, use_fast=True)\n",
    "qwen25_mdl = AutoModelForCausalLM.from_pretrained(\n",
    "    qwen25_model_id,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\",)\n",
    "qwen25_mdl.eval()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1757130629842,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -480
    },
    "id": "CwLPSbykOR4x"
   },
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "def llm_binary_predict_batched(\n",
    "    titles,\n",
    "    tok,\n",
    "    mdl,\n",
    "    batch_size=32,\n",
    "    max_new_tokens=1,\n",
    "    trunc_len=128,\n",
    "    heartbeat_every=50,):\n",
    "\n",
    "\n",
    "    preds = []\n",
    "    mdl.eval()\n",
    "\n",
    "    prompt_tpl = 'Headline: \"{title}\"\\nAnswer strictly 1 if price will go up, else 0.'\n",
    "\n",
    "    n = len(titles)\n",
    "    num_batches = ceil(n / batch_size)\n",
    "\n",
    "    for bi in range(num_batches):\n",
    "        start = bi * batch_size\n",
    "        end = min((bi + 1) * batch_size, n)\n",
    "        batch_titles = titles[start:end]\n",
    "\n",
    "        prompts = [prompt_tpl.format(title=t) for t in batch_titles]\n",
    "\n",
    "        try:\n",
    "\n",
    "            enc = tok(\n",
    "                prompts,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=trunc_len,\n",
    "            ).to(mdl.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                out = mdl.generate(\n",
    "                    **enc,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    do_sample=False,\n",
    "                    num_beams=1,\n",
    "                    use_cache=True,)\n",
    "\n",
    "\n",
    "            gen_only = out[:, enc[\"input_ids\"].shape[1]:]\n",
    "            texts = tok.batch_decode(gen_only, skip_special_tokens=True)\n",
    "\n",
    "            for txt in texts:\n",
    "                s = txt.strip()\n",
    "                s_clean = s.lower().strip()\n",
    "                if s_clean.startswith(\"1\"):\n",
    "                    preds.append(1)\n",
    "                elif s_clean.startswith(\"0\"):\n",
    "                    preds.append(0)\n",
    "                else:\n",
    "                    preds.append(1 if \"1\" in s_clean else 0)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] batch {bi+1}/{num_batches} failed: {e}\")\n",
    "            preds.extend([0] * (end - start))\n",
    "\n",
    "        if (bi + 1) % heartbeat_every == 0:\n",
    "            print(f\"  processed {end}/{n} titles...\", flush=True)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    return preds[:n]\n",
    "\n",
    "\n",
    "def evaluate_group(group_dict, tok, mdl, sample_n=None, seed=42):\n",
    "\n",
    "    df_all = pd.concat(group_dict.values(), ignore_index=True)\n",
    "\n",
    "    if sample_n is not None and sample_n < len(df_all):\n",
    "        df_all = df_all.sample(sample_n, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "    titles = df_all[\"title\"].astype(str).tolist()\n",
    "    labels = df_all[\"label\"].astype(int).tolist()\n",
    "\n",
    "    preds = llm_binary_predict_batched(\n",
    "        titles,\n",
    "        tok,\n",
    "        mdl,\n",
    "        batch_size=32,\n",
    "        max_new_tokens=1,\n",
    "        trunc_len=128,\n",
    "        heartbeat_every=50,)\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    micro_f1 = f1_score(labels, preds, average=\"micro\")\n",
    "    macro_f1 = f1_score(labels, preds, average=\"macro\")\n",
    "    weighted_f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    macro_recall = recall_score(labels, preds, average=\"macro\")\n",
    "\n",
    "    return {\n",
    "        \"Accuracy\": acc,\n",
    "        \"Micro-F1\": micro_f1,\n",
    "        \"Macro-F1\": macro_f1,\n",
    "        \"Weighted-F1\": weighted_f1,\n",
    "        \"Macro-Recall\": macro_recall}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 745046,
     "status": "ok",
     "timestamp": 1757131374891,
     "user": {
      "displayName": "Peng",
      "userId": "11558679983686850100"
     },
     "user_tz": -480
    },
    "id": "6xo4RZDCIRf1",
    "outputId": "18e96156-ba2f-462d-a251-993e9e45da93"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating group2 with Qwen2.5-1.5B-Instruct ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy    :  46.00%\n",
      "Micro-F1    :  46.00%\n",
      "Macro-F1    :  31.51%\n",
      "Weighted-F1 :  28.99%\n",
      "Macro-Recall:  50.00%\n",
      "\n",
      "=== Evaluating group3 with Qwen2.5-1.5B-Instruct ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy    :  44.00%\n",
      "Micro-F1    :  44.00%\n",
      "Macro-F1    :  30.56%\n",
      "Weighted-F1 :  26.89%\n",
      "Macro-Recall:  50.00%\n",
      "\n",
      "=== Evaluating group4 with Qwen2.5-1.5B-Instruct ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy    :  51.00%\n",
      "Micro-F1    :  51.00%\n",
      "Macro-F1    :  33.77%\n",
      "Weighted-F1 :  34.45%\n",
      "Macro-Recall:  50.00%\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for group_name, labeled_group in [\n",
    "    (\"group2\", labeled_group2),\n",
    "    (\"group3\", labeled_group3),\n",
    "    (\"group4\", labeled_group4),\n",
    "]:\n",
    "    print(f\"\\n=== Evaluating {group_name} with Qwen2.5-1.5B-Instruct ===\")\n",
    "    metrics = evaluate_group(labeled_group, qwen25_tok, qwen25_mdl, sample_n=100)  # ← set None for full run\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k:12}: {v*100:6.2f}%\")\n",
    "    results[(group_name, \"Qwen2.5-1.5B-Instruct\")] = metrics\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOhHu+SPwgAkMyjYU0lMYXl",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
